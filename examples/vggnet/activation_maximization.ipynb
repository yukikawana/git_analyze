{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Maximization on VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize activation over final dense layer outputs, we need to switch the `softmax` activation out for `linear` since gradient of output node will depend on all the other node activations. Doing this in keras is tricky, so we provide `utils.apply_modifications` to modify network parameters and rebuild the graph.\n",
    "\n",
    "If this swapping is not done, the results might be suboptimal. We will start by swapping out 'softmax' for 'linear' and compare what happens if we dont do this at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "# Build the VGG16 network with ImageNet weights\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "# Utility to search for layer index by name. \n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
    "\n",
    "# Swap softmax with linear\n",
    "model.layers[layer_idx].activation = activations.linear\n",
    "model = utils.apply_modifications(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualizing a specific output category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try visualizing a specific output category. We will pick `ouzel` which corresponds to imagenet category `20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f54187c57d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f546866b3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vis.visualization import visualize_activation\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "\n",
    "# 20 is the imagenet category for 'ouzel'\n",
    "img = visualize_activation(model, layer_idx, filter_indices=20)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that sort of looks like a bird. Lets see if we can get better results with more iterations. This time, lets see the verbose output during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, named_losses: [('ActivationMax Loss', -0.23652837),\n",
      " ('L-6.0 Norm Loss', 0.063257359),\n",
      " ('TV(2.0) Loss', 6454.2188)], overall loss: 6454.04541016\n",
      "Iteration: 2, named_losses: [('ActivationMax Loss', -0.38862073),\n",
      " ('L-6.0 Norm Loss', 0.062633291),\n",
      " ('TV(2.0) Loss', 3182.8979)], overall loss: 3182.57202148\n",
      "Iteration: 3, named_losses: [('ActivationMax Loss', -0.80792999),\n",
      " ('L-6.0 Norm Loss', 0.062285963),\n",
      " ('TV(2.0) Loss', 1660.4106)], overall loss: 1659.66503906\n",
      "Iteration: 4, named_losses: [('ActivationMax Loss', -1.3529515),\n",
      " ('L-6.0 Norm Loss', 0.062074915),\n",
      " ('TV(2.0) Loss', 848.22705)], overall loss: 846.936157227\n",
      "Iteration: 5, named_losses: [('ActivationMax Loss', -1.9309249),\n",
      " ('L-6.0 Norm Loss', 0.061946813),\n",
      " ('TV(2.0) Loss', 414.09256)], overall loss: 412.223571777\n",
      "Iteration: 6, named_losses: [('ActivationMax Loss', -2.6263728),\n",
      " ('L-6.0 Norm Loss', 0.0618737),\n",
      " ('TV(2.0) Loss', 215.12886)], overall loss: 212.564361572\n",
      "Iteration: 7, named_losses: [('ActivationMax Loss', -2.5867615),\n",
      " ('L-6.0 Norm Loss', 0.061841115),\n",
      " ('TV(2.0) Loss', 155.9509)], overall loss: 153.425979614\n",
      "Iteration: 8, named_losses: [('ActivationMax Loss', -3.5514884),\n",
      " ('L-6.0 Norm Loss', 0.061822094),\n",
      " ('TV(2.0) Loss', 109.82358)], overall loss: 106.333908081\n",
      "Iteration: 9, named_losses: [('ActivationMax Loss', -4.945785),\n",
      " ('L-6.0 Norm Loss', 0.061810952),\n",
      " ('TV(2.0) Loss', 83.539429)], overall loss: 78.655456543\n",
      "Iteration: 10, named_losses: [('ActivationMax Loss', -6.3652425),\n",
      " ('L-6.0 Norm Loss', 0.061804228),\n",
      " ('TV(2.0) Loss', 63.23769)], overall loss: 56.9342498779\n",
      "Iteration: 11, named_losses: [('ActivationMax Loss', -6.484271),\n",
      " ('L-6.0 Norm Loss', 0.061801635),\n",
      " ('TV(2.0) Loss', 54.48938)], overall loss: 48.06690979\n",
      "Iteration: 12, named_losses: [('ActivationMax Loss', -8.5590439),\n",
      " ('L-6.0 Norm Loss', 0.061799563),\n",
      " ('TV(2.0) Loss', 47.193943)], overall loss: 38.6967010498\n",
      "Iteration: 13, named_losses: [('ActivationMax Loss', -9.3418932),\n",
      " ('L-6.0 Norm Loss', 0.061799757),\n",
      " ('TV(2.0) Loss', 42.500381)], overall loss: 33.220287323\n",
      "Iteration: 14, named_losses: [('ActivationMax Loss', -11.457891),\n",
      " ('L-6.0 Norm Loss', 0.061800055),\n",
      " ('TV(2.0) Loss', 37.536629)], overall loss: 26.140537262\n",
      "Iteration: 15, named_losses: [('ActivationMax Loss', -13.214234),\n",
      " ('L-6.0 Norm Loss', 0.06180035),\n",
      " ('TV(2.0) Loss', 41.195164)], overall loss: 28.0427284241\n",
      "Iteration: 16, named_losses: [('ActivationMax Loss', -14.028315),\n",
      " ('L-6.0 Norm Loss', 0.061799593),\n",
      " ('TV(2.0) Loss', 35.364285)], overall loss: 21.397769928\n",
      "Iteration: 17, named_losses: [('ActivationMax Loss', -15.863123),\n",
      " ('L-6.0 Norm Loss', 0.061801132),\n",
      " ('TV(2.0) Loss', 34.293465)], overall loss: 18.4921417236\n",
      "Iteration: 18, named_losses: [('ActivationMax Loss', -18.475391),\n",
      " ('L-6.0 Norm Loss', 0.061800994),\n",
      " ('TV(2.0) Loss', 33.899643)], overall loss: 15.4860534668\n",
      "Iteration: 19, named_losses: [('ActivationMax Loss', -18.495914),\n",
      " ('L-6.0 Norm Loss', 0.061802723),\n",
      " ('TV(2.0) Loss', 35.162006)], overall loss: 16.7278938293\n",
      "Iteration: 20, named_losses: [('ActivationMax Loss', -18.313532),\n",
      " ('L-6.0 Norm Loss', 0.061802253),\n",
      " ('TV(2.0) Loss', 29.397903)], overall loss: 11.1461734772\n",
      "Iteration: 21, named_losses: [('ActivationMax Loss', -19.300627),\n",
      " ('L-6.0 Norm Loss', 0.061804581),\n",
      " ('TV(2.0) Loss', 28.367479)], overall loss: 9.12865638733\n",
      "Iteration: 22, named_losses: [('ActivationMax Loss', -20.045309),\n",
      " ('L-6.0 Norm Loss', 0.06180536),\n",
      " ('TV(2.0) Loss', 31.620438)], overall loss: 11.6369342804\n",
      "Iteration: 23, named_losses: [('ActivationMax Loss', -20.119078),\n",
      " ('L-6.0 Norm Loss', 0.061806351),\n",
      " ('TV(2.0) Loss', 30.15749)], overall loss: 10.1002178192\n",
      "Iteration: 24, named_losses: [('ActivationMax Loss', -22.055819),\n",
      " ('L-6.0 Norm Loss', 0.06180672),\n",
      " ('TV(2.0) Loss', 28.695919)], overall loss: 6.70190811157\n",
      "Iteration: 25, named_losses: [('ActivationMax Loss', -21.252024),\n",
      " ('L-6.0 Norm Loss', 0.061808087),\n",
      " ('TV(2.0) Loss', 28.868387)], overall loss: 7.67817115784\n",
      "Iteration: 26, named_losses: [('ActivationMax Loss', -24.149572),\n",
      " ('L-6.0 Norm Loss', 0.061808869),\n",
      " ('TV(2.0) Loss', 28.443205)], overall loss: 4.35544204712\n",
      "Iteration: 27, named_losses: [('ActivationMax Loss', -21.984728),\n",
      " ('L-6.0 Norm Loss', 0.061811917),\n",
      " ('TV(2.0) Loss', 25.873085)], overall loss: 3.95016860962\n",
      "Iteration: 28, named_losses: [('ActivationMax Loss', -25.596382),\n",
      " ('L-6.0 Norm Loss', 0.06181211),\n",
      " ('TV(2.0) Loss', 27.402933)], overall loss: 1.86836242676\n",
      "Iteration: 29, named_losses: [('ActivationMax Loss', -25.598436),\n",
      " ('L-6.0 Norm Loss', 0.061814241),\n",
      " ('TV(2.0) Loss', 26.746065)], overall loss: 1.20944213867\n",
      "Iteration: 30, named_losses: [('ActivationMax Loss', -26.960041),\n",
      " ('L-6.0 Norm Loss', 0.061815515),\n",
      " ('TV(2.0) Loss', 26.27165)], overall loss: -0.626575469971\n",
      "Iteration: 31, named_losses: [('ActivationMax Loss', -27.99596),\n",
      " ('L-6.0 Norm Loss', 0.061817057),\n",
      " ('TV(2.0) Loss', 25.858105)], overall loss: -2.0760383606\n",
      "Iteration: 32, named_losses: [('ActivationMax Loss', -30.061876),\n",
      " ('L-6.0 Norm Loss', 0.06181993),\n",
      " ('TV(2.0) Loss', 27.144829)], overall loss: -2.85522842407\n",
      "Iteration: 33, named_losses: [('ActivationMax Loss', -30.428617),\n",
      " ('L-6.0 Norm Loss', 0.061821472),\n",
      " ('TV(2.0) Loss', 28.232914)], overall loss: -2.13388252258\n",
      "Iteration: 34, named_losses: [('ActivationMax Loss', -31.3414),\n",
      " ('L-6.0 Norm Loss', 0.061822757),\n",
      " ('TV(2.0) Loss', 27.857368)], overall loss: -3.42220878601\n",
      "Iteration: 35, named_losses: [('ActivationMax Loss', -32.909225),\n",
      " ('L-6.0 Norm Loss', 0.061823759),\n",
      " ('TV(2.0) Loss', 27.695848)], overall loss: -5.15155220032\n",
      "Iteration: 36, named_losses: [('ActivationMax Loss', -34.16124),\n",
      " ('L-6.0 Norm Loss', 0.061824419),\n",
      " ('TV(2.0) Loss', 27.488815)], overall loss: -6.61059951782\n",
      "Iteration: 37, named_losses: [('ActivationMax Loss', -33.892025),\n",
      " ('L-6.0 Norm Loss', 0.061826207),\n",
      " ('TV(2.0) Loss', 29.994162)], overall loss: -3.83603858948\n",
      "Iteration: 38, named_losses: [('ActivationMax Loss', -37.093285),\n",
      " ('L-6.0 Norm Loss', 0.061826963),\n",
      " ('TV(2.0) Loss', 28.946991)], overall loss: -8.08446502686\n",
      "Iteration: 39, named_losses: [('ActivationMax Loss', -35.601936),\n",
      " ('L-6.0 Norm Loss', 0.061829448),\n",
      " ('TV(2.0) Loss', 30.475609)], overall loss: -5.06449890137\n",
      "Iteration: 40, named_losses: [('ActivationMax Loss', -36.167427),\n",
      " ('L-6.0 Norm Loss', 0.061830733),\n",
      " ('TV(2.0) Loss', 27.217939)], overall loss: -8.88765525818\n",
      "Iteration: 41, named_losses: [('ActivationMax Loss', -36.437752),\n",
      " ('L-6.0 Norm Loss', 0.061833352),\n",
      " ('TV(2.0) Loss', 29.842951)], overall loss: -6.53296852112\n",
      "Iteration: 42, named_losses: [('ActivationMax Loss', -38.717491),\n",
      " ('L-6.0 Norm Loss', 0.061834473),\n",
      " ('TV(2.0) Loss', 28.724369)], overall loss: -9.93128585815\n",
      "Iteration: 43, named_losses: [('ActivationMax Loss', -40.111435),\n",
      " ('L-6.0 Norm Loss', 0.061836857),\n",
      " ('TV(2.0) Loss', 29.991962)], overall loss: -10.057636261\n",
      "Iteration: 44, named_losses: [('ActivationMax Loss', -36.964535),\n",
      " ('L-6.0 Norm Loss', 0.061837479),\n",
      " ('TV(2.0) Loss', 28.889849)], overall loss: -8.01284980774\n",
      "Iteration: 45, named_losses: [('ActivationMax Loss', -37.320229),\n",
      " ('L-6.0 Norm Loss', 0.061839785),\n",
      " ('TV(2.0) Loss', 29.115425)], overall loss: -8.14296340942\n",
      "Iteration: 46, named_losses: [('ActivationMax Loss', -39.844109),\n",
      " ('L-6.0 Norm Loss', 0.061841976),\n",
      " ('TV(2.0) Loss', 27.454922)], overall loss: -12.3273429871\n",
      "Iteration: 47, named_losses: [('ActivationMax Loss', -37.078773),\n",
      " ('L-6.0 Norm Loss', 0.06184388),\n",
      " ('TV(2.0) Loss', 28.987179)], overall loss: -8.02975082397\n",
      "Iteration: 48, named_losses: [('ActivationMax Loss', -40.641052),\n",
      " ('L-6.0 Norm Loss', 0.061846986),\n",
      " ('TV(2.0) Loss', 28.054935)], overall loss: -12.524269104\n",
      "Iteration: 49, named_losses: [('ActivationMax Loss', -37.442184),\n",
      " ('L-6.0 Norm Loss', 0.06184734),\n",
      " ('TV(2.0) Loss', 29.338017)], overall loss: -8.04232025146\n",
      "Iteration: 50, named_losses: [('ActivationMax Loss', -42.176495),\n",
      " ('L-6.0 Norm Loss', 0.061851047),\n",
      " ('TV(2.0) Loss', 28.415512)], overall loss: -13.699131012\n",
      "Iteration: 51, named_losses: [('ActivationMax Loss', -41.259262),\n",
      " ('L-6.0 Norm Loss', 0.061850555),\n",
      " ('TV(2.0) Loss', 29.640665)], overall loss: -11.5567455292\n",
      "Iteration: 52, named_losses: [('ActivationMax Loss', -43.201393),\n",
      " ('L-6.0 Norm Loss', 0.061853886),\n",
      " ('TV(2.0) Loss', 29.517044)], overall loss: -13.6224937439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 53, named_losses: [('ActivationMax Loss', -44.086586),\n",
      " ('L-6.0 Norm Loss', 0.061855465),\n",
      " ('TV(2.0) Loss', 29.958473)], overall loss: -14.0662574768\n",
      "Iteration: 54, named_losses: [('ActivationMax Loss', -46.265251),\n",
      " ('L-6.0 Norm Loss', 0.061856702),\n",
      " ('TV(2.0) Loss', 30.644764)], overall loss: -15.558631897\n",
      "Iteration: 55, named_losses: [('ActivationMax Loss', -46.109085),\n",
      " ('L-6.0 Norm Loss', 0.061856948),\n",
      " ('TV(2.0) Loss', 32.343224)], overall loss: -13.7040061951\n",
      "Iteration: 56, named_losses: [('ActivationMax Loss', -46.154011),\n",
      " ('L-6.0 Norm Loss', 0.061859209),\n",
      " ('TV(2.0) Loss', 29.866631)], overall loss: -16.2255210876\n",
      "Iteration: 57, named_losses: [('ActivationMax Loss', -48.1483),\n",
      " ('L-6.0 Norm Loss', 0.061861962),\n",
      " ('TV(2.0) Loss', 31.805061)], overall loss: -16.281375885\n",
      "Iteration: 58, named_losses: [('ActivationMax Loss', -47.153053),\n",
      " ('L-6.0 Norm Loss', 0.061863355),\n",
      " ('TV(2.0) Loss', 32.083012)], overall loss: -15.0081787109\n",
      "Iteration: 59, named_losses: [('ActivationMax Loss', -49.861752),\n",
      " ('L-6.0 Norm Loss', 0.061865266),\n",
      " ('TV(2.0) Loss', 31.922997)], overall loss: -17.8768882751\n",
      "Iteration: 60, named_losses: [('ActivationMax Loss', -47.217636),\n",
      " ('L-6.0 Norm Loss', 0.061866514),\n",
      " ('TV(2.0) Loss', 32.404999)], overall loss: -14.7507705688\n",
      "Iteration: 61, named_losses: [('ActivationMax Loss', -47.677673),\n",
      " ('L-6.0 Norm Loss', 0.061867926),\n",
      " ('TV(2.0) Loss', 31.500904)], overall loss: -16.1149024963\n",
      "Iteration: 62, named_losses: [('ActivationMax Loss', -48.208061),\n",
      " ('L-6.0 Norm Loss', 0.061871447),\n",
      " ('TV(2.0) Loss', 30.149673)], overall loss: -17.9965171814\n",
      "Iteration: 63, named_losses: [('ActivationMax Loss', -48.763683),\n",
      " ('L-6.0 Norm Loss', 0.061873682),\n",
      " ('TV(2.0) Loss', 30.57785)], overall loss: -18.1239585876\n",
      "Iteration: 64, named_losses: [('ActivationMax Loss', -49.660126),\n",
      " ('L-6.0 Norm Loss', 0.061875287),\n",
      " ('TV(2.0) Loss', 31.21563)], overall loss: -18.3826217651\n",
      "Iteration: 65, named_losses: [('ActivationMax Loss', -50.520203),\n",
      " ('L-6.0 Norm Loss', 0.061876103),\n",
      " ('TV(2.0) Loss', 32.312675)], overall loss: -18.145652771\n",
      "Iteration: 66, named_losses: [('ActivationMax Loss', -52.544136),\n",
      " ('L-6.0 Norm Loss', 0.061878048),\n",
      " ('TV(2.0) Loss', 32.571915)], overall loss: -19.9103431702\n",
      "Iteration: 67, named_losses: [('ActivationMax Loss', -53.523029),\n",
      " ('L-6.0 Norm Loss', 0.061879326),\n",
      " ('TV(2.0) Loss', 33.641754)], overall loss: -19.8193969727\n",
      "Iteration: 68, named_losses: [('ActivationMax Loss', -52.716587),\n",
      " ('L-6.0 Norm Loss', 0.061881624),\n",
      " ('TV(2.0) Loss', 33.863369)], overall loss: -18.7913360596\n",
      "Iteration: 69, named_losses: [('ActivationMax Loss', -55.23497),\n",
      " ('L-6.0 Norm Loss', 0.061883334),\n",
      " ('TV(2.0) Loss', 33.474625)], overall loss: -21.6984634399\n",
      "Iteration: 70, named_losses: [('ActivationMax Loss', -52.227325),\n",
      " ('L-6.0 Norm Loss', 0.06188463),\n",
      " ('TV(2.0) Loss', 32.758717)], overall loss: -19.4067230225\n",
      "Iteration: 71, named_losses: [('ActivationMax Loss', -55.021244),\n",
      " ('L-6.0 Norm Loss', 0.061888061),\n",
      " ('TV(2.0) Loss', 32.97226)], overall loss: -21.9870948792\n",
      "Iteration: 72, named_losses: [('ActivationMax Loss', -51.993431),\n",
      " ('L-6.0 Norm Loss', 0.061890103),\n",
      " ('TV(2.0) Loss', 33.576019)], overall loss: -18.3555221558\n",
      "Iteration: 73, named_losses: [('ActivationMax Loss', -51.739635),\n",
      " ('L-6.0 Norm Loss', 0.061892442),\n",
      " ('TV(2.0) Loss', 33.036316)], overall loss: -18.6414260864\n",
      "Iteration: 74, named_losses: [('ActivationMax Loss', -53.485603),\n",
      " ('L-6.0 Norm Loss', 0.061893709),\n",
      " ('TV(2.0) Loss', 33.111656)], overall loss: -20.3120536804\n",
      "Iteration: 75, named_losses: [('ActivationMax Loss', -54.898994),\n",
      " ('L-6.0 Norm Loss', 0.061895926),\n",
      " ('TV(2.0) Loss', 32.140568)], overall loss: -22.6965293884\n",
      "Iteration: 76, named_losses: [('ActivationMax Loss', -54.934818),\n",
      " ('L-6.0 Norm Loss', 0.061899848),\n",
      " ('TV(2.0) Loss', 34.627632)], overall loss: -20.2452850342\n",
      "Iteration: 77, named_losses: [('ActivationMax Loss', -56.31601),\n",
      " ('L-6.0 Norm Loss', 0.061902486),\n",
      " ('TV(2.0) Loss', 32.778198)], overall loss: -23.4759101868\n",
      "Iteration: 78, named_losses: [('ActivationMax Loss', -53.962742),\n",
      " ('L-6.0 Norm Loss', 0.061902851),\n",
      " ('TV(2.0) Loss', 34.602619)], overall loss: -19.2982215881\n",
      "Iteration: 79, named_losses: [('ActivationMax Loss', -57.711037),\n",
      " ('L-6.0 Norm Loss', 0.061906222),\n",
      " ('TV(2.0) Loss', 33.1007)], overall loss: -24.5484313965\n",
      "Iteration: 80, named_losses: [('ActivationMax Loss', -57.343006),\n",
      " ('L-6.0 Norm Loss', 0.061907329),\n",
      " ('TV(2.0) Loss', 34.195686)], overall loss: -23.0854110718\n",
      "Iteration: 81, named_losses: [('ActivationMax Loss', -57.987549),\n",
      " ('L-6.0 Norm Loss', 0.061911486),\n",
      " ('TV(2.0) Loss', 34.674801)], overall loss: -23.2508354187\n",
      "Iteration: 82, named_losses: [('ActivationMax Loss', -58.243034),\n",
      " ('L-6.0 Norm Loss', 0.061911572),\n",
      " ('TV(2.0) Loss', 35.852852)], overall loss: -22.3282699585\n",
      "Iteration: 83, named_losses: [('ActivationMax Loss', -58.164795),\n",
      " ('L-6.0 Norm Loss', 0.061914217),\n",
      " ('TV(2.0) Loss', 34.573292)], overall loss: -23.5295906067\n",
      "Iteration: 84, named_losses: [('ActivationMax Loss', -56.125908),\n",
      " ('L-6.0 Norm Loss', 0.061915576),\n",
      " ('TV(2.0) Loss', 36.036762)], overall loss: -20.0272293091\n",
      "Iteration: 85, named_losses: [('ActivationMax Loss', -58.724846),\n",
      " ('L-6.0 Norm Loss', 0.061918765),\n",
      " ('TV(2.0) Loss', 34.975479)], overall loss: -23.6874465942\n",
      "Iteration: 86, named_losses: [('ActivationMax Loss', -59.214962),\n",
      " ('L-6.0 Norm Loss', 0.061919611),\n",
      " ('TV(2.0) Loss', 35.705399)], overall loss: -23.44764328\n",
      "Iteration: 87, named_losses: [('ActivationMax Loss', -59.618362),\n",
      " ('L-6.0 Norm Loss', 0.061922338),\n",
      " ('TV(2.0) Loss', 35.50737)], overall loss: -24.0490684509\n",
      "Iteration: 88, named_losses: [('ActivationMax Loss', -56.870541),\n",
      " ('L-6.0 Norm Loss', 0.061923005),\n",
      " ('TV(2.0) Loss', 36.305687)], overall loss: -20.5029296875\n",
      "Iteration: 89, named_losses: [('ActivationMax Loss', -61.641098),\n",
      " ('L-6.0 Norm Loss', 0.061927117),\n",
      " ('TV(2.0) Loss', 35.943317)], overall loss: -25.6358528137\n",
      "Iteration: 90, named_losses: [('ActivationMax Loss', -60.660625),\n",
      " ('L-6.0 Norm Loss', 0.061929706),\n",
      " ('TV(2.0) Loss', 37.644867)], overall loss: -22.9538269043\n",
      "Iteration: 91, named_losses: [('ActivationMax Loss', -62.358536),\n",
      " ('L-6.0 Norm Loss', 0.061931904),\n",
      " ('TV(2.0) Loss', 37.847153)], overall loss: -24.4494514465\n",
      "Iteration: 92, named_losses: [('ActivationMax Loss', -61.129471),\n",
      " ('L-6.0 Norm Loss', 0.061932709),\n",
      " ('TV(2.0) Loss', 37.4174)], overall loss: -23.650138855\n",
      "Iteration: 93, named_losses: [('ActivationMax Loss', -65.039703),\n",
      " ('L-6.0 Norm Loss', 0.061934352),\n",
      " ('TV(2.0) Loss', 36.973907)], overall loss: -28.0038604736\n",
      "Iteration: 94, named_losses: [('ActivationMax Loss', -59.182236),\n",
      " ('L-6.0 Norm Loss', 0.061936423),\n",
      " ('TV(2.0) Loss', 38.246948)], overall loss: -20.8733520508\n",
      "Iteration: 95, named_losses: [('ActivationMax Loss', -64.539116),\n",
      " ('L-6.0 Norm Loss', 0.061939154),\n",
      " ('TV(2.0) Loss', 37.030712)], overall loss: -27.4464683533\n",
      "Iteration: 96, named_losses: [('ActivationMax Loss', -63.779083),\n",
      " ('L-6.0 Norm Loss', 0.061940245),\n",
      " ('TV(2.0) Loss', 36.523453)], overall loss: -27.1936912537\n",
      "Iteration: 97, named_losses: [('ActivationMax Loss', -62.718475),\n",
      " ('L-6.0 Norm Loss', 0.061943103),\n",
      " ('TV(2.0) Loss', 38.223194)], overall loss: -24.4333381653\n",
      "Iteration: 98, named_losses: [('ActivationMax Loss', -64.328323),\n",
      " ('L-6.0 Norm Loss', 0.061946243),\n",
      " ('TV(2.0) Loss', 36.81934)], overall loss: -27.4470405579\n",
      "Iteration: 99, named_losses: [('ActivationMax Loss', -61.688591),\n",
      " ('L-6.0 Norm Loss', 0.061947718),\n",
      " ('TV(2.0) Loss', 38.590553)], overall loss: -23.0360908508\n",
      "Iteration: 100, named_losses: [('ActivationMax Loss', -63.715092),\n",
      " ('L-6.0 Norm Loss', 0.061950605),\n",
      " ('TV(2.0) Loss', 37.157112)], overall loss: -26.4960289001\n",
      "Iteration: 101, named_losses: [('ActivationMax Loss', -63.079693),\n",
      " ('L-6.0 Norm Loss', 0.061953332),\n",
      " ('TV(2.0) Loss', 38.64534)], overall loss: -24.3723983765\n",
      "Iteration: 102, named_losses: [('ActivationMax Loss', -67.596748),\n",
      " ('L-6.0 Norm Loss', 0.061954927),\n",
      " ('TV(2.0) Loss', 37.264496)], overall loss: -30.2702941895\n",
      "Iteration: 103, named_losses: [('ActivationMax Loss', -64.665634),\n",
      " ('L-6.0 Norm Loss', 0.061956808),\n",
      " ('TV(2.0) Loss', 39.460297)], overall loss: -25.1433792114\n",
      "Iteration: 104, named_losses: [('ActivationMax Loss', -68.975647),\n",
      " ('L-6.0 Norm Loss', 0.061960772),\n",
      " ('TV(2.0) Loss', 38.939651)], overall loss: -29.9740371704\n",
      "Iteration: 105, named_losses: [('ActivationMax Loss', -64.330437),\n",
      " ('L-6.0 Norm Loss', 0.061962936),\n",
      " ('TV(2.0) Loss', 39.573547)], overall loss: -24.6949234009\n",
      "Iteration: 106, named_losses: [('ActivationMax Loss', -67.531845),\n",
      " ('L-6.0 Norm Loss', 0.061964907),\n",
      " ('TV(2.0) Loss', 38.740841)], overall loss: -28.7290382385\n",
      "Iteration: 107, named_losses: [('ActivationMax Loss', -65.714714),\n",
      " ('L-6.0 Norm Loss', 0.061967168),\n",
      " ('TV(2.0) Loss', 39.831196)], overall loss: -25.8215522766\n",
      "Iteration: 108, named_losses: [('ActivationMax Loss', -69.740456),\n",
      " ('L-6.0 Norm Loss', 0.061969183),\n",
      " ('TV(2.0) Loss', 39.020229)], overall loss: -30.6582603455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 109, named_losses: [('ActivationMax Loss', -67.502937),\n",
      " ('L-6.0 Norm Loss', 0.061970394),\n",
      " ('TV(2.0) Loss', 40.945988)], overall loss: -26.4949760437\n",
      "Iteration: 110, named_losses: [('ActivationMax Loss', -66.85611),\n",
      " ('L-6.0 Norm Loss', 0.061974101),\n",
      " ('TV(2.0) Loss', 39.716278)], overall loss: -27.0778579712\n",
      "Iteration: 111, named_losses: [('ActivationMax Loss', -68.598862),\n",
      " ('L-6.0 Norm Loss', 0.061975479),\n",
      " ('TV(2.0) Loss', 41.146049)], overall loss: -27.390838623\n",
      "Iteration: 112, named_losses: [('ActivationMax Loss', -70.261124),\n",
      " ('L-6.0 Norm Loss', 0.061977986),\n",
      " ('TV(2.0) Loss', 39.597488)], overall loss: -30.6016540527\n",
      "Iteration: 113, named_losses: [('ActivationMax Loss', -66.714874),\n",
      " ('L-6.0 Norm Loss', 0.061979216),\n",
      " ('TV(2.0) Loss', 41.231609)], overall loss: -25.4212837219\n",
      "Iteration: 114, named_losses: [('ActivationMax Loss', -70.259338),\n",
      " ('L-6.0 Norm Loss', 0.061982453),\n",
      " ('TV(2.0) Loss', 40.249741)], overall loss: -29.9476165771\n",
      "Iteration: 115, named_losses: [('ActivationMax Loss', -69.399033),\n",
      " ('L-6.0 Norm Loss', 0.061984587),\n",
      " ('TV(2.0) Loss', 40.335354)], overall loss: -29.0016975403\n",
      "Iteration: 116, named_losses: [('ActivationMax Loss', -71.065346),\n",
      " ('L-6.0 Norm Loss', 0.061987635),\n",
      " ('TV(2.0) Loss', 41.799816)], overall loss: -29.203540802\n",
      "Iteration: 117, named_losses: [('ActivationMax Loss', -72.835297),\n",
      " ('L-6.0 Norm Loss', 0.061989769),\n",
      " ('TV(2.0) Loss', 40.994858)], overall loss: -31.7784500122\n",
      "Iteration: 118, named_losses: [('ActivationMax Loss', -71.935997),\n",
      " ('L-6.0 Norm Loss', 0.061991692),\n",
      " ('TV(2.0) Loss', 42.238888)], overall loss: -29.6351203918\n",
      "Iteration: 119, named_losses: [('ActivationMax Loss', -69.850922),\n",
      " ('L-6.0 Norm Loss', 0.061993204),\n",
      " ('TV(2.0) Loss', 41.699505)], overall loss: -28.0894203186\n",
      "Iteration: 120, named_losses: [('ActivationMax Loss', -75.908012),\n",
      " ('L-6.0 Norm Loss', 0.061996683),\n",
      " ('TV(2.0) Loss', 42.971413)], overall loss: -32.8746032715\n",
      "Iteration: 121, named_losses: [('ActivationMax Loss', -72.654961),\n",
      " ('L-6.0 Norm Loss', 0.061998829),\n",
      " ('TV(2.0) Loss', 42.499371)], overall loss: -30.0935935974\n",
      "Iteration: 122, named_losses: [('ActivationMax Loss', -75.435516),\n",
      " ('L-6.0 Norm Loss', 0.062002096),\n",
      " ('TV(2.0) Loss', 43.51696)], overall loss: -31.856552124\n",
      "Iteration: 123, named_losses: [('ActivationMax Loss', -72.926193),\n",
      " ('L-6.0 Norm Loss', 0.062002286),\n",
      " ('TV(2.0) Loss', 43.371983)], overall loss: -29.4922065735\n",
      "Iteration: 124, named_losses: [('ActivationMax Loss', -75.617188),\n",
      " ('L-6.0 Norm Loss', 0.062004235),\n",
      " ('TV(2.0) Loss', 42.084713)], overall loss: -33.4704704285\n",
      "Iteration: 125, named_losses: [('ActivationMax Loss', -73.594917),\n",
      " ('L-6.0 Norm Loss', 0.062007267),\n",
      " ('TV(2.0) Loss', 41.617821)], overall loss: -31.9150924683\n",
      "Iteration: 126, named_losses: [('ActivationMax Loss', -73.42215),\n",
      " ('L-6.0 Norm Loss', 0.062009893),\n",
      " ('TV(2.0) Loss', 42.667156)], overall loss: -30.69298172\n",
      "Iteration: 127, named_losses: [('ActivationMax Loss', -75.425133),\n",
      " ('L-6.0 Norm Loss', 0.062012836),\n",
      " ('TV(2.0) Loss', 41.357712)], overall loss: -34.0054092407\n",
      "Iteration: 128, named_losses: [('ActivationMax Loss', -74.973259),\n",
      " ('L-6.0 Norm Loss', 0.062015209),\n",
      " ('TV(2.0) Loss', 43.628525)], overall loss: -31.2827224731\n",
      "Iteration: 129, named_losses: [('ActivationMax Loss', -74.783409),\n",
      " ('L-6.0 Norm Loss', 0.062016975),\n",
      " ('TV(2.0) Loss', 42.33416)], overall loss: -32.3872299194\n",
      "Iteration: 130, named_losses: [('ActivationMax Loss', -74.920242),\n",
      " ('L-6.0 Norm Loss', 0.062019497),\n",
      " ('TV(2.0) Loss', 43.562069)], overall loss: -31.2961540222\n",
      "Iteration: 131, named_losses: [('ActivationMax Loss', -74.740448),\n",
      " ('L-6.0 Norm Loss', 0.062022813),\n",
      " ('TV(2.0) Loss', 42.652668)], overall loss: -32.0257606506\n",
      "Iteration: 132, named_losses: [('ActivationMax Loss', -75.096954),\n",
      " ('L-6.0 Norm Loss', 0.062026158),\n",
      " ('TV(2.0) Loss', 44.228703)], overall loss: -30.806224823\n",
      "Iteration: 133, named_losses: [('ActivationMax Loss', -77.228172),\n",
      " ('L-6.0 Norm Loss', 0.062028356),\n",
      " ('TV(2.0) Loss', 42.727112)], overall loss: -34.4390335083\n",
      "Iteration: 134, named_losses: [('ActivationMax Loss', -74.963516),\n",
      " ('L-6.0 Norm Loss', 0.0620302),\n",
      " ('TV(2.0) Loss', 44.56879)], overall loss: -30.332698822\n",
      "Iteration: 135, named_losses: [('ActivationMax Loss', -79.792282),\n",
      " ('L-6.0 Norm Loss', 0.0620308),\n",
      " ('TV(2.0) Loss', 43.277798)], overall loss: -36.4524497986\n",
      "Iteration: 136, named_losses: [('ActivationMax Loss', -79.992973),\n",
      " ('L-6.0 Norm Loss', 0.062034879),\n",
      " ('TV(2.0) Loss', 44.352303)], overall loss: -35.5786361694\n",
      "Iteration: 137, named_losses: [('ActivationMax Loss', -79.821152),\n",
      " ('L-6.0 Norm Loss', 0.062036708),\n",
      " ('TV(2.0) Loss', 43.979073)], overall loss: -35.7800445557\n",
      "Iteration: 138, named_losses: [('ActivationMax Loss', -78.841103),\n",
      " ('L-6.0 Norm Loss', 0.062039301),\n",
      " ('TV(2.0) Loss', 44.952309)], overall loss: -33.826751709\n",
      "Iteration: 139, named_losses: [('ActivationMax Loss', -80.606247),\n",
      " ('L-6.0 Norm Loss', 0.06204002),\n",
      " ('TV(2.0) Loss', 44.716331)], overall loss: -35.82787323\n",
      "Iteration: 140, named_losses: [('ActivationMax Loss', -76.75869),\n",
      " ('L-6.0 Norm Loss', 0.062042855),\n",
      " ('TV(2.0) Loss', 45.755634)], overall loss: -30.9410133362\n",
      "Iteration: 141, named_losses: [('ActivationMax Loss', -83.319611),\n",
      " ('L-6.0 Norm Loss', 0.06204576),\n",
      " ('TV(2.0) Loss', 44.883099)], overall loss: -38.3744697571\n",
      "Iteration: 142, named_losses: [('ActivationMax Loss', -79.541634),\n",
      " ('L-6.0 Norm Loss', 0.06204699),\n",
      " ('TV(2.0) Loss', 45.271133)], overall loss: -34.2084503174\n",
      "Iteration: 143, named_losses: [('ActivationMax Loss', -80.731552),\n",
      " ('L-6.0 Norm Loss', 0.062049665),\n",
      " ('TV(2.0) Loss', 45.206139)], overall loss: -35.4633636475\n",
      "Iteration: 144, named_losses: [('ActivationMax Loss', -79.980911),\n",
      " ('L-6.0 Norm Loss', 0.062052872),\n",
      " ('TV(2.0) Loss', 44.870697)], overall loss: -35.0481643677\n",
      "Iteration: 145, named_losses: [('ActivationMax Loss', -81.328354),\n",
      " ('L-6.0 Norm Loss', 0.062056288),\n",
      " ('TV(2.0) Loss', 45.792099)], overall loss: -35.4741973877\n",
      "Iteration: 146, named_losses: [('ActivationMax Loss', -81.544037),\n",
      " ('L-6.0 Norm Loss', 0.062058225),\n",
      " ('TV(2.0) Loss', 45.111961)], overall loss: -36.3700180054\n",
      "Iteration: 147, named_losses: [('ActivationMax Loss', -81.074211),\n",
      " ('L-6.0 Norm Loss', 0.062061563),\n",
      " ('TV(2.0) Loss', 46.412197)], overall loss: -34.5999488831\n",
      "Iteration: 148, named_losses: [('ActivationMax Loss', -80.086357),\n",
      " ('L-6.0 Norm Loss', 0.062064234),\n",
      " ('TV(2.0) Loss', 45.670845)], overall loss: -34.3534469604\n",
      "Iteration: 149, named_losses: [('ActivationMax Loss', -82.843842),\n",
      " ('L-6.0 Norm Loss', 0.06206771),\n",
      " ('TV(2.0) Loss', 45.885696)], overall loss: -36.8960800171\n",
      "Iteration: 150, named_losses: [('ActivationMax Loss', -78.639267),\n",
      " ('L-6.0 Norm Loss', 0.062069252),\n",
      " ('TV(2.0) Loss', 46.430859)], overall loss: -32.1463356018\n",
      "Iteration: 151, named_losses: [('ActivationMax Loss', -80.162636),\n",
      " ('L-6.0 Norm Loss', 0.062073454),\n",
      " ('TV(2.0) Loss', 46.224102)], overall loss: -33.8764610291\n",
      "Iteration: 152, named_losses: [('ActivationMax Loss', -83.380333),\n",
      " ('L-6.0 Norm Loss', 0.062075607),\n",
      " ('TV(2.0) Loss', 45.654049)], overall loss: -37.6642112732\n",
      "Iteration: 153, named_losses: [('ActivationMax Loss', -79.667259),\n",
      " ('L-6.0 Norm Loss', 0.062078506),\n",
      " ('TV(2.0) Loss', 46.391674)], overall loss: -33.2135047913\n",
      "Iteration: 154, named_losses: [('ActivationMax Loss', -85.381157),\n",
      " ('L-6.0 Norm Loss', 0.062079921),\n",
      " ('TV(2.0) Loss', 45.745384)], overall loss: -39.5736923218\n",
      "Iteration: 155, named_losses: [('ActivationMax Loss', -83.609505),\n",
      " ('L-6.0 Norm Loss', 0.062081322),\n",
      " ('TV(2.0) Loss', 45.66774)], overall loss: -37.8796844482\n",
      "Iteration: 156, named_losses: [('ActivationMax Loss', -82.389984),\n",
      " ('L-6.0 Norm Loss', 0.062085431),\n",
      " ('TV(2.0) Loss', 45.546261)], overall loss: -36.7816352844\n",
      "Iteration: 157, named_losses: [('ActivationMax Loss', -85.263351),\n",
      " ('L-6.0 Norm Loss', 0.062087107),\n",
      " ('TV(2.0) Loss', 46.222122)], overall loss: -38.9791412354\n",
      "Iteration: 158, named_losses: [('ActivationMax Loss', -84.586708),\n",
      " ('L-6.0 Norm Loss', 0.062091459),\n",
      " ('TV(2.0) Loss', 46.984322)], overall loss: -37.5402984619\n",
      "Iteration: 159, named_losses: [('ActivationMax Loss', -84.094154),\n",
      " ('L-6.0 Norm Loss', 0.062092349),\n",
      " ('TV(2.0) Loss', 47.529602)], overall loss: -36.502456665\n",
      "Iteration: 160, named_losses: [('ActivationMax Loss', -83.759598),\n",
      " ('L-6.0 Norm Loss', 0.062095713),\n",
      " ('TV(2.0) Loss', 47.066273)], overall loss: -36.6312294006\n",
      "Iteration: 161, named_losses: [('ActivationMax Loss', -87.085777),\n",
      " ('L-6.0 Norm Loss', 0.062097646),\n",
      " ('TV(2.0) Loss', 47.431206)], overall loss: -39.5924758911\n",
      "Iteration: 162, named_losses: [('ActivationMax Loss', -85.065201),\n",
      " ('L-6.0 Norm Loss', 0.062101152),\n",
      " ('TV(2.0) Loss', 47.21352)], overall loss: -37.7895774841\n",
      "Iteration: 163, named_losses: [('ActivationMax Loss', -89.332809),\n",
      " ('L-6.0 Norm Loss', 0.062102411),\n",
      " ('TV(2.0) Loss', 47.742847)], overall loss: -41.5278587341\n",
      "Iteration: 164, named_losses: [('ActivationMax Loss', -84.093758),\n",
      " ('L-6.0 Norm Loss', 0.062105823),\n",
      " ('TV(2.0) Loss', 48.421814)], overall loss: -35.6098403931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 165, named_losses: [('ActivationMax Loss', -85.390877),\n",
      " ('L-6.0 Norm Loss', 0.062107254),\n",
      " ('TV(2.0) Loss', 48.512482)], overall loss: -36.8162841797\n",
      "Iteration: 166, named_losses: [('ActivationMax Loss', -89.280769),\n",
      " ('L-6.0 Norm Loss', 0.062109586),\n",
      " ('TV(2.0) Loss', 47.733635)], overall loss: -41.4850234985\n",
      "Iteration: 167, named_losses: [('ActivationMax Loss', -86.773773),\n",
      " ('L-6.0 Norm Loss', 0.062111802),\n",
      " ('TV(2.0) Loss', 48.405296)], overall loss: -38.3063659668\n",
      "Iteration: 168, named_losses: [('ActivationMax Loss', -89.266304),\n",
      " ('L-6.0 Norm Loss', 0.062114853),\n",
      " ('TV(2.0) Loss', 47.790852)], overall loss: -41.4133338928\n",
      "Iteration: 169, named_losses: [('ActivationMax Loss', -85.502991),\n",
      " ('L-6.0 Norm Loss', 0.062117644),\n",
      " ('TV(2.0) Loss', 49.425331)], overall loss: -36.0155410767\n",
      "Iteration: 170, named_losses: [('ActivationMax Loss', -85.752541),\n",
      " ('L-6.0 Norm Loss', 0.062120721),\n",
      " ('TV(2.0) Loss', 48.492397)], overall loss: -37.1980247498\n",
      "Iteration: 171, named_losses: [('ActivationMax Loss', -89.4711),\n",
      " ('L-6.0 Norm Loss', 0.06212277),\n",
      " ('TV(2.0) Loss', 48.197517)], overall loss: -41.2114562988\n",
      "Iteration: 172, named_losses: [('ActivationMax Loss', -84.256325),\n",
      " ('L-6.0 Norm Loss', 0.062125843),\n",
      " ('TV(2.0) Loss', 48.419376)], overall loss: -35.7748222351\n",
      "Iteration: 173, named_losses: [('ActivationMax Loss', -89.153397),\n",
      " ('L-6.0 Norm Loss', 0.062128186),\n",
      " ('TV(2.0) Loss', 48.649033)], overall loss: -40.442237854\n",
      "Iteration: 174, named_losses: [('ActivationMax Loss', -89.42482),\n",
      " ('L-6.0 Norm Loss', 0.062131628),\n",
      " ('TV(2.0) Loss', 48.84708)], overall loss: -40.5156059265\n",
      "Iteration: 175, named_losses: [('ActivationMax Loss', -89.780655),\n",
      " ('L-6.0 Norm Loss', 0.062133592),\n",
      " ('TV(2.0) Loss', 48.559395)], overall loss: -41.1591262817\n",
      "Iteration: 176, named_losses: [('ActivationMax Loss', -87.505371),\n",
      " ('L-6.0 Norm Loss', 0.062137157),\n",
      " ('TV(2.0) Loss', 49.34486)], overall loss: -38.0983772278\n",
      "Iteration: 177, named_losses: [('ActivationMax Loss', -90.80497),\n",
      " ('L-6.0 Norm Loss', 0.062139709),\n",
      " ('TV(2.0) Loss', 48.458553)], overall loss: -42.2842750549\n",
      "Iteration: 178, named_losses: [('ActivationMax Loss', -90.880226),\n",
      " ('L-6.0 Norm Loss', 0.062140673),\n",
      " ('TV(2.0) Loss', 50.447983)], overall loss: -40.3701019287\n",
      "Iteration: 179, named_losses: [('ActivationMax Loss', -88.743202),\n",
      " ('L-6.0 Norm Loss', 0.062143836),\n",
      " ('TV(2.0) Loss', 48.893677)], overall loss: -39.7873840332\n",
      "Iteration: 180, named_losses: [('ActivationMax Loss', -92.292145),\n",
      " ('L-6.0 Norm Loss', 0.062145084),\n",
      " ('TV(2.0) Loss', 49.529144)], overall loss: -42.7008590698\n",
      "Iteration: 181, named_losses: [('ActivationMax Loss', -90.334358),\n",
      " ('L-6.0 Norm Loss', 0.062147133),\n",
      " ('TV(2.0) Loss', 49.376839)], overall loss: -40.8953704834\n",
      "Iteration: 182, named_losses: [('ActivationMax Loss', -93.505615),\n",
      " ('L-6.0 Norm Loss', 0.062149204),\n",
      " ('TV(2.0) Loss', 49.574623)], overall loss: -43.8688430786\n",
      "Iteration: 183, named_losses: [('ActivationMax Loss', -89.368149),\n",
      " ('L-6.0 Norm Loss', 0.062151831),\n",
      " ('TV(2.0) Loss', 50.479847)], overall loss: -38.8261528015\n",
      "Iteration: 184, named_losses: [('ActivationMax Loss', -91.670448),\n",
      " ('L-6.0 Norm Loss', 0.062155567),\n",
      " ('TV(2.0) Loss', 50.49128)], overall loss: -41.1170120239\n",
      "Iteration: 185, named_losses: [('ActivationMax Loss', -91.845055),\n",
      " ('L-6.0 Norm Loss', 0.062158324),\n",
      " ('TV(2.0) Loss', 50.634842)], overall loss: -41.1480560303\n",
      "Iteration: 186, named_losses: [('ActivationMax Loss', -93.885223),\n",
      " ('L-6.0 Norm Loss', 0.062160846),\n",
      " ('TV(2.0) Loss', 51.10173)], overall loss: -42.7213287354\n",
      "Iteration: 187, named_losses: [('ActivationMax Loss', -87.125481),\n",
      " ('L-6.0 Norm Loss', 0.062164038),\n",
      " ('TV(2.0) Loss', 51.786007)], overall loss: -35.2773094177\n",
      "Iteration: 188, named_losses: [('ActivationMax Loss', -94.432861),\n",
      " ('L-6.0 Norm Loss', 0.062165372),\n",
      " ('TV(2.0) Loss', 50.792919)], overall loss: -43.5777778625\n",
      "Iteration: 189, named_losses: [('ActivationMax Loss', -92.998787),\n",
      " ('L-6.0 Norm Loss', 0.062168516),\n",
      " ('TV(2.0) Loss', 50.617378)], overall loss: -42.3192367554\n",
      "Iteration: 190, named_losses: [('ActivationMax Loss', -94.037918),\n",
      " ('L-6.0 Norm Loss', 0.062169418),\n",
      " ('TV(2.0) Loss', 50.953403)], overall loss: -43.0223426819\n",
      "Iteration: 191, named_losses: [('ActivationMax Loss', -92.43676),\n",
      " ('L-6.0 Norm Loss', 0.062173188),\n",
      " ('TV(2.0) Loss', 50.758865)], overall loss: -41.6157226562\n",
      "Iteration: 192, named_losses: [('ActivationMax Loss', -94.432266),\n",
      " ('L-6.0 Norm Loss', 0.06217441),\n",
      " ('TV(2.0) Loss', 51.39883)], overall loss: -42.9712638855\n",
      "Iteration: 193, named_losses: [('ActivationMax Loss', -94.284225),\n",
      " ('L-6.0 Norm Loss', 0.062178232),\n",
      " ('TV(2.0) Loss', 50.184383)], overall loss: -44.0376625061\n",
      "Iteration: 194, named_losses: [('ActivationMax Loss', -95.05014),\n",
      " ('L-6.0 Norm Loss', 0.062180407),\n",
      " ('TV(2.0) Loss', 51.994118)], overall loss: -42.9938430786\n",
      "Iteration: 195, named_losses: [('ActivationMax Loss', -92.610092),\n",
      " ('L-6.0 Norm Loss', 0.062183313),\n",
      " ('TV(2.0) Loss', 50.379807)], overall loss: -42.1681060791\n",
      "Iteration: 196, named_losses: [('ActivationMax Loss', -93.360321),\n",
      " ('L-6.0 Norm Loss', 0.062185787),\n",
      " ('TV(2.0) Loss', 52.400833)], overall loss: -40.8973007202\n",
      "Iteration: 197, named_losses: [('ActivationMax Loss', -95.436844),\n",
      " ('L-6.0 Norm Loss', 0.062187679),\n",
      " ('TV(2.0) Loss', 50.404907)], overall loss: -44.9697494507\n",
      "Iteration: 198, named_losses: [('ActivationMax Loss', -95.442024),\n",
      " ('L-6.0 Norm Loss', 0.062191367),\n",
      " ('TV(2.0) Loss', 51.862446)], overall loss: -43.5173835754\n",
      "Iteration: 199, named_losses: [('ActivationMax Loss', -94.860168),\n",
      " ('L-6.0 Norm Loss', 0.06219282),\n",
      " ('TV(2.0) Loss', 50.87299)], overall loss: -43.9249839783\n",
      "Iteration: 200, named_losses: [('ActivationMax Loss', -95.843559),\n",
      " ('L-6.0 Norm Loss', 0.06219532),\n",
      " ('TV(2.0) Loss', 52.328056)], overall loss: -43.4533081055\n",
      "Iteration: 201, named_losses: [('ActivationMax Loss', -97.020523),\n",
      " ('L-6.0 Norm Loss', 0.062198706),\n",
      " ('TV(2.0) Loss', 51.485229)], overall loss: -45.4730911255\n",
      "Iteration: 202, named_losses: [('ActivationMax Loss', -98.151337),\n",
      " ('L-6.0 Norm Loss', 0.062202491),\n",
      " ('TV(2.0) Loss', 53.322594)], overall loss: -44.7665405273\n",
      "Iteration: 203, named_losses: [('ActivationMax Loss', -95.426918),\n",
      " ('L-6.0 Norm Loss', 0.062204488),\n",
      " ('TV(2.0) Loss', 53.215027)], overall loss: -42.1496887207\n",
      "Iteration: 204, named_losses: [('ActivationMax Loss', -98.068665),\n",
      " ('L-6.0 Norm Loss', 0.062205907),\n",
      " ('TV(2.0) Loss', 54.215515)], overall loss: -43.7909469604\n",
      "Iteration: 205, named_losses: [('ActivationMax Loss', -95.863991),\n",
      " ('L-6.0 Norm Loss', 0.062209398),\n",
      " ('TV(2.0) Loss', 53.636875)], overall loss: -42.1649055481\n",
      "Iteration: 206, named_losses: [('ActivationMax Loss', -98.729401),\n",
      " ('L-6.0 Norm Loss', 0.062210523),\n",
      " ('TV(2.0) Loss', 54.282337)], overall loss: -44.384853363\n",
      "Iteration: 207, named_losses: [('ActivationMax Loss', -96.110382),\n",
      " ('L-6.0 Norm Loss', 0.06221268),\n",
      " ('TV(2.0) Loss', 53.255119)], overall loss: -42.7930526733\n",
      "Iteration: 208, named_losses: [('ActivationMax Loss', -98.217163),\n",
      " ('L-6.0 Norm Loss', 0.062215261),\n",
      " ('TV(2.0) Loss', 53.936409)], overall loss: -44.218536377\n",
      "Iteration: 209, named_losses: [('ActivationMax Loss', -101.32747),\n",
      " ('L-6.0 Norm Loss', 0.062217344),\n",
      " ('TV(2.0) Loss', 52.384407)], overall loss: -48.8808441162\n",
      "Iteration: 210, named_losses: [('ActivationMax Loss', -102.2364),\n",
      " ('L-6.0 Norm Loss', 0.062220935),\n",
      " ('TV(2.0) Loss', 54.511322)], overall loss: -47.6628646851\n",
      "Iteration: 211, named_losses: [('ActivationMax Loss', -99.871643),\n",
      " ('L-6.0 Norm Loss', 0.062223282),\n",
      " ('TV(2.0) Loss', 53.764351)], overall loss: -46.0450668335\n",
      "Iteration: 212, named_losses: [('ActivationMax Loss', -99.592453),\n",
      " ('L-6.0 Norm Loss', 0.062226798),\n",
      " ('TV(2.0) Loss', 54.300797)], overall loss: -45.2294311523\n",
      "Iteration: 213, named_losses: [('ActivationMax Loss', -101.54116),\n",
      " ('L-6.0 Norm Loss', 0.062228646),\n",
      " ('TV(2.0) Loss', 54.218273)], overall loss: -47.2606620789\n",
      "Iteration: 214, named_losses: [('ActivationMax Loss', -99.221146),\n",
      " ('L-6.0 Norm Loss', 0.062231064),\n",
      " ('TV(2.0) Loss', 53.949696)], overall loss: -45.2092170715\n",
      "Iteration: 215, named_losses: [('ActivationMax Loss', -101.65981),\n",
      " ('L-6.0 Norm Loss', 0.062233083),\n",
      " ('TV(2.0) Loss', 53.531853)], overall loss: -48.0657196045\n",
      "Iteration: 216, named_losses: [('ActivationMax Loss', -100.08763),\n",
      " ('L-6.0 Norm Loss', 0.06223705),\n",
      " ('TV(2.0) Loss', 54.031837)], overall loss: -45.9935531616\n",
      "Iteration: 217, named_losses: [('ActivationMax Loss', -100.44298),\n",
      " ('L-6.0 Norm Loss', 0.06223971),\n",
      " ('TV(2.0) Loss', 53.505089)], overall loss: -46.8756484985\n",
      "Iteration: 218, named_losses: [('ActivationMax Loss', -99.539574),\n",
      " ('L-6.0 Norm Loss', 0.06224139),\n",
      " ('TV(2.0) Loss', 54.246307)], overall loss: -45.2310256958\n",
      "Iteration: 219, named_losses: [('ActivationMax Loss', -100.79933),\n",
      " ('L-6.0 Norm Loss', 0.062244087),\n",
      " ('TV(2.0) Loss', 53.78986)], overall loss: -46.9472312927\n",
      "Iteration: 220, named_losses: [('ActivationMax Loss', -102.77279),\n",
      " ('L-6.0 Norm Loss', 0.062245931),\n",
      " ('TV(2.0) Loss', 53.56958)], overall loss: -49.1409606934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 221, named_losses: [('ActivationMax Loss', -99.615746),\n",
      " ('L-6.0 Norm Loss', 0.062250301),\n",
      " ('TV(2.0) Loss', 54.42424)], overall loss: -45.1292572021\n",
      "Iteration: 222, named_losses: [('ActivationMax Loss', -103.13307),\n",
      " ('L-6.0 Norm Loss', 0.062251575),\n",
      " ('TV(2.0) Loss', 54.386383)], overall loss: -48.6844406128\n",
      "Iteration: 223, named_losses: [('ActivationMax Loss', -96.229507),\n",
      " ('L-6.0 Norm Loss', 0.062256515),\n",
      " ('TV(2.0) Loss', 56.048912)], overall loss: -40.1183395386\n",
      "Iteration: 224, named_losses: [('ActivationMax Loss', -104.89292),\n",
      " ('L-6.0 Norm Loss', 0.062258039),\n",
      " ('TV(2.0) Loss', 55.30854)], overall loss: -49.5221252441\n",
      "Iteration: 225, named_losses: [('ActivationMax Loss', -97.592621),\n",
      " ('L-6.0 Norm Loss', 0.062260509),\n",
      " ('TV(2.0) Loss', 55.895943)], overall loss: -41.6344146729\n",
      "Iteration: 226, named_losses: [('ActivationMax Loss', -107.39828),\n",
      " ('L-6.0 Norm Loss', 0.062263034),\n",
      " ('TV(2.0) Loss', 55.099182)], overall loss: -52.2368392944\n",
      "Iteration: 227, named_losses: [('ActivationMax Loss', -102.12806),\n",
      " ('L-6.0 Norm Loss', 0.062265098),\n",
      " ('TV(2.0) Loss', 56.022156)], overall loss: -46.0436401367\n",
      "Iteration: 228, named_losses: [('ActivationMax Loss', -101.97128),\n",
      " ('L-6.0 Norm Loss', 0.062269926),\n",
      " ('TV(2.0) Loss', 55.481373)], overall loss: -46.4276313782\n",
      "Iteration: 229, named_losses: [('ActivationMax Loss', -99.648331),\n",
      " ('L-6.0 Norm Loss', 0.06227085),\n",
      " ('TV(2.0) Loss', 56.001991)], overall loss: -43.5840682983\n",
      "Iteration: 230, named_losses: [('ActivationMax Loss', -105.40375),\n",
      " ('L-6.0 Norm Loss', 0.062272847),\n",
      " ('TV(2.0) Loss', 55.378609)], overall loss: -49.9628677368\n",
      "Iteration: 231, named_losses: [('ActivationMax Loss', -99.835266),\n",
      " ('L-6.0 Norm Loss', 0.062275451),\n",
      " ('TV(2.0) Loss', 55.077816)], overall loss: -44.6951713562\n",
      "Iteration: 232, named_losses: [('ActivationMax Loss', -102.91353),\n",
      " ('L-6.0 Norm Loss', 0.062278029),\n",
      " ('TV(2.0) Loss', 56.010662)], overall loss: -46.840587616\n",
      "Iteration: 233, named_losses: [('ActivationMax Loss', -102.78975),\n",
      " ('L-6.0 Norm Loss', 0.062280152),\n",
      " ('TV(2.0) Loss', 55.402893)], overall loss: -47.3245773315\n",
      "Iteration: 234, named_losses: [('ActivationMax Loss', -104.81158),\n",
      " ('L-6.0 Norm Loss', 0.062282369),\n",
      " ('TV(2.0) Loss', 57.238579)], overall loss: -47.5107269287\n",
      "Iteration: 235, named_losses: [('ActivationMax Loss', -105.24297),\n",
      " ('L-6.0 Norm Loss', 0.062285244),\n",
      " ('TV(2.0) Loss', 55.808445)], overall loss: -49.3722343445\n",
      "Iteration: 236, named_losses: [('ActivationMax Loss', -104.96114),\n",
      " ('L-6.0 Norm Loss', 0.062287703),\n",
      " ('TV(2.0) Loss', 57.81691)], overall loss: -47.0819396973\n",
      "Iteration: 237, named_losses: [('ActivationMax Loss', -102.74883),\n",
      " ('L-6.0 Norm Loss', 0.06228995),\n",
      " ('TV(2.0) Loss', 56.162151)], overall loss: -46.5243873596\n",
      "Iteration: 238, named_losses: [('ActivationMax Loss', -102.88896),\n",
      " ('L-6.0 Norm Loss', 0.062293455),\n",
      " ('TV(2.0) Loss', 58.491325)], overall loss: -44.3353424072\n",
      "Iteration: 239, named_losses: [('ActivationMax Loss', -104.80802),\n",
      " ('L-6.0 Norm Loss', 0.062297061),\n",
      " ('TV(2.0) Loss', 56.09906)], overall loss: -48.6466674805\n",
      "Iteration: 240, named_losses: [('ActivationMax Loss', -107.4565),\n",
      " ('L-6.0 Norm Loss', 0.062297102),\n",
      " ('TV(2.0) Loss', 57.593307)], overall loss: -49.8009033203\n",
      "Iteration: 241, named_losses: [('ActivationMax Loss', -103.24889),\n",
      " ('L-6.0 Norm Loss', 0.062300507),\n",
      " ('TV(2.0) Loss', 56.637703)], overall loss: -46.5488815308\n",
      "Iteration: 242, named_losses: [('ActivationMax Loss', -104.22279),\n",
      " ('L-6.0 Norm Loss', 0.062303975),\n",
      " ('TV(2.0) Loss', 58.854889)], overall loss: -45.3056030273\n",
      "Iteration: 243, named_losses: [('ActivationMax Loss', -106.06935),\n",
      " ('L-6.0 Norm Loss', 0.062306654),\n",
      " ('TV(2.0) Loss', 57.842873)], overall loss: -48.1641693115\n",
      "Iteration: 244, named_losses: [('ActivationMax Loss', -107.12758),\n",
      " ('L-6.0 Norm Loss', 0.062309526),\n",
      " ('TV(2.0) Loss', 57.721554)], overall loss: -49.3437156677\n",
      "Iteration: 245, named_losses: [('ActivationMax Loss', -110.45367),\n",
      " ('L-6.0 Norm Loss', 0.06231118),\n",
      " ('TV(2.0) Loss', 56.592937)], overall loss: -53.7984199524\n",
      "Iteration: 246, named_losses: [('ActivationMax Loss', -105.25899),\n",
      " ('L-6.0 Norm Loss', 0.062314849),\n",
      " ('TV(2.0) Loss', 59.685715)], overall loss: -45.5109558105\n",
      "Iteration: 247, named_losses: [('ActivationMax Loss', -106.68048),\n",
      " ('L-6.0 Norm Loss', 0.06231707),\n",
      " ('TV(2.0) Loss', 56.97971)], overall loss: -49.6384544373\n",
      "Iteration: 248, named_losses: [('ActivationMax Loss', -107.46439),\n",
      " ('L-6.0 Norm Loss', 0.062318288),\n",
      " ('TV(2.0) Loss', 58.886314)], overall loss: -48.5157546997\n",
      "Iteration: 249, named_losses: [('ActivationMax Loss', -107.52404),\n",
      " ('L-6.0 Norm Loss', 0.062321611),\n",
      " ('TV(2.0) Loss', 57.354012)], overall loss: -50.1077041626\n",
      "Iteration: 250, named_losses: [('ActivationMax Loss', -105.10503),\n",
      " ('L-6.0 Norm Loss', 0.062323544),\n",
      " ('TV(2.0) Loss', 58.235168)], overall loss: -46.8075332642\n",
      "Iteration: 251, named_losses: [('ActivationMax Loss', -110.02692),\n",
      " ('L-6.0 Norm Loss', 0.06232591),\n",
      " ('TV(2.0) Loss', 57.247654)], overall loss: -52.7169456482\n",
      "Iteration: 252, named_losses: [('ActivationMax Loss', -109.05452),\n",
      " ('L-6.0 Norm Loss', 0.062328301),\n",
      " ('TV(2.0) Loss', 58.03125)], overall loss: -50.9609451294\n",
      "Iteration: 253, named_losses: [('ActivationMax Loss', -110.25841),\n",
      " ('L-6.0 Norm Loss', 0.062332455),\n",
      " ('TV(2.0) Loss', 58.473946)], overall loss: -51.7221298218\n",
      "Iteration: 254, named_losses: [('ActivationMax Loss', -109.35121),\n",
      " ('L-6.0 Norm Loss', 0.062333677),\n",
      " ('TV(2.0) Loss', 59.535042)], overall loss: -49.7538375854\n",
      "Iteration: 255, named_losses: [('ActivationMax Loss', -108.26953),\n",
      " ('L-6.0 Norm Loss', 0.062336184),\n",
      " ('TV(2.0) Loss', 58.487885)], overall loss: -49.7193069458\n",
      "Iteration: 256, named_losses: [('ActivationMax Loss', -111.23071),\n",
      " ('L-6.0 Norm Loss', 0.062341016),\n",
      " ('TV(2.0) Loss', 58.887402)], overall loss: -52.2809638977\n",
      "Iteration: 257, named_losses: [('ActivationMax Loss', -106.85623),\n",
      " ('L-6.0 Norm Loss', 0.062343534),\n",
      " ('TV(2.0) Loss', 59.184872)], overall loss: -47.6090202332\n",
      "Iteration: 258, named_losses: [('ActivationMax Loss', -109.48892),\n",
      " ('L-6.0 Norm Loss', 0.062346313),\n",
      " ('TV(2.0) Loss', 58.449951)], overall loss: -50.9766235352\n",
      "Iteration: 259, named_losses: [('ActivationMax Loss', -105.8782),\n",
      " ('L-6.0 Norm Loss', 0.062348217),\n",
      " ('TV(2.0) Loss', 58.297905)], overall loss: -47.5179519653\n",
      "Iteration: 260, named_losses: [('ActivationMax Loss', -109.84384),\n",
      " ('L-6.0 Norm Loss', 0.062351409),\n",
      " ('TV(2.0) Loss', 57.896324)], overall loss: -51.8851623535\n",
      "Iteration: 261, named_losses: [('ActivationMax Loss', -105.68251),\n",
      " ('L-6.0 Norm Loss', 0.062352248),\n",
      " ('TV(2.0) Loss', 59.137436)], overall loss: -46.4827194214\n",
      "Iteration: 262, named_losses: [('ActivationMax Loss', -106.68652),\n",
      " ('L-6.0 Norm Loss', 0.062354982),\n",
      " ('TV(2.0) Loss', 58.494118)], overall loss: -48.1300506592\n",
      "Iteration: 263, named_losses: [('ActivationMax Loss', -109.27451),\n",
      " ('L-6.0 Norm Loss', 0.062356852),\n",
      " ('TV(2.0) Loss', 58.842148)], overall loss: -50.3700027466\n",
      "Iteration: 264, named_losses: [('ActivationMax Loss', -110.98386),\n",
      " ('L-6.0 Norm Loss', 0.062358826),\n",
      " ('TV(2.0) Loss', 57.431625)], overall loss: -53.4898834229\n",
      "Iteration: 265, named_losses: [('ActivationMax Loss', -109.06016),\n",
      " ('L-6.0 Norm Loss', 0.062362604),\n",
      " ('TV(2.0) Loss', 59.318092)], overall loss: -49.6797027588\n",
      "Iteration: 266, named_losses: [('ActivationMax Loss', -113.14455),\n",
      " ('L-6.0 Norm Loss', 0.062365636),\n",
      " ('TV(2.0) Loss', 58.969494)], overall loss: -54.1126976013\n",
      "Iteration: 267, named_losses: [('ActivationMax Loss', -107.90953),\n",
      " ('L-6.0 Norm Loss', 0.062368978),\n",
      " ('TV(2.0) Loss', 59.832138)], overall loss: -48.0150222778\n",
      "Iteration: 268, named_losses: [('ActivationMax Loss', -112.32379),\n",
      " ('L-6.0 Norm Loss', 0.062371835),\n",
      " ('TV(2.0) Loss', 58.449043)], overall loss: -53.8123779297\n",
      "Iteration: 269, named_losses: [('ActivationMax Loss', -105.80854),\n",
      " ('L-6.0 Norm Loss', 0.062376611),\n",
      " ('TV(2.0) Loss', 60.501823)], overall loss: -45.2443389893\n",
      "Iteration: 270, named_losses: [('ActivationMax Loss', -114.73972),\n",
      " ('L-6.0 Norm Loss', 0.062378291),\n",
      " ('TV(2.0) Loss', 57.394127)], overall loss: -57.2832107544\n",
      "Iteration: 271, named_losses: [('ActivationMax Loss', -112.65745),\n",
      " ('L-6.0 Norm Loss', 0.062381741),\n",
      " ('TV(2.0) Loss', 60.287937)], overall loss: -52.3071327209\n",
      "Iteration: 272, named_losses: [('ActivationMax Loss', -110.16896),\n",
      " ('L-6.0 Norm Loss', 0.062383883),\n",
      " ('TV(2.0) Loss', 58.064457)], overall loss: -52.0421180725\n",
      "Iteration: 273, named_losses: [('ActivationMax Loss', -112.46786),\n",
      " ('L-6.0 Norm Loss', 0.062387586),\n",
      " ('TV(2.0) Loss', 61.057594)], overall loss: -51.3478851318\n",
      "Iteration: 274, named_losses: [('ActivationMax Loss', -112.55803),\n",
      " ('L-6.0 Norm Loss', 0.062390991),\n",
      " ('TV(2.0) Loss', 59.246513)], overall loss: -53.2491226196\n",
      "Iteration: 275, named_losses: [('ActivationMax Loss', -113.26693),\n",
      " ('L-6.0 Norm Loss', 0.062393028),\n",
      " ('TV(2.0) Loss', 60.512764)], overall loss: -52.6917724609\n",
      "Iteration: 276, named_losses: [('ActivationMax Loss', -110.98032),\n",
      " ('L-6.0 Norm Loss', 0.062395461),\n",
      " ('TV(2.0) Loss', 59.328346)], overall loss: -51.5895843506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 277, named_losses: [('ActivationMax Loss', -113.67271),\n",
      " ('L-6.0 Norm Loss', 0.062397864),\n",
      " ('TV(2.0) Loss', 60.281197)], overall loss: -53.3291168213\n",
      "Iteration: 278, named_losses: [('ActivationMax Loss', -113.04047),\n",
      " ('L-6.0 Norm Loss', 0.062401105),\n",
      " ('TV(2.0) Loss', 59.962502)], overall loss: -53.0155715942\n",
      "Iteration: 279, named_losses: [('ActivationMax Loss', -110.9659),\n",
      " ('L-6.0 Norm Loss', 0.062404241),\n",
      " ('TV(2.0) Loss', 61.521461)], overall loss: -49.3820343018\n",
      "Iteration: 280, named_losses: [('ActivationMax Loss', -115.2794),\n",
      " ('L-6.0 Norm Loss', 0.062406976),\n",
      " ('TV(2.0) Loss', 59.865814)], overall loss: -55.3511810303\n",
      "Iteration: 281, named_losses: [('ActivationMax Loss', -113.83675),\n",
      " ('L-6.0 Norm Loss', 0.062409297),\n",
      " ('TV(2.0) Loss', 61.184658)], overall loss: -52.5896873474\n",
      "Iteration: 282, named_losses: [('ActivationMax Loss', -113.69312),\n",
      " ('L-6.0 Norm Loss', 0.062412113),\n",
      " ('TV(2.0) Loss', 61.390404)], overall loss: -52.2403030396\n",
      "Iteration: 283, named_losses: [('ActivationMax Loss', -110.56762),\n",
      " ('L-6.0 Norm Loss', 0.062414687),\n",
      " ('TV(2.0) Loss', 61.344093)], overall loss: -49.1611099243\n",
      "Iteration: 284, named_losses: [('ActivationMax Loss', -117.77048),\n",
      " ('L-6.0 Norm Loss', 0.062416639),\n",
      " ('TV(2.0) Loss', 60.605007)], overall loss: -57.1030540466\n",
      "Iteration: 285, named_losses: [('ActivationMax Loss', -112.71196),\n",
      " ('L-6.0 Norm Loss', 0.062421471),\n",
      " ('TV(2.0) Loss', 60.330814)], overall loss: -52.3187217712\n",
      "Iteration: 286, named_losses: [('ActivationMax Loss', -114.80564),\n",
      " ('L-6.0 Norm Loss', 0.062424406),\n",
      " ('TV(2.0) Loss', 61.049095)], overall loss: -53.6941223145\n",
      "Iteration: 287, named_losses: [('ActivationMax Loss', -113.91931),\n",
      " ('L-6.0 Norm Loss', 0.062425874),\n",
      " ('TV(2.0) Loss', 60.590805)], overall loss: -53.2660827637\n",
      "Iteration: 288, named_losses: [('ActivationMax Loss', -111.59972),\n",
      " ('L-6.0 Norm Loss', 0.062429011),\n",
      " ('TV(2.0) Loss', 61.732868)], overall loss: -49.8044166565\n",
      "Iteration: 289, named_losses: [('ActivationMax Loss', -111.73795),\n",
      " ('L-6.0 Norm Loss', 0.062431265),\n",
      " ('TV(2.0) Loss', 60.907837)], overall loss: -50.7676849365\n",
      "Iteration: 290, named_losses: [('ActivationMax Loss', -117.24828),\n",
      " ('L-6.0 Norm Loss', 0.06243369),\n",
      " ('TV(2.0) Loss', 61.591736)], overall loss: -55.5941085815\n",
      "Iteration: 291, named_losses: [('ActivationMax Loss', -111.78905),\n",
      " ('L-6.0 Norm Loss', 0.062436443),\n",
      " ('TV(2.0) Loss', 60.753647)], overall loss: -50.9729690552\n",
      "Iteration: 292, named_losses: [('ActivationMax Loss', -118.45187),\n",
      " ('L-6.0 Norm Loss', 0.062438887),\n",
      " ('TV(2.0) Loss', 62.12117)], overall loss: -56.2682647705\n",
      "Iteration: 293, named_losses: [('ActivationMax Loss', -116.6367),\n",
      " ('L-6.0 Norm Loss', 0.062440295),\n",
      " ('TV(2.0) Loss', 60.871086)], overall loss: -55.7031707764\n",
      "Iteration: 294, named_losses: [('ActivationMax Loss', -115.75291),\n",
      " ('L-6.0 Norm Loss', 0.062444028),\n",
      " ('TV(2.0) Loss', 63.294563)], overall loss: -52.395904541\n",
      "Iteration: 295, named_losses: [('ActivationMax Loss', -120.34003),\n",
      " ('L-6.0 Norm Loss', 0.06244716),\n",
      " ('TV(2.0) Loss', 61.570621)], overall loss: -58.7069664001\n",
      "Iteration: 296, named_losses: [('ActivationMax Loss', -117.56786),\n",
      " ('L-6.0 Norm Loss', 0.062450472),\n",
      " ('TV(2.0) Loss', 63.104797)], overall loss: -54.400604248\n",
      "Iteration: 297, named_losses: [('ActivationMax Loss', -113.33498),\n",
      " ('L-6.0 Norm Loss', 0.062454388),\n",
      " ('TV(2.0) Loss', 61.661232)], overall loss: -51.611289978\n",
      "Iteration: 298, named_losses: [('ActivationMax Loss', -118.31075),\n",
      " ('L-6.0 Norm Loss', 0.062455233),\n",
      " ('TV(2.0) Loss', 63.642811)], overall loss: -54.6054878235\n",
      "Iteration: 299, named_losses: [('ActivationMax Loss', -117.61665),\n",
      " ('L-6.0 Norm Loss', 0.06245964),\n",
      " ('TV(2.0) Loss', 62.600723)], overall loss: -54.9534606934\n",
      "Iteration: 300, named_losses: [('ActivationMax Loss', -119.81812),\n",
      " ('L-6.0 Norm Loss', 0.062461127),\n",
      " ('TV(2.0) Loss', 64.406975)], overall loss: -55.3486862183\n",
      "Iteration: 301, named_losses: [('ActivationMax Loss', -115.15378),\n",
      " ('L-6.0 Norm Loss', 0.062465556),\n",
      " ('TV(2.0) Loss', 63.529228)], overall loss: -51.5620880127\n",
      "Iteration: 302, named_losses: [('ActivationMax Loss', -120.78297),\n",
      " ('L-6.0 Norm Loss', 0.06246718),\n",
      " ('TV(2.0) Loss', 64.324959)], overall loss: -56.3955383301\n",
      "Iteration: 303, named_losses: [('ActivationMax Loss', -117.20642),\n",
      " ('L-6.0 Norm Loss', 0.062469989),\n",
      " ('TV(2.0) Loss', 62.703556)], overall loss: -54.4403953552\n",
      "Iteration: 304, named_losses: [('ActivationMax Loss', -118.35577),\n",
      " ('L-6.0 Norm Loss', 0.06247329),\n",
      " ('TV(2.0) Loss', 62.835968)], overall loss: -55.4573364258\n",
      "Iteration: 305, named_losses: [('ActivationMax Loss', -114.39941),\n",
      " ('L-6.0 Norm Loss', 0.062474035),\n",
      " ('TV(2.0) Loss', 62.751347)], overall loss: -51.5855827332\n",
      "Iteration: 306, named_losses: [('ActivationMax Loss', -120.15311),\n",
      " ('L-6.0 Norm Loss', 0.062477037),\n",
      " ('TV(2.0) Loss', 63.238388)], overall loss: -56.8522415161\n",
      "Iteration: 307, named_losses: [('ActivationMax Loss', -120.37479),\n",
      " ('L-6.0 Norm Loss', 0.062480524),\n",
      " ('TV(2.0) Loss', 63.184933)], overall loss: -57.1273841858\n",
      "Iteration: 308, named_losses: [('ActivationMax Loss', -121.32096),\n",
      " ('L-6.0 Norm Loss', 0.062484484),\n",
      " ('TV(2.0) Loss', 63.428288)], overall loss: -57.8301887512\n",
      "Iteration: 309, named_losses: [('ActivationMax Loss', -115.52107),\n",
      " ('L-6.0 Norm Loss', 0.062486794),\n",
      " ('TV(2.0) Loss', 63.913368)], overall loss: -51.5452194214\n",
      "Iteration: 310, named_losses: [('ActivationMax Loss', -124.97328),\n",
      " ('L-6.0 Norm Loss', 0.062488586),\n",
      " ('TV(2.0) Loss', 63.717896)], overall loss: -61.1928939819\n",
      "Iteration: 311, named_losses: [('ActivationMax Loss', -118.34658),\n",
      " ('L-6.0 Norm Loss', 0.062492564),\n",
      " ('TV(2.0) Loss', 64.756485)], overall loss: -53.5276031494\n",
      "Iteration: 312, named_losses: [('ActivationMax Loss', -119.46532),\n",
      " ('L-6.0 Norm Loss', 0.062495649),\n",
      " ('TV(2.0) Loss', 63.611916)], overall loss: -55.7909088135\n",
      "Iteration: 313, named_losses: [('ActivationMax Loss', -122.28691),\n",
      " ('L-6.0 Norm Loss', 0.062497236),\n",
      " ('TV(2.0) Loss', 64.647598)], overall loss: -57.5768127441\n",
      "Iteration: 314, named_losses: [('ActivationMax Loss', -122.96639),\n",
      " ('L-6.0 Norm Loss', 0.062500447),\n",
      " ('TV(2.0) Loss', 62.292442)], overall loss: -60.6114501953\n",
      "Iteration: 315, named_losses: [('ActivationMax Loss', -119.77802),\n",
      " ('L-6.0 Norm Loss', 0.062504053),\n",
      " ('TV(2.0) Loss', 64.834229)], overall loss: -54.8812789917\n",
      "Iteration: 316, named_losses: [('ActivationMax Loss', -117.60001),\n",
      " ('L-6.0 Norm Loss', 0.06250526),\n",
      " ('TV(2.0) Loss', 63.93232)], overall loss: -53.6051864624\n",
      "Iteration: 317, named_losses: [('ActivationMax Loss', -123.99913),\n",
      " ('L-6.0 Norm Loss', 0.062507831),\n",
      " ('TV(2.0) Loss', 64.633331)], overall loss: -59.3032913208\n",
      "Iteration: 318, named_losses: [('ActivationMax Loss', -121.27096),\n",
      " ('L-6.0 Norm Loss', 0.062513039),\n",
      " ('TV(2.0) Loss', 64.345695)], overall loss: -56.8627471924\n",
      "Iteration: 319, named_losses: [('ActivationMax Loss', -117.6347),\n",
      " ('L-6.0 Norm Loss', 0.062513158),\n",
      " ('TV(2.0) Loss', 65.665977)], overall loss: -51.906211853\n",
      "Iteration: 320, named_losses: [('ActivationMax Loss', -117.37431),\n",
      " ('L-6.0 Norm Loss', 0.062515974),\n",
      " ('TV(2.0) Loss', 64.24369)], overall loss: -53.0680999756\n",
      "Iteration: 321, named_losses: [('ActivationMax Loss', -121.93172),\n",
      " ('L-6.0 Norm Loss', 0.062519491),\n",
      " ('TV(2.0) Loss', 65.83168)], overall loss: -56.0375137329\n",
      "Iteration: 322, named_losses: [('ActivationMax Loss', -122.42195),\n",
      " ('L-6.0 Norm Loss', 0.062522121),\n",
      " ('TV(2.0) Loss', 64.549812)], overall loss: -57.8096160889\n",
      "Iteration: 323, named_losses: [('ActivationMax Loss', -119.37366),\n",
      " ('L-6.0 Norm Loss', 0.062525228),\n",
      " ('TV(2.0) Loss', 66.0327)], overall loss: -53.2784347534\n",
      "Iteration: 324, named_losses: [('ActivationMax Loss', -122.82781),\n",
      " ('L-6.0 Norm Loss', 0.0625275),\n",
      " ('TV(2.0) Loss', 64.045074)], overall loss: -58.7202072144\n",
      "Iteration: 325, named_losses: [('ActivationMax Loss', -125.21674),\n",
      " ('L-6.0 Norm Loss', 0.062531345),\n",
      " ('TV(2.0) Loss', 66.365929)], overall loss: -58.7882766724\n",
      "Iteration: 326, named_losses: [('ActivationMax Loss', -119.97245),\n",
      " ('L-6.0 Norm Loss', 0.062533632),\n",
      " ('TV(2.0) Loss', 64.306755)], overall loss: -55.6031646729\n",
      "Iteration: 327, named_losses: [('ActivationMax Loss', -124.83936),\n",
      " ('L-6.0 Norm Loss', 0.062537),\n",
      " ('TV(2.0) Loss', 67.791237)], overall loss: -56.9855804443\n",
      "Iteration: 328, named_losses: [('ActivationMax Loss', -123.57245),\n",
      " ('L-6.0 Norm Loss', 0.062539473),\n",
      " ('TV(2.0) Loss', 65.386154)], overall loss: -58.1237564087\n",
      "Iteration: 329, named_losses: [('ActivationMax Loss', -128.15799),\n",
      " ('L-6.0 Norm Loss', 0.062542871),\n",
      " ('TV(2.0) Loss', 67.297089)], overall loss: -60.7983551025\n",
      "Iteration: 330, named_losses: [('ActivationMax Loss', -125.39252),\n",
      " ('L-6.0 Norm Loss', 0.062543713),\n",
      " ('TV(2.0) Loss', 66.658058)], overall loss: -58.671913147\n",
      "Iteration: 331, named_losses: [('ActivationMax Loss', -124.52459),\n",
      " ('L-6.0 Norm Loss', 0.062547408),\n",
      " ('TV(2.0) Loss', 67.887253)], overall loss: -56.5747909546\n",
      "Iteration: 332, named_losses: [('ActivationMax Loss', -126.75299),\n",
      " ('L-6.0 Norm Loss', 0.062550858),\n",
      " ('TV(2.0) Loss', 66.926544)], overall loss: -59.7638931274\n",
      "Iteration: 333, named_losses: [('ActivationMax Loss', -126.92239),\n",
      " ('L-6.0 Norm Loss', 0.062551469),\n",
      " ('TV(2.0) Loss', 67.777855)], overall loss: -59.0819854736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 334, named_losses: [('ActivationMax Loss', -128.23158),\n",
      " ('L-6.0 Norm Loss', 0.062553898),\n",
      " ('TV(2.0) Loss', 66.244347)], overall loss: -61.9246749878\n",
      "Iteration: 335, named_losses: [('ActivationMax Loss', -126.21046),\n",
      " ('L-6.0 Norm Loss', 0.062556483),\n",
      " ('TV(2.0) Loss', 68.55365)], overall loss: -57.59425354\n",
      "Iteration: 336, named_losses: [('ActivationMax Loss', -131.94002),\n",
      " ('L-6.0 Norm Loss', 0.062559016),\n",
      " ('TV(2.0) Loss', 65.856522)], overall loss: -66.0209350586\n",
      "Iteration: 337, named_losses: [('ActivationMax Loss', -121.91496),\n",
      " ('L-6.0 Norm Loss', 0.062561005),\n",
      " ('TV(2.0) Loss', 68.109398)], overall loss: -53.7430038452\n",
      "Iteration: 338, named_losses: [('ActivationMax Loss', -127.00276),\n",
      " ('L-6.0 Norm Loss', 0.062565938),\n",
      " ('TV(2.0) Loss', 66.57309)], overall loss: -60.3671035767\n",
      "Iteration: 339, named_losses: [('ActivationMax Loss', -128.661),\n",
      " ('L-6.0 Norm Loss', 0.062566891),\n",
      " ('TV(2.0) Loss', 68.095688)], overall loss: -60.502746582\n",
      "Iteration: 340, named_losses: [('ActivationMax Loss', -128.08061),\n",
      " ('L-6.0 Norm Loss', 0.062571406),\n",
      " ('TV(2.0) Loss', 66.769478)], overall loss: -61.2485580444\n",
      "Iteration: 341, named_losses: [('ActivationMax Loss', -126.48823),\n",
      " ('L-6.0 Norm Loss', 0.062574655),\n",
      " ('TV(2.0) Loss', 67.75621)], overall loss: -58.6694412231\n",
      "Iteration: 342, named_losses: [('ActivationMax Loss', -122.09871),\n",
      " ('L-6.0 Norm Loss', 0.06257762),\n",
      " ('TV(2.0) Loss', 68.08297)], overall loss: -53.953163147\n",
      "Iteration: 343, named_losses: [('ActivationMax Loss', -129.41879),\n",
      " ('L-6.0 Norm Loss', 0.062582485),\n",
      " ('TV(2.0) Loss', 68.20504)], overall loss: -61.1511764526\n",
      "Iteration: 344, named_losses: [('ActivationMax Loss', -129.94044),\n",
      " ('L-6.0 Norm Loss', 0.062582567),\n",
      " ('TV(2.0) Loss', 67.162056)], overall loss: -62.7158126831\n",
      "Iteration: 345, named_losses: [('ActivationMax Loss', -128.48019),\n",
      " ('L-6.0 Norm Loss', 0.062585577),\n",
      " ('TV(2.0) Loss', 67.913086)], overall loss: -60.5045166016\n",
      "Iteration: 346, named_losses: [('ActivationMax Loss', -127.74873),\n",
      " ('L-6.0 Norm Loss', 0.062588036),\n",
      " ('TV(2.0) Loss', 67.574097)], overall loss: -60.1120376587\n",
      "Iteration: 347, named_losses: [('ActivationMax Loss', -132.95065),\n",
      " ('L-6.0 Norm Loss', 0.062591277),\n",
      " ('TV(2.0) Loss', 67.622467)], overall loss: -65.2655944824\n",
      "Iteration: 348, named_losses: [('ActivationMax Loss', -128.11633),\n",
      " ('L-6.0 Norm Loss', 0.062594086),\n",
      " ('TV(2.0) Loss', 67.88636)], overall loss: -60.1673812866\n",
      "Iteration: 349, named_losses: [('ActivationMax Loss', -131.07497),\n",
      " ('L-6.0 Norm Loss', 0.062597908),\n",
      " ('TV(2.0) Loss', 68.484192)], overall loss: -62.5281829834\n",
      "Iteration: 350, named_losses: [('ActivationMax Loss', -129.94107),\n",
      " ('L-6.0 Norm Loss', 0.062600285),\n",
      " ('TV(2.0) Loss', 67.976715)], overall loss: -61.9017486572\n",
      "Iteration: 351, named_losses: [('ActivationMax Loss', -129.11758),\n",
      " ('L-6.0 Norm Loss', 0.06260372),\n",
      " ('TV(2.0) Loss', 68.361275)], overall loss: -60.6937026978\n",
      "Iteration: 352, named_losses: [('ActivationMax Loss', -133.146),\n",
      " ('L-6.0 Norm Loss', 0.062606096),\n",
      " ('TV(2.0) Loss', 67.894104)], overall loss: -65.1892852783\n",
      "Iteration: 353, named_losses: [('ActivationMax Loss', -132.46341),\n",
      " ('L-6.0 Norm Loss', 0.062606864),\n",
      " ('TV(2.0) Loss', 67.656242)], overall loss: -64.7445602417\n",
      "Iteration: 354, named_losses: [('ActivationMax Loss', -132.15623),\n",
      " ('L-6.0 Norm Loss', 0.062608726),\n",
      " ('TV(2.0) Loss', 68.552078)], overall loss: -63.5415496826\n",
      "Iteration: 355, named_losses: [('ActivationMax Loss', -129.22891),\n",
      " ('L-6.0 Norm Loss', 0.062612206),\n",
      " ('TV(2.0) Loss', 69.048164)], overall loss: -60.1181411743\n",
      "Iteration: 356, named_losses: [('ActivationMax Loss', -133.20561),\n",
      " ('L-6.0 Norm Loss', 0.06261757),\n",
      " ('TV(2.0) Loss', 70.207153)], overall loss: -62.935836792\n",
      "Iteration: 357, named_losses: [('ActivationMax Loss', -131.26804),\n",
      " ('L-6.0 Norm Loss', 0.06261877),\n",
      " ('TV(2.0) Loss', 69.271111)], overall loss: -61.9343032837\n",
      "Iteration: 358, named_losses: [('ActivationMax Loss', -128.54488),\n",
      " ('L-6.0 Norm Loss', 0.06262318),\n",
      " ('TV(2.0) Loss', 68.563278)], overall loss: -59.9189758301\n",
      "Iteration: 359, named_losses: [('ActivationMax Loss', -130.45735),\n",
      " ('L-6.0 Norm Loss', 0.062625036),\n",
      " ('TV(2.0) Loss', 70.652313)], overall loss: -59.7424163818\n",
      "Iteration: 360, named_losses: [('ActivationMax Loss', -132.21051),\n",
      " ('L-6.0 Norm Loss', 0.062629029),\n",
      " ('TV(2.0) Loss', 68.366829)], overall loss: -63.7810592651\n",
      "Iteration: 361, named_losses: [('ActivationMax Loss', -130.64563),\n",
      " ('L-6.0 Norm Loss', 0.062630475),\n",
      " ('TV(2.0) Loss', 70.482147)], overall loss: -60.1008453369\n",
      "Iteration: 362, named_losses: [('ActivationMax Loss', -132.06813),\n",
      " ('L-6.0 Norm Loss', 0.062631987),\n",
      " ('TV(2.0) Loss', 67.446716)], overall loss: -64.5587768555\n",
      "Iteration: 363, named_losses: [('ActivationMax Loss', -133.40018),\n",
      " ('L-6.0 Norm Loss', 0.062635988),\n",
      " ('TV(2.0) Loss', 70.401825)], overall loss: -62.9357147217\n",
      "Iteration: 364, named_losses: [('ActivationMax Loss', -135.07903),\n",
      " ('L-6.0 Norm Loss', 0.062636971),\n",
      " ('TV(2.0) Loss', 68.114494)], overall loss: -66.9018936157\n",
      "Iteration: 365, named_losses: [('ActivationMax Loss', -131.06651),\n",
      " ('L-6.0 Norm Loss', 0.062642239),\n",
      " ('TV(2.0) Loss', 70.215195)], overall loss: -60.7886810303\n",
      "Iteration: 366, named_losses: [('ActivationMax Loss', -136.80736),\n",
      " ('L-6.0 Norm Loss', 0.062643565),\n",
      " ('TV(2.0) Loss', 69.126923)], overall loss: -67.6177978516\n",
      "Iteration: 367, named_losses: [('ActivationMax Loss', -133.58646),\n",
      " ('L-6.0 Norm Loss', 0.062649891),\n",
      " ('TV(2.0) Loss', 71.965187)], overall loss: -61.5586166382\n",
      "Iteration: 368, named_losses: [('ActivationMax Loss', -131.34685),\n",
      " ('L-6.0 Norm Loss', 0.062649332),\n",
      " ('TV(2.0) Loss', 69.641693)], overall loss: -61.6425018311\n",
      "Iteration: 369, named_losses: [('ActivationMax Loss', -130.57747),\n",
      " ('L-6.0 Norm Loss', 0.06265381),\n",
      " ('TV(2.0) Loss', 71.754013)], overall loss: -58.7608032227\n",
      "Iteration: 370, named_losses: [('ActivationMax Loss', -137.10349),\n",
      " ('L-6.0 Norm Loss', 0.062655605),\n",
      " ('TV(2.0) Loss', 69.931778)], overall loss: -67.1090545654\n",
      "Iteration: 371, named_losses: [('ActivationMax Loss', -132.79715),\n",
      " ('L-6.0 Norm Loss', 0.062658906),\n",
      " ('TV(2.0) Loss', 71.617867)], overall loss: -61.1166305542\n",
      "Iteration: 372, named_losses: [('ActivationMax Loss', -137.56659),\n",
      " ('L-6.0 Norm Loss', 0.062662616),\n",
      " ('TV(2.0) Loss', 69.219322)], overall loss: -68.2845993042\n",
      "Iteration: 373, named_losses: [('ActivationMax Loss', -134.81746),\n",
      " ('L-6.0 Norm Loss', 0.06266503),\n",
      " ('TV(2.0) Loss', 71.961533)], overall loss: -62.793258667\n",
      "Iteration: 374, named_losses: [('ActivationMax Loss', -139.00591),\n",
      " ('L-6.0 Norm Loss', 0.062667996),\n",
      " ('TV(2.0) Loss', 69.214539)], overall loss: -69.7286987305\n",
      "Iteration: 375, named_losses: [('ActivationMax Loss', -139.2256),\n",
      " ('L-6.0 Norm Loss', 0.062671885),\n",
      " ('TV(2.0) Loss', 71.610886)], overall loss: -67.5520477295\n",
      "Iteration: 376, named_losses: [('ActivationMax Loss', -133.77788),\n",
      " ('L-6.0 Norm Loss', 0.062674493),\n",
      " ('TV(2.0) Loss', 69.77211)], overall loss: -63.9430999756\n",
      "Iteration: 377, named_losses: [('ActivationMax Loss', -134.90303),\n",
      " ('L-6.0 Norm Loss', 0.06267599),\n",
      " ('TV(2.0) Loss', 72.281776)], overall loss: -62.5585708618\n",
      "Iteration: 378, named_losses: [('ActivationMax Loss', -141.00626),\n",
      " ('L-6.0 Norm Loss', 0.062678412),\n",
      " ('TV(2.0) Loss', 69.659378)], overall loss: -71.2841949463\n",
      "Iteration: 379, named_losses: [('ActivationMax Loss', -134.6557),\n",
      " ('L-6.0 Norm Loss', 0.0626828),\n",
      " ('TV(2.0) Loss', 71.964676)], overall loss: -62.6283416748\n",
      "Iteration: 380, named_losses: [('ActivationMax Loss', -137.4716),\n",
      " ('L-6.0 Norm Loss', 0.062684275),\n",
      " ('TV(2.0) Loss', 70.77964)], overall loss: -66.6292800903\n",
      "Iteration: 381, named_losses: [('ActivationMax Loss', -132.44173),\n",
      " ('L-6.0 Norm Loss', 0.062688865),\n",
      " ('TV(2.0) Loss', 72.374557)], overall loss: -60.004486084\n",
      "Iteration: 382, named_losses: [('ActivationMax Loss', -138.54382),\n",
      " ('L-6.0 Norm Loss', 0.062690705),\n",
      " ('TV(2.0) Loss', 70.004189)], overall loss: -68.4769515991\n",
      "Iteration: 383, named_losses: [('ActivationMax Loss', -134.41895),\n",
      " ('L-6.0 Norm Loss', 0.062693328),\n",
      " ('TV(2.0) Loss', 72.389236)], overall loss: -61.967010498\n",
      "Iteration: 384, named_losses: [('ActivationMax Loss', -139.12358),\n",
      " ('L-6.0 Norm Loss', 0.062696755),\n",
      " ('TV(2.0) Loss', 70.258804)], overall loss: -68.8020782471\n",
      "Iteration: 385, named_losses: [('ActivationMax Loss', -136.84123),\n",
      " ('L-6.0 Norm Loss', 0.06269788),\n",
      " ('TV(2.0) Loss', 72.239212)], overall loss: -64.5393218994\n",
      "Iteration: 386, named_losses: [('ActivationMax Loss', -140.56212),\n",
      " ('L-6.0 Norm Loss', 0.062701583),\n",
      " ('TV(2.0) Loss', 70.644363)], overall loss: -69.8550567627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 387, named_losses: [('ActivationMax Loss', -139.22398),\n",
      " ('L-6.0 Norm Loss', 0.062704168),\n",
      " ('TV(2.0) Loss', 74.081116)], overall loss: -65.0801696777\n",
      "Iteration: 388, named_losses: [('ActivationMax Loss', -138.96829),\n",
      " ('L-6.0 Norm Loss', 0.062707596),\n",
      " ('TV(2.0) Loss', 70.89389)], overall loss: -68.0116882324\n",
      "Iteration: 389, named_losses: [('ActivationMax Loss', -139.73682),\n",
      " ('L-6.0 Norm Loss', 0.062710412),\n",
      " ('TV(2.0) Loss', 72.549789)], overall loss: -67.1243133545\n",
      "Iteration: 390, named_losses: [('ActivationMax Loss', -139.44196),\n",
      " ('L-6.0 Norm Loss', 0.06271337),\n",
      " ('TV(2.0) Loss', 71.712288)], overall loss: -67.6669540405\n",
      "Iteration: 391, named_losses: [('ActivationMax Loss', -134.49084),\n",
      " ('L-6.0 Norm Loss', 0.06271641),\n",
      " ('TV(2.0) Loss', 72.558838)], overall loss: -61.8692932129\n",
      "Iteration: 392, named_losses: [('ActivationMax Loss', -140.21024),\n",
      " ('L-6.0 Norm Loss', 0.062720843),\n",
      " ('TV(2.0) Loss', 72.034454)], overall loss: -68.113067627\n",
      "Iteration: 393, named_losses: [('ActivationMax Loss', -136.07448),\n",
      " ('L-6.0 Norm Loss', 0.062722653),\n",
      " ('TV(2.0) Loss', 72.450783)], overall loss: -63.5609664917\n",
      "Iteration: 394, named_losses: [('ActivationMax Loss', -139.89977),\n",
      " ('L-6.0 Norm Loss', 0.062726103),\n",
      " ('TV(2.0) Loss', 71.97332)], overall loss: -67.8637161255\n",
      "Iteration: 395, named_losses: [('ActivationMax Loss', -137.41159),\n",
      " ('L-6.0 Norm Loss', 0.062728763),\n",
      " ('TV(2.0) Loss', 72.810127)], overall loss: -64.538734436\n",
      "Iteration: 396, named_losses: [('ActivationMax Loss', -142.73474),\n",
      " ('L-6.0 Norm Loss', 0.062729523),\n",
      " ('TV(2.0) Loss', 71.824142)], overall loss: -70.847869873\n",
      "Iteration: 397, named_losses: [('ActivationMax Loss', -140.91222),\n",
      " ('L-6.0 Norm Loss', 0.062735163),\n",
      " ('TV(2.0) Loss', 72.537704)], overall loss: -68.3117828369\n",
      "Iteration: 398, named_losses: [('ActivationMax Loss', -141.6562),\n",
      " ('L-6.0 Norm Loss', 0.062736541),\n",
      " ('TV(2.0) Loss', 72.590141)], overall loss: -69.0033187866\n",
      "Iteration: 399, named_losses: [('ActivationMax Loss', -142.24733),\n",
      " ('L-6.0 Norm Loss', 0.062739797),\n",
      " ('TV(2.0) Loss', 73.031548)], overall loss: -69.1530380249\n",
      "Iteration: 400, named_losses: [('ActivationMax Loss', -140.57739),\n",
      " ('L-6.0 Norm Loss', 0.062742479),\n",
      " ('TV(2.0) Loss', 71.91481)], overall loss: -68.5998382568\n",
      "Iteration: 401, named_losses: [('ActivationMax Loss', -139.50906),\n",
      " ('L-6.0 Norm Loss', 0.062745459),\n",
      " ('TV(2.0) Loss', 72.750412)], overall loss: -66.6959075928\n",
      "Iteration: 402, named_losses: [('ActivationMax Loss', -143.27655),\n",
      " ('L-6.0 Norm Loss', 0.062746421),\n",
      " ('TV(2.0) Loss', 72.066795)], overall loss: -71.1470108032\n",
      "Iteration: 403, named_losses: [('ActivationMax Loss', -139.51357),\n",
      " ('L-6.0 Norm Loss', 0.0627506),\n",
      " ('TV(2.0) Loss', 72.188675)], overall loss: -67.2621459961\n",
      "Iteration: 404, named_losses: [('ActivationMax Loss', -142.9973),\n",
      " ('L-6.0 Norm Loss', 0.062753551),\n",
      " ('TV(2.0) Loss', 73.722336)], overall loss: -69.2122039795\n",
      "Iteration: 405, named_losses: [('ActivationMax Loss', -138.62294),\n",
      " ('L-6.0 Norm Loss', 0.062756181),\n",
      " ('TV(2.0) Loss', 72.825424)], overall loss: -65.7347564697\n",
      "Iteration: 406, named_losses: [('ActivationMax Loss', -144.35088),\n",
      " ('L-6.0 Norm Loss', 0.062758714),\n",
      " ('TV(2.0) Loss', 72.848717)], overall loss: -71.4393997192\n",
      "Iteration: 407, named_losses: [('ActivationMax Loss', -142.30759),\n",
      " ('L-6.0 Norm Loss', 0.062761292),\n",
      " ('TV(2.0) Loss', 72.600616)], overall loss: -69.6442108154\n",
      "Iteration: 408, named_losses: [('ActivationMax Loss', -139.07095),\n",
      " ('L-6.0 Norm Loss', 0.062764049),\n",
      " ('TV(2.0) Loss', 73.714333)], overall loss: -65.2938613892\n",
      "Iteration: 409, named_losses: [('ActivationMax Loss', -142.22777),\n",
      " ('L-6.0 Norm Loss', 0.062768973),\n",
      " ('TV(2.0) Loss', 73.164162)], overall loss: -69.000831604\n",
      "Iteration: 410, named_losses: [('ActivationMax Loss', -141.81583),\n",
      " ('L-6.0 Norm Loss', 0.062771544),\n",
      " ('TV(2.0) Loss', 74.028221)], overall loss: -67.7248306274\n",
      "Iteration: 411, named_losses: [('ActivationMax Loss', -145.58228),\n",
      " ('L-6.0 Norm Loss', 0.062774956),\n",
      " ('TV(2.0) Loss', 72.881424)], overall loss: -72.6380767822\n",
      "Iteration: 412, named_losses: [('ActivationMax Loss', -141.39627),\n",
      " ('L-6.0 Norm Loss', 0.062777713),\n",
      " ('TV(2.0) Loss', 73.894768)], overall loss: -67.4387283325\n",
      "Iteration: 413, named_losses: [('ActivationMax Loss', -144.37086),\n",
      " ('L-6.0 Norm Loss', 0.062781513),\n",
      " ('TV(2.0) Loss', 73.111526)], overall loss: -71.1965637207\n",
      "Iteration: 414, named_losses: [('ActivationMax Loss', -138.76392),\n",
      " ('L-6.0 Norm Loss', 0.062785141),\n",
      " ('TV(2.0) Loss', 75.106598)], overall loss: -63.5945281982\n",
      "Iteration: 415, named_losses: [('ActivationMax Loss', -143.54672),\n",
      " ('L-6.0 Norm Loss', 0.062787428),\n",
      " ('TV(2.0) Loss', 73.301186)], overall loss: -70.1827468872\n",
      "Iteration: 416, named_losses: [('ActivationMax Loss', -145.38876),\n",
      " ('L-6.0 Norm Loss', 0.062790059),\n",
      " ('TV(2.0) Loss', 73.513397)], overall loss: -71.8125762939\n",
      "Iteration: 417, named_losses: [('ActivationMax Loss', -141.94406),\n",
      " ('L-6.0 Norm Loss', 0.062794261),\n",
      " ('TV(2.0) Loss', 73.975876)], overall loss: -67.9053955078\n",
      "Iteration: 418, named_losses: [('ActivationMax Loss', -145.29156),\n",
      " ('L-6.0 Norm Loss', 0.062796727),\n",
      " ('TV(2.0) Loss', 74.410286)], overall loss: -70.8184890747\n",
      "Iteration: 419, named_losses: [('ActivationMax Loss', -143.43077),\n",
      " ('L-6.0 Norm Loss', 0.062801622),\n",
      " ('TV(2.0) Loss', 74.717628)], overall loss: -68.6503372192\n",
      "Iteration: 420, named_losses: [('ActivationMax Loss', -145.27054),\n",
      " ('L-6.0 Norm Loss', 0.062802516),\n",
      " ('TV(2.0) Loss', 74.45179)], overall loss: -70.7559432983\n",
      "Iteration: 421, named_losses: [('ActivationMax Loss', -144.51248),\n",
      " ('L-6.0 Norm Loss', 0.06280712),\n",
      " ('TV(2.0) Loss', 75.463799)], overall loss: -68.9858779907\n",
      "Iteration: 422, named_losses: [('ActivationMax Loss', -143.84959),\n",
      " ('L-6.0 Norm Loss', 0.062810346),\n",
      " ('TV(2.0) Loss', 73.957306)], overall loss: -69.8294830322\n",
      "Iteration: 423, named_losses: [('ActivationMax Loss', -145.58594),\n",
      " ('L-6.0 Norm Loss', 0.062813528),\n",
      " ('TV(2.0) Loss', 75.771721)], overall loss: -69.7513961792\n",
      "Iteration: 424, named_losses: [('ActivationMax Loss', -145.62358),\n",
      " ('L-6.0 Norm Loss', 0.062815405),\n",
      " ('TV(2.0) Loss', 74.289368)], overall loss: -71.2713928223\n",
      "Iteration: 425, named_losses: [('ActivationMax Loss', -146.09689),\n",
      " ('L-6.0 Norm Loss', 0.062818445),\n",
      " ('TV(2.0) Loss', 75.823204)], overall loss: -70.2108688354\n",
      "Iteration: 426, named_losses: [('ActivationMax Loss', -146.34875),\n",
      " ('L-6.0 Norm Loss', 0.062820926),\n",
      " ('TV(2.0) Loss', 75.21714)], overall loss: -71.0687942505\n",
      "Iteration: 427, named_losses: [('ActivationMax Loss', -146.32544),\n",
      " ('L-6.0 Norm Loss', 0.062824123),\n",
      " ('TV(2.0) Loss', 76.579735)], overall loss: -69.6828842163\n",
      "Iteration: 428, named_losses: [('ActivationMax Loss', -143.18855),\n",
      " ('L-6.0 Norm Loss', 0.062829934),\n",
      " ('TV(2.0) Loss', 75.339134)], overall loss: -67.7865829468\n",
      "Iteration: 429, named_losses: [('ActivationMax Loss', -147.66838),\n",
      " ('L-6.0 Norm Loss', 0.062832028),\n",
      " ('TV(2.0) Loss', 75.044785)], overall loss: -72.560760498\n",
      "Iteration: 430, named_losses: [('ActivationMax Loss', -144.30286),\n",
      " ('L-6.0 Norm Loss', 0.062835753),\n",
      " ('TV(2.0) Loss', 75.958275)], overall loss: -68.2817459106\n",
      "Iteration: 431, named_losses: [('ActivationMax Loss', -147.18196),\n",
      " ('L-6.0 Norm Loss', 0.062836736),\n",
      " ('TV(2.0) Loss', 74.678429)], overall loss: -72.4406967163\n",
      "Iteration: 432, named_losses: [('ActivationMax Loss', -147.75696),\n",
      " ('L-6.0 Norm Loss', 0.062842377),\n",
      " ('TV(2.0) Loss', 75.894943)], overall loss: -71.7991790771\n",
      "Iteration: 433, named_losses: [('ActivationMax Loss', -147.81818),\n",
      " ('L-6.0 Norm Loss', 0.062843263),\n",
      " ('TV(2.0) Loss', 75.730606)], overall loss: -72.0247344971\n",
      "Iteration: 434, named_losses: [('ActivationMax Loss', -146.59891),\n",
      " ('L-6.0 Norm Loss', 0.062848829),\n",
      " ('TV(2.0) Loss', 76.10627)], overall loss: -70.4297866821\n",
      "Iteration: 435, named_losses: [('ActivationMax Loss', -149.23871),\n",
      " ('L-6.0 Norm Loss', 0.06285169),\n",
      " ('TV(2.0) Loss', 76.117828)], overall loss: -73.0580291748\n",
      "Iteration: 436, named_losses: [('ActivationMax Loss', -147.08241),\n",
      " ('L-6.0 Norm Loss', 0.062855311),\n",
      " ('TV(2.0) Loss', 75.71904)], overall loss: -71.3005218506\n",
      "Iteration: 437, named_losses: [('ActivationMax Loss', -144.91626),\n",
      " ('L-6.0 Norm Loss', 0.062857568),\n",
      " ('TV(2.0) Loss', 76.941376)], overall loss: -67.9120330811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 438, named_losses: [('ActivationMax Loss', -147.75507),\n",
      " ('L-6.0 Norm Loss', 0.062860459),\n",
      " ('TV(2.0) Loss', 76.416542)], overall loss: -71.2756576538\n",
      "Iteration: 439, named_losses: [('ActivationMax Loss', -146.35825),\n",
      " ('L-6.0 Norm Loss', 0.062862925),\n",
      " ('TV(2.0) Loss', 76.040337)], overall loss: -70.2550430298\n",
      "Iteration: 440, named_losses: [('ActivationMax Loss', -148.93884),\n",
      " ('L-6.0 Norm Loss', 0.062865987),\n",
      " ('TV(2.0) Loss', 76.496346)], overall loss: -72.3796310425\n",
      "Iteration: 441, named_losses: [('ActivationMax Loss', -149.18591),\n",
      " ('L-6.0 Norm Loss', 0.062868237),\n",
      " ('TV(2.0) Loss', 75.888054)], overall loss: -73.234992981\n",
      "Iteration: 442, named_losses: [('ActivationMax Loss', -147.30473),\n",
      " ('L-6.0 Norm Loss', 0.062870204),\n",
      " ('TV(2.0) Loss', 76.196472)], overall loss: -71.0453948975\n",
      "Iteration: 443, named_losses: [('ActivationMax Loss', -151.31589),\n",
      " ('L-6.0 Norm Loss', 0.062872902),\n",
      " ('TV(2.0) Loss', 75.758423)], overall loss: -75.4945983887\n",
      "Iteration: 444, named_losses: [('ActivationMax Loss', -150.51529),\n",
      " ('L-6.0 Norm Loss', 0.062876485),\n",
      " ('TV(2.0) Loss', 76.557373)], overall loss: -73.89503479\n",
      "Iteration: 445, named_losses: [('ActivationMax Loss', -148.82353),\n",
      " ('L-6.0 Norm Loss', 0.062880047),\n",
      " ('TV(2.0) Loss', 76.311279)], overall loss: -72.4493713379\n",
      "Iteration: 446, named_losses: [('ActivationMax Loss', -151.71295),\n",
      " ('L-6.0 Norm Loss', 0.062881567),\n",
      " ('TV(2.0) Loss', 77.365166)], overall loss: -74.28490448\n",
      "Iteration: 447, named_losses: [('ActivationMax Loss', -149.31421),\n",
      " ('L-6.0 Norm Loss', 0.062885903),\n",
      " ('TV(2.0) Loss', 76.143028)], overall loss: -73.1082992554\n",
      "Iteration: 448, named_losses: [('ActivationMax Loss', -150.26318),\n",
      " ('L-6.0 Norm Loss', 0.062887043),\n",
      " ('TV(2.0) Loss', 76.581001)], overall loss: -73.6193008423\n",
      "Iteration: 449, named_losses: [('ActivationMax Loss', -151.68604),\n",
      " ('L-6.0 Norm Loss', 0.062892638),\n",
      " ('TV(2.0) Loss', 76.890915)], overall loss: -74.7322235107\n",
      "Iteration: 450, named_losses: [('ActivationMax Loss', -148.28377),\n",
      " ('L-6.0 Norm Loss', 0.062893935),\n",
      " ('TV(2.0) Loss', 75.592422)], overall loss: -72.6284484863\n",
      "Iteration: 451, named_losses: [('ActivationMax Loss', -147.35678),\n",
      " ('L-6.0 Norm Loss', 0.062899612),\n",
      " ('TV(2.0) Loss', 78.092506)], overall loss: -69.2013778687\n",
      "Iteration: 452, named_losses: [('ActivationMax Loss', -153.25745),\n",
      " ('L-6.0 Norm Loss', 0.062901981),\n",
      " ('TV(2.0) Loss', 75.079391)], overall loss: -78.1151580811\n",
      "Iteration: 453, named_losses: [('ActivationMax Loss', -149.76364),\n",
      " ('L-6.0 Norm Loss', 0.062906131),\n",
      " ('TV(2.0) Loss', 78.377541)], overall loss: -71.3231887817\n",
      "Iteration: 454, named_losses: [('ActivationMax Loss', -152.04784),\n",
      " ('L-6.0 Norm Loss', 0.062906317),\n",
      " ('TV(2.0) Loss', 75.977638)], overall loss: -76.0072860718\n",
      "Iteration: 455, named_losses: [('ActivationMax Loss', -148.44231),\n",
      " ('L-6.0 Norm Loss', 0.062910795),\n",
      " ('TV(2.0) Loss', 77.3032)], overall loss: -71.0761947632\n",
      "Iteration: 456, named_losses: [('ActivationMax Loss', -153.95691),\n",
      " ('L-6.0 Norm Loss', 0.062912397),\n",
      " ('TV(2.0) Loss', 76.434631)], overall loss: -77.4593658447\n",
      "Iteration: 457, named_losses: [('ActivationMax Loss', -146.44548),\n",
      " ('L-6.0 Norm Loss', 0.062915578),\n",
      " ('TV(2.0) Loss', 79.674469)], overall loss: -66.7080993652\n",
      "Iteration: 458, named_losses: [('ActivationMax Loss', -153.75627),\n",
      " ('L-6.0 Norm Loss', 0.062918916),\n",
      " ('TV(2.0) Loss', 76.898636)], overall loss: -76.7947235107\n",
      "Iteration: 459, named_losses: [('ActivationMax Loss', -148.89221),\n",
      " ('L-6.0 Norm Loss', 0.062920913),\n",
      " ('TV(2.0) Loss', 78.139786)], overall loss: -70.6894989014\n",
      "Iteration: 460, named_losses: [('ActivationMax Loss', -151.31757),\n",
      " ('L-6.0 Norm Loss', 0.062923267),\n",
      " ('TV(2.0) Loss', 77.551994)], overall loss: -73.7026443481\n",
      "Iteration: 461, named_losses: [('ActivationMax Loss', -149.07819),\n",
      " ('L-6.0 Norm Loss', 0.062927485),\n",
      " ('TV(2.0) Loss', 78.698586)], overall loss: -70.3166732788\n",
      "Iteration: 462, named_losses: [('ActivationMax Loss', -152.49811),\n",
      " ('L-6.0 Norm Loss', 0.062929422),\n",
      " ('TV(2.0) Loss', 76.873299)], overall loss: -75.561882019\n",
      "Iteration: 463, named_losses: [('ActivationMax Loss', -153.04091),\n",
      " ('L-6.0 Norm Loss', 0.062933609),\n",
      " ('TV(2.0) Loss', 78.172409)], overall loss: -74.8055725098\n",
      "Iteration: 464, named_losses: [('ActivationMax Loss', -153.95583),\n",
      " ('L-6.0 Norm Loss', 0.062936828),\n",
      " ('TV(2.0) Loss', 77.590683)], overall loss: -76.3022003174\n",
      "Iteration: 465, named_losses: [('ActivationMax Loss', -156.10953),\n",
      " ('L-6.0 Norm Loss', 0.062940717),\n",
      " ('TV(2.0) Loss', 79.119041)], overall loss: -76.9275436401\n",
      "Iteration: 466, named_losses: [('ActivationMax Loss', -145.71866),\n",
      " ('L-6.0 Norm Loss', 0.062943906),\n",
      " ('TV(2.0) Loss', 77.459)], overall loss: -68.1967163086\n",
      "Iteration: 467, named_losses: [('ActivationMax Loss', -154.64474),\n",
      " ('L-6.0 Norm Loss', 0.06294553),\n",
      " ('TV(2.0) Loss', 79.444298)], overall loss: -75.1375045776\n",
      "Iteration: 468, named_losses: [('ActivationMax Loss', -151.04881),\n",
      " ('L-6.0 Norm Loss', 0.062949196),\n",
      " ('TV(2.0) Loss', 76.980843)], overall loss: -74.005027771\n",
      "Iteration: 469, named_losses: [('ActivationMax Loss', -150.35945),\n",
      " ('L-6.0 Norm Loss', 0.062951736),\n",
      " ('TV(2.0) Loss', 78.674477)], overall loss: -71.6220169067\n",
      "Iteration: 470, named_losses: [('ActivationMax Loss', -156.62335),\n",
      " ('L-6.0 Norm Loss', 0.062953971),\n",
      " ('TV(2.0) Loss', 77.191483)], overall loss: -79.3689117432\n",
      "Iteration: 471, named_losses: [('ActivationMax Loss', -150.9823),\n",
      " ('L-6.0 Norm Loss', 0.062956445),\n",
      " ('TV(2.0) Loss', 79.235382)], overall loss: -71.6839599609\n",
      "Iteration: 472, named_losses: [('ActivationMax Loss', -154.03825),\n",
      " ('L-6.0 Norm Loss', 0.062961489),\n",
      " ('TV(2.0) Loss', 78.472809)], overall loss: -75.5024871826\n",
      "Iteration: 473, named_losses: [('ActivationMax Loss', -155.60004),\n",
      " ('L-6.0 Norm Loss', 0.062963262),\n",
      " ('TV(2.0) Loss', 79.945732)], overall loss: -75.5913467407\n",
      "Iteration: 474, named_losses: [('ActivationMax Loss', -152.65337),\n",
      " ('L-6.0 Norm Loss', 0.062968515),\n",
      " ('TV(2.0) Loss', 78.46608)], overall loss: -74.1243133545\n",
      "Iteration: 475, named_losses: [('ActivationMax Loss', -155.58034),\n",
      " ('L-6.0 Norm Loss', 0.062968753),\n",
      " ('TV(2.0) Loss', 79.334038)], overall loss: -76.1833267212\n",
      "Iteration: 476, named_losses: [('ActivationMax Loss', -154.90883),\n",
      " ('L-6.0 Norm Loss', 0.062973261),\n",
      " ('TV(2.0) Loss', 77.762535)], overall loss: -77.0833206177\n",
      "Iteration: 477, named_losses: [('ActivationMax Loss', -154.97806),\n",
      " ('L-6.0 Norm Loss', 0.062974989),\n",
      " ('TV(2.0) Loss', 79.10482)], overall loss: -75.8102645874\n",
      "Iteration: 478, named_losses: [('ActivationMax Loss', -152.09059),\n",
      " ('L-6.0 Norm Loss', 0.06297908),\n",
      " ('TV(2.0) Loss', 77.791771)], overall loss: -74.2358474731\n",
      "Iteration: 479, named_losses: [('ActivationMax Loss', -156.22099),\n",
      " ('L-6.0 Norm Loss', 0.062981963),\n",
      " ('TV(2.0) Loss', 80.056015)], overall loss: -76.1019897461\n",
      "Iteration: 480, named_losses: [('ActivationMax Loss', -153.39764),\n",
      " ('L-6.0 Norm Loss', 0.062984273),\n",
      " ('TV(2.0) Loss', 78.513351)], overall loss: -74.8213043213\n",
      "Iteration: 481, named_losses: [('ActivationMax Loss', -151.23335),\n",
      " ('L-6.0 Norm Loss', 0.062989578),\n",
      " ('TV(2.0) Loss', 79.159126)], overall loss: -72.0112380981\n",
      "Iteration: 482, named_losses: [('ActivationMax Loss', -154.5715),\n",
      " ('L-6.0 Norm Loss', 0.062990718),\n",
      " ('TV(2.0) Loss', 79.108345)], overall loss: -75.4001693726\n",
      "Iteration: 483, named_losses: [('ActivationMax Loss', -152.37679),\n",
      " ('L-6.0 Norm Loss', 0.062994026),\n",
      " ('TV(2.0) Loss', 79.806961)], overall loss: -72.5068359375\n",
      "Iteration: 484, named_losses: [('ActivationMax Loss', -155.35219),\n",
      " ('L-6.0 Norm Loss', 0.062995881),\n",
      " ('TV(2.0) Loss', 78.537399)], overall loss: -76.7518005371\n",
      "Iteration: 485, named_losses: [('ActivationMax Loss', -150.86581),\n",
      " ('L-6.0 Norm Loss', 0.062999263),\n",
      " ('TV(2.0) Loss', 79.860237)], overall loss: -70.9425735474\n",
      "Iteration: 486, named_losses: [('ActivationMax Loss', -156.02452),\n",
      " ('L-6.0 Norm Loss', 0.063002303),\n",
      " ('TV(2.0) Loss', 78.534882)], overall loss: -77.4266357422\n",
      "Iteration: 487, named_losses: [('ActivationMax Loss', -152.9613),\n",
      " ('L-6.0 Norm Loss', 0.063006014),\n",
      " ('TV(2.0) Loss', 79.599541)], overall loss: -73.2987594604\n",
      "Iteration: 488, named_losses: [('ActivationMax Loss', -156.22141),\n",
      " ('L-6.0 Norm Loss', 0.063007407),\n",
      " ('TV(2.0) Loss', 78.018753)], overall loss: -78.1396484375\n",
      "Iteration: 489, named_losses: [('ActivationMax Loss', -151.55534),\n",
      " ('L-6.0 Norm Loss', 0.063012823),\n",
      " ('TV(2.0) Loss', 78.805367)], overall loss: -72.686958313\n",
      "Iteration: 490, named_losses: [('ActivationMax Loss', -158.52295),\n",
      " ('L-6.0 Norm Loss', 0.06301301),\n",
      " ('TV(2.0) Loss', 78.155159)], overall loss: -80.3047714233\n",
      "Iteration: 491, named_losses: [('ActivationMax Loss', -155.44188),\n",
      " ('L-6.0 Norm Loss', 0.06301821),\n",
      " ('TV(2.0) Loss', 79.538498)], overall loss: -75.8403625488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 492, named_losses: [('ActivationMax Loss', -157.68198),\n",
      " ('L-6.0 Norm Loss', 0.063019305),\n",
      " ('TV(2.0) Loss', 78.353966)], overall loss: -79.2649917603\n",
      "Iteration: 493, named_losses: [('ActivationMax Loss', -152.27164),\n",
      " ('L-6.0 Norm Loss', 0.063024096),\n",
      " ('TV(2.0) Loss', 80.214958)], overall loss: -71.9936599731\n",
      "Iteration: 494, named_losses: [('ActivationMax Loss', -158.29787),\n",
      " ('L-6.0 Norm Loss', 0.063024633),\n",
      " ('TV(2.0) Loss', 78.70845)], overall loss: -79.5263977051\n",
      "Iteration: 495, named_losses: [('ActivationMax Loss', -153.49835),\n",
      " ('L-6.0 Norm Loss', 0.063029215),\n",
      " ('TV(2.0) Loss', 81.051971)], overall loss: -72.3833465576\n",
      "Iteration: 496, named_losses: [('ActivationMax Loss', -158.92822),\n",
      " ('L-6.0 Norm Loss', 0.063031986),\n",
      " ('TV(2.0) Loss', 78.715805)], overall loss: -80.1493835449\n",
      "Iteration: 497, named_losses: [('ActivationMax Loss', -155.50137),\n",
      " ('L-6.0 Norm Loss', 0.063035131),\n",
      " ('TV(2.0) Loss', 81.072197)], overall loss: -74.3661422729\n",
      "Iteration: 498, named_losses: [('ActivationMax Loss', -156.21909),\n",
      " ('L-6.0 Norm Loss', 0.063035503),\n",
      " ('TV(2.0) Loss', 78.621414)], overall loss: -77.5346374512\n",
      "Iteration: 499, named_losses: [('ActivationMax Loss', -157.51538),\n",
      " ('L-6.0 Norm Loss', 0.063039958),\n",
      " ('TV(2.0) Loss', 81.570572)], overall loss: -75.8817749023\n",
      "Iteration: 500, named_losses: [('ActivationMax Loss', -157.952),\n",
      " ('L-6.0 Norm Loss', 0.063040897),\n",
      " ('TV(2.0) Loss', 79.478874)], overall loss: -78.4100875854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5418480e90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f541879fcd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 20 is the imagenet category for 'ouzel'\n",
    "img = visualize_activation(model, layer_idx, filter_indices=20, max_iter=500, verbose=True)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss appears to be converging. So more iterations definitely seem to give better output. One way to get crisper results is to use `Jitter` input_modifier. As the name suggests, `Jitter` moves pixels around in the image. Lets try this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5418100150>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f54683b88d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vis.input_modifiers import Jitter\n",
    "\n",
    "# 20 is the imagenet category for 'ouzel'\n",
    "# Jitter 16 pixels along all dimensions to during the optimization process.\n",
    "img = visualize_activation(model, layer_idx, filter_indices=20, max_iter=500, input_modifiers=[Jitter(16)])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that! Not only has the conv net captured what it means to be an ouzel, but it also seems to encode for different orientations and scales, a further proof of rotational and scale invariance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try this for a bunch of other random categories. This will take a while. Go grab a nice cup of coffee and prepare to be amused :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f54b190d150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "categories = np.random.permutation(1000)[:15]\n",
    "\n",
    "vis_images = []\n",
    "image_modifiers = [Jitter(16)]\n",
    "for idx in categories:    \n",
    "    img = visualize_activation(model, layer_idx, filter_indices=idx, max_iter=500, input_modifiers=image_modifiers)\n",
    "    \n",
    "    # Reverse lookup index to imagenet label and overlay it on the image.\n",
    "    img = utils.draw_text(img, utils.get_imagenet_label(idx))\n",
    "    vis_images.append(img)\n",
    "\n",
    "# Generate stitched images with 5 cols (so it will have 3 rows).\n",
    "plt.rcParams['figure.figsize'] = (50, 50)\n",
    "stitched = utils.stitch_images(vis_images, cols=5)\n",
    "plt.axis('off')\n",
    "plt.imshow(stitched)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them make sense if you stare at ot for a while. There are ways of improving this. We will cover some ideas for this in the next section. You can come back here and try those out as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Conv filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a CNN, each Conv layer has several learned *template matching* filters that maximize their output when a similar \n",
    "template pattern is found in the input image. First Conv layer is easy to interpret; simply visualize the weights as an image. To see what the Conv layer is doing, a simple option is to apply the filter over raw input pixels. \n",
    "Subsequent Conv filters operate over the outputs of previous Conv filters (which indicate the presence or absence \n",
    "of some templates), making them hard to interpret.\n",
    "\n",
    "One way of interpreting them is to generate an input image that maximizes the filter output. This allows us to generate an input that activates the filter.\n",
    "\n",
    "Lets start by visualizing the second conv layer of vggnet (named as 'block1_conv2'). Here is the VGG16 model for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53cdb67fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vis.visualization import get_num_filters\n",
    "\n",
    "# The name of the layer we want to visualize\n",
    "# You can see this in the model definition.\n",
    "layer_name = 'block1_conv2'\n",
    "layer_idx = utils.find_layer_idx(model, layer_name)\n",
    "\n",
    "# Visualize all filters in this layer.\n",
    "filters = np.arange(get_num_filters(model.layers[layer_idx]))\n",
    "\n",
    "# Generate input image for each filter.\n",
    "vis_images = []\n",
    "for idx in filters:\n",
    "    img = visualize_activation(model, layer_idx, filter_indices=idx)\n",
    "    \n",
    "    # Utility to overlay text on image.\n",
    "    img = utils.draw_text(img, 'Filter {}'.format(idx))    \n",
    "    vis_images.append(img)\n",
    "\n",
    "# Generate stitched image palette with 8 cols.\n",
    "stitched = utils.stitch_images(vis_images, cols=8)    \n",
    "plt.axis('off')\n",
    "plt.imshow(stitched)\n",
    "plt.title(layer_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They mostly seem to match for specific color and directional patterns. Lets try a bunch of other layers.\n",
    "We will randomly visualize 10 filters within various layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53ca126490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53c6f71c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53cc80ac10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53cd2e0510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_indices = []\n",
    "for layer_name in ['block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3']:\n",
    "    layer_idx = utils.find_layer_idx(model, layer_name)\n",
    "\n",
    "    # Visualize all filters in this layer.\n",
    "    filters = np.random.permutation(get_num_filters(model.layers[layer_idx]))[:10]\n",
    "    selected_indices.append(filters)\n",
    "\n",
    "    # Generate input image for each filter.\n",
    "    vis_images = []\n",
    "    for idx in filters:\n",
    "        img = visualize_activation(model, layer_idx, filter_indices=idx)\n",
    "\n",
    "        # Utility to overlay text on image.\n",
    "        img = utils.draw_text(img, 'Filter {}'.format(idx))    \n",
    "        vis_images.append(img)\n",
    "\n",
    "    # Generate stitched image palette with 5 cols so we get 2 rows.\n",
    "    stitched = utils.stitch_images(vis_images, cols=5)    \n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(stitched)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how filters evolved to look for simple -> complex abstract patterns.\n",
    "\n",
    "We also notice that some of the filters in `block5_conv3` (the last one) failed to converge.  This is usually because regularization losses (total variation and LP norm) are overtaking activation maximization loss (set verbose=True to observe). There are a couple of options to make this work better,\n",
    "\n",
    "- Different regularization weights.\n",
    "- Increase number of iterations.\n",
    "- Add `Jitter` input_modifier.\n",
    "- Try with 0 regularization weights, generate a converged image and use that as `seed_input` with regularization enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show a subset of these ideas here. Lets start by adidng Jitter and disabling total variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53c8a3e450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_idx = utils.find_layer_idx(model, 'block5_conv3')\n",
    "\n",
    "# We need to select the same random filters in order to compare the results.\n",
    "filters = selected_indices[-1]\n",
    "selected_indices.append(filters)\n",
    "\n",
    "# Generate input image for each filter.\n",
    "vis_images = []\n",
    "for idx in filters:\n",
    "    # We will jitter 5% relative to the image size.\n",
    "    img = visualize_activation(model, layer_idx, filter_indices=idx, \n",
    "                               tv_weight=0.,\n",
    "                               input_modifiers=[Jitter(0.05)])\n",
    "\n",
    "    # Utility to overlay text on image.\n",
    "    img = utils.draw_text(img, 'Filter {}'.format(idx))    \n",
    "    vis_images.append(img)\n",
    "\n",
    "# Generate stitched image palette with 5 cols so we get 2 rows.\n",
    "stitched = utils.stitch_images(vis_images, cols=5)    \n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(stitched)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how previously unconverged filters show something this time. Lets take a specific output from here and use it as a `seed_input` with total_variation enabled this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53c20a9290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate input image for each filter.\n",
    "new_vis_images = []\n",
    "for i, idx in enumerate(filters):\n",
    "    # We will seed with optimized image this time.\n",
    "    img = visualize_activation(model, layer_idx, filter_indices=idx, \n",
    "                               seed_input=vis_images[i],\n",
    "                               input_modifiers=[Jitter(0.05)])\n",
    "\n",
    "    # Utility to overlay text on image.\n",
    "    img = utils.draw_text(img, 'Filter {}'.format(idx))    \n",
    "    new_vis_images.append(img)\n",
    "\n",
    "# Generate stitched image palette with 5 cols so we get 2 rows.\n",
    "stitched = utils.stitch_images(new_vis_images, cols=5)    \n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(stitched)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that, folks, is how we roll :)\n",
    "This trick works pretty well to get those stubborn filters to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other fun stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API to `visualize_activation` accepts `filter_indices`. This is generally meant for *multi label* classifiers, but nothing prevents us from having some fun. \n",
    "\n",
    "By setting `filter_indices`, to multiple output categories, we can generate an input that the network thinks is both those categories. Maybe we can generate a cool looking crab fish. I will leave this as an exersice to the reader. You mgith have to experiment with regularization weights a lot.\n",
    "\n",
    "Ideally, we can use a GAN trained on imagenet and use the discriminator loss as a regularizer. This is easily done using `visualize_activations_with_losses` API. If you ever do this, please consider submitting a PR :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations without swapping softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alluded at the beginning of the tutorial, we want to compare and see what happens if we didnt swap out softmax for linear activation.\n",
    "\n",
    "Lets try the `ouzel` visualization again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f53c40eae90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53c48293d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
    "\n",
    "# Swap linear back with softmax\n",
    "model.layers[layer_idx].activation = activations.softmax\n",
    "model = utils.apply_modifications(model)\n",
    "\n",
    "img = visualize_activation(model, layer_idx, filter_indices=20, input_modifiers=[Jitter(16)])\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not work! The reason is that maximizing an output node can be done by minimizing other outputs. Softmax is weird that way. It is the only activation that depends on other node output(s) in the layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
