{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Maximization on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build the mnist model and train it for 5 epochs. It should get to about ~99% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s - loss: 0.2346 - acc: 0.9279 - val_loss: 0.0552 - val_acc: 0.9817\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s - loss: 0.0845 - acc: 0.9756 - val_loss: 0.0363 - val_acc: 0.9881\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s - loss: 0.0620 - acc: 0.9819 - val_loss: 0.0336 - val_acc: 0.9897\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s - loss: 0.0533 - acc: 0.9839 - val_loss: 0.0317 - val_acc: 0.9895\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s - loss: 0.0436 - acc: 0.9865 - val_loss: 0.0295 - val_acc: 0.9910\n",
      "Test loss: 0.0295144059571\n",
      "Test accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax', name='preds'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize activation over final dense layer outputs, we need to switch the `softmax` activation out for `linear` since gradient of output node will depend on all the other node activations. Doing this in keras is tricky, so we provide `utils.apply_modifications` to modify network parameters and rebuild the graph.\n",
    "\n",
    "If this swapping is not done, the results might be suboptimal. We will start by swapping out 'softmax' for 'linear' and compare what happens if we dont do this at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by visualizing input that maximizes the output of node 0. Hopefully this looks like a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd935d14a50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9341a45d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vis.visualization import visualize_activation\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "\n",
    "# Utility to search for layer index by name. \n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx = utils.find_layer_idx(model, 'preds')\n",
    "\n",
    "# Swap softmax with linear\n",
    "model.layers[layer_idx].activation = activations.linear\n",
    "model = utils.apply_modifications(model)\n",
    "\n",
    "# This is the output node we want to maximize.\n",
    "filter_idx = 0\n",
    "img = visualize_activation(model, layer_idx, filter_indices=filter_idx)\n",
    "plt.imshow(img[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hmm, it sort of looks like a 0, but not as clear as we hoped for. Activation maximization is notorious because regularization parameters needs to be tuned depending on the problem. Lets enumerate all the possible reasons why this didn't work very well.\n",
    "    \n",
    "- The input to network is preprocessed to range (0, 1). We should specify `input_range = (0., 1.)` to constrain the input to this range.\n",
    "- The regularization parameter default weights might be dominating activation maximization loss weight. One way to debug this is to use `verbose=True` and examine individual loss values.\n",
    "\n",
    "Lets do these step by step and see if we can improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging step 1: Specifying input_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd93360dfd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda1b1312d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = visualize_activation(model, layer_idx, filter_indices=filter_idx, input_range=(0., 1.))\n",
    "plt.imshow(img[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better but still seems noisy. Lets examining the losses with `verbose=True` and tuning the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging step 2: Tuning regularization weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the issues with activation maximization is that the input can go out of the training distribution space. Total variation and L-p norm are used to provide some hardcoded image priors for natural images. For example, Total variation ensures that images are blobber and not scattered. Unfotunately, sometimes these losses can dominate the main `ActivationMaximization` loss.\n",
    "\n",
    "Lets see what individual losses are, with `verbose=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, named_losses: [('ActivationMax Loss', 0.085303515),\n",
      " ('L-6.0 Norm Loss', 0.019828862),\n",
      " ('TV(2.0) Loss', 0.095951974)], overall loss: 0.201084345579\n",
      "Iteration: 2, named_losses: [('ActivationMax Loss', -2.3290093),\n",
      " ('L-6.0 Norm Loss', 0.17779008),\n",
      " ('TV(2.0) Loss', 507.10727)], overall loss: 504.956054688\n",
      "Iteration: 3, named_losses: [('ActivationMax Loss', -128.49548),\n",
      " ('L-6.0 Norm Loss', 0.19667891),\n",
      " ('TV(2.0) Loss', 164.45709)], overall loss: 36.1582946777\n",
      "Iteration: 4, named_losses: [('ActivationMax Loss', -250.48241),\n",
      " ('L-6.0 Norm Loss', 0.19896419),\n",
      " ('TV(2.0) Loss', 178.49908)], overall loss: -71.784362793\n",
      "Iteration: 5, named_losses: [('ActivationMax Loss', -355.06989),\n",
      " ('L-6.0 Norm Loss', 0.23281187),\n",
      " ('TV(2.0) Loss', 168.41599)], overall loss: -186.421081543\n",
      "Iteration: 6, named_losses: [('ActivationMax Loss', -434.47665),\n",
      " ('L-6.0 Norm Loss', 0.27569067),\n",
      " ('TV(2.0) Loss', 198.10806)], overall loss: -236.092895508\n",
      "Iteration: 7, named_losses: [('ActivationMax Loss', -523.30029),\n",
      " ('L-6.0 Norm Loss', 0.30946341),\n",
      " ('TV(2.0) Loss', 210.7379)], overall loss: -312.252929688\n",
      "Iteration: 8, named_losses: [('ActivationMax Loss', -589.32751),\n",
      " ('L-6.0 Norm Loss', 0.34706685),\n",
      " ('TV(2.0) Loss', 243.71239)], overall loss: -345.268066406\n",
      "Iteration: 9, named_losses: [('ActivationMax Loss', -649.27594),\n",
      " ('L-6.0 Norm Loss', 0.37611905),\n",
      " ('TV(2.0) Loss', 256.13412)], overall loss: -392.765716553\n",
      "Iteration: 10, named_losses: [('ActivationMax Loss', -709.78107),\n",
      " ('L-6.0 Norm Loss', 0.41110787),\n",
      " ('TV(2.0) Loss', 284.26044)], overall loss: -425.10949707\n",
      "Iteration: 11, named_losses: [('ActivationMax Loss', -761.53046),\n",
      " ('L-6.0 Norm Loss', 0.43992683),\n",
      " ('TV(2.0) Loss', 306.72531)], overall loss: -454.365203857\n",
      "Iteration: 12, named_losses: [('ActivationMax Loss', -803.29578),\n",
      " ('L-6.0 Norm Loss', 0.46880898),\n",
      " ('TV(2.0) Loss', 327.67728)], overall loss: -475.149688721\n",
      "Iteration: 13, named_losses: [('ActivationMax Loss', -846.75684),\n",
      " ('L-6.0 Norm Loss', 0.49682248),\n",
      " ('TV(2.0) Loss', 344.90573)], overall loss: -501.354278564\n",
      "Iteration: 14, named_losses: [('ActivationMax Loss', -883.35626),\n",
      " ('L-6.0 Norm Loss', 0.52348113),\n",
      " ('TV(2.0) Loss', 363.5777)], overall loss: -519.255065918\n",
      "Iteration: 15, named_losses: [('ActivationMax Loss', -919.09418),\n",
      " ('L-6.0 Norm Loss', 0.5485183),\n",
      " ('TV(2.0) Loss', 378.69067)], overall loss: -539.854980469\n",
      "Iteration: 16, named_losses: [('ActivationMax Loss', -948.51349),\n",
      " ('L-6.0 Norm Loss', 0.56927013),\n",
      " ('TV(2.0) Loss', 390.0394)], overall loss: -557.904785156\n",
      "Iteration: 17, named_losses: [('ActivationMax Loss', -978.81732),\n",
      " ('L-6.0 Norm Loss', 0.59408391),\n",
      " ('TV(2.0) Loss', 406.41376)], overall loss: -571.809509277\n",
      "Iteration: 18, named_losses: [('ActivationMax Loss', -1010.2624),\n",
      " ('L-6.0 Norm Loss', 0.61176264),\n",
      " ('TV(2.0) Loss', 424.86453)], overall loss: -584.786132812\n",
      "Iteration: 19, named_losses: [('ActivationMax Loss', -1032.5134),\n",
      " ('L-6.0 Norm Loss', 0.63450885),\n",
      " ('TV(2.0) Loss', 435.95856)], overall loss: -595.920349121\n",
      "Iteration: 20, named_losses: [('ActivationMax Loss', -1055.4211),\n",
      " ('L-6.0 Norm Loss', 0.64917922),\n",
      " ('TV(2.0) Loss', 447.13791)], overall loss: -607.634033203\n",
      "Iteration: 21, named_losses: [('ActivationMax Loss', -1078.2205),\n",
      " ('L-6.0 Norm Loss', 0.67029834),\n",
      " ('TV(2.0) Loss', 460.73138)], overall loss: -616.818786621\n",
      "Iteration: 22, named_losses: [('ActivationMax Loss', -1096.8605),\n",
      " ('L-6.0 Norm Loss', 0.68686134),\n",
      " ('TV(2.0) Loss', 468.72369)], overall loss: -627.449890137\n",
      "Iteration: 23, named_losses: [('ActivationMax Loss', -1115.5505),\n",
      " ('L-6.0 Norm Loss', 0.70358962),\n",
      " ('TV(2.0) Loss', 477.83783)], overall loss: -637.009094238\n",
      "Iteration: 24, named_losses: [('ActivationMax Loss', -1135.0172),\n",
      " ('L-6.0 Norm Loss', 0.72281784),\n",
      " ('TV(2.0) Loss', 488.59711)], overall loss: -645.69732666\n",
      "Iteration: 25, named_losses: [('ActivationMax Loss', -1152.8055),\n",
      " ('L-6.0 Norm Loss', 0.73808521),\n",
      " ('TV(2.0) Loss', 493.71191)], overall loss: -658.35559082\n",
      "Iteration: 26, named_losses: [('ActivationMax Loss', -1169.7035),\n",
      " ('L-6.0 Norm Loss', 0.75386697),\n",
      " ('TV(2.0) Loss', 499.82452)], overall loss: -669.125061035\n",
      "Iteration: 27, named_losses: [('ActivationMax Loss', -1189.5547),\n",
      " ('L-6.0 Norm Loss', 0.77207923),\n",
      " ('TV(2.0) Loss', 516.74023)], overall loss: -672.042358398\n",
      "Iteration: 28, named_losses: [('ActivationMax Loss', -1206.5782),\n",
      " ('L-6.0 Norm Loss', 0.7868582),\n",
      " ('TV(2.0) Loss', 521.85266)], overall loss: -683.938720703\n",
      "Iteration: 29, named_losses: [('ActivationMax Loss', -1226.3325),\n",
      " ('L-6.0 Norm Loss', 0.80321062),\n",
      " ('TV(2.0) Loss', 536.8913)], overall loss: -688.638000488\n",
      "Iteration: 30, named_losses: [('ActivationMax Loss', -1243.972),\n",
      " ('L-6.0 Norm Loss', 0.81793976),\n",
      " ('TV(2.0) Loss', 539.8042)], overall loss: -703.349853516\n",
      "Iteration: 31, named_losses: [('ActivationMax Loss', -1262.2374),\n",
      " ('L-6.0 Norm Loss', 0.83369249),\n",
      " ('TV(2.0) Loss', 555.77899)], overall loss: -705.624694824\n",
      "Iteration: 32, named_losses: [('ActivationMax Loss', -1277.142),\n",
      " ('L-6.0 Norm Loss', 0.84892559),\n",
      " ('TV(2.0) Loss', 559.75134)], overall loss: -716.541748047\n",
      "Iteration: 33, named_losses: [('ActivationMax Loss', -1291.3621),\n",
      " ('L-6.0 Norm Loss', 0.86305672),\n",
      " ('TV(2.0) Loss', 571.33044)], overall loss: -719.168579102\n",
      "Iteration: 34, named_losses: [('ActivationMax Loss', -1304.9154),\n",
      " ('L-6.0 Norm Loss', 0.87694514),\n",
      " ('TV(2.0) Loss', 578.43042)], overall loss: -725.608032227\n",
      "Iteration: 35, named_losses: [('ActivationMax Loss', -1318.2659),\n",
      " ('L-6.0 Norm Loss', 0.89181656),\n",
      " ('TV(2.0) Loss', 591.61877)], overall loss: -725.755249023\n",
      "Iteration: 36, named_losses: [('ActivationMax Loss', -1329.4354),\n",
      " ('L-6.0 Norm Loss', 0.90528834),\n",
      " ('TV(2.0) Loss', 595.05951)], overall loss: -733.47064209\n",
      "Iteration: 37, named_losses: [('ActivationMax Loss', -1341.7084),\n",
      " ('L-6.0 Norm Loss', 0.91941857),\n",
      " ('TV(2.0) Loss', 605.28784)], overall loss: -735.501098633\n",
      "Iteration: 38, named_losses: [('ActivationMax Loss', -1354.1248),\n",
      " ('L-6.0 Norm Loss', 0.93217278),\n",
      " ('TV(2.0) Loss', 612.47931)], overall loss: -740.713317871\n",
      "Iteration: 39, named_losses: [('ActivationMax Loss', -1364.4468),\n",
      " ('L-6.0 Norm Loss', 0.94575441),\n",
      " ('TV(2.0) Loss', 619.2569)], overall loss: -744.24407959\n",
      "Iteration: 40, named_losses: [('ActivationMax Loss', -1374.3206),\n",
      " ('L-6.0 Norm Loss', 0.9577564),\n",
      " ('TV(2.0) Loss', 625.97729)], overall loss: -747.385498047\n",
      "Iteration: 41, named_losses: [('ActivationMax Loss', -1384.3914),\n",
      " ('L-6.0 Norm Loss', 0.96838021),\n",
      " ('TV(2.0) Loss', 633.25616)], overall loss: -750.166809082\n",
      "Iteration: 42, named_losses: [('ActivationMax Loss', -1393.1029),\n",
      " ('L-6.0 Norm Loss', 0.98039478),\n",
      " ('TV(2.0) Loss', 639.09497)], overall loss: -753.027587891\n",
      "Iteration: 43, named_losses: [('ActivationMax Loss', -1402.049),\n",
      " ('L-6.0 Norm Loss', 0.99183488),\n",
      " ('TV(2.0) Loss', 644.44427)], overall loss: -756.612854004\n",
      "Iteration: 44, named_losses: [('ActivationMax Loss', -1410.5962),\n",
      " ('L-6.0 Norm Loss', 1.0034488),\n",
      " ('TV(2.0) Loss', 651.58691)], overall loss: -758.005859375\n",
      "Iteration: 45, named_losses: [('ActivationMax Loss', -1419.7144),\n",
      " ('L-6.0 Norm Loss', 1.0143936),\n",
      " ('TV(2.0) Loss', 658.54822)], overall loss: -760.151733398\n",
      "Iteration: 46, named_losses: [('ActivationMax Loss', -1427.2026),\n",
      " ('L-6.0 Norm Loss', 1.0241909),\n",
      " ('TV(2.0) Loss', 662.922)], overall loss: -763.256469727\n",
      "Iteration: 47, named_losses: [('ActivationMax Loss', -1435.6279),\n",
      " ('L-6.0 Norm Loss', 1.0342484),\n",
      " ('TV(2.0) Loss', 667.56097)], overall loss: -767.032653809\n",
      "Iteration: 48, named_losses: [('ActivationMax Loss', -1440.479),\n",
      " ('L-6.0 Norm Loss', 1.0420389),\n",
      " ('TV(2.0) Loss', 672.19409)], overall loss: -767.242919922\n",
      "Iteration: 49, named_losses: [('ActivationMax Loss', -1453.1812),\n",
      " ('L-6.0 Norm Loss', 1.0515953),\n",
      " ('TV(2.0) Loss', 680.70709)], overall loss: -771.422424316\n",
      "Iteration: 50, named_losses: [('ActivationMax Loss', -1454.2151),\n",
      " ('L-6.0 Norm Loss', 1.0597379),\n",
      " ('TV(2.0) Loss', 682.62335)], overall loss: -770.532043457\n",
      "Iteration: 51, named_losses: [('ActivationMax Loss', -1468.5298),\n",
      " ('L-6.0 Norm Loss', 1.0692028),\n",
      " ('TV(2.0) Loss', 692.5365)], overall loss: -774.924072266\n",
      "Iteration: 52, named_losses: [('ActivationMax Loss', -1468.5466),\n",
      " ('L-6.0 Norm Loss', 1.0780802),\n",
      " ('TV(2.0) Loss', 694.88562)], overall loss: -772.582885742\n",
      "Iteration: 53, named_losses: [('ActivationMax Loss', -1478.1926),\n",
      " ('L-6.0 Norm Loss', 1.0857446),\n",
      " ('TV(2.0) Loss', 699.60638)], overall loss: -777.500549316\n",
      "Iteration: 54, named_losses: [('ActivationMax Loss', -1483.5609),\n",
      " ('L-6.0 Norm Loss', 1.0934379),\n",
      " ('TV(2.0) Loss', 706.73547)], overall loss: -775.732055664\n",
      "Iteration: 55, named_losses: [('ActivationMax Loss', -1491.7773),\n",
      " ('L-6.0 Norm Loss', 1.1018604),\n",
      " ('TV(2.0) Loss', 710.49097)], overall loss: -780.184570312\n",
      "Iteration: 56, named_losses: [('ActivationMax Loss', -1496.1121),\n",
      " ('L-6.0 Norm Loss', 1.1094059),\n",
      " ('TV(2.0) Loss', 716.18457)], overall loss: -778.818115234\n",
      "Iteration: 57, named_losses: [('ActivationMax Loss', -1503.3049),\n",
      " ('L-6.0 Norm Loss', 1.1176937),\n",
      " ('TV(2.0) Loss', 719.49158)], overall loss: -782.695678711\n",
      "Iteration: 58, named_losses: [('ActivationMax Loss', -1507.1075),\n",
      " ('L-6.0 Norm Loss', 1.1252179),\n",
      " ('TV(2.0) Loss', 723.2171)], overall loss: -782.765197754\n",
      "Iteration: 59, named_losses: [('ActivationMax Loss', -1508.6431),\n",
      " ('L-6.0 Norm Loss', 1.1337999),\n",
      " ('TV(2.0) Loss', 722.63678)], overall loss: -784.872497559\n",
      "Iteration: 60, named_losses: [('ActivationMax Loss', -1514.7773),\n",
      " ('L-6.0 Norm Loss', 1.1402849),\n",
      " ('TV(2.0) Loss', 728.61115)], overall loss: -785.025939941\n",
      "Iteration: 61, named_losses: [('ActivationMax Loss', -1514.7411),\n",
      " ('L-6.0 Norm Loss', 1.1487488),\n",
      " ('TV(2.0) Loss', 727.47565)], overall loss: -786.116638184\n",
      "Iteration: 62, named_losses: [('ActivationMax Loss', -1521.7354),\n",
      " ('L-6.0 Norm Loss', 1.1555879),\n",
      " ('TV(2.0) Loss', 731.41626)], overall loss: -789.163452148\n",
      "Iteration: 63, named_losses: [('ActivationMax Loss', -1522.7675),\n",
      " ('L-6.0 Norm Loss', 1.1635603),\n",
      " ('TV(2.0) Loss', 733.62488)], overall loss: -787.979003906\n",
      "Iteration: 64, named_losses: [('ActivationMax Loss', -1529.7563),\n",
      " ('L-6.0 Norm Loss', 1.1674472),\n",
      " ('TV(2.0) Loss', 736.68823)], overall loss: -791.900634766\n",
      "Iteration: 65, named_losses: [('ActivationMax Loss', -1531.0713),\n",
      " ('L-6.0 Norm Loss', 1.1751125),\n",
      " ('TV(2.0) Loss', 739.92639)], overall loss: -789.969726562\n",
      "Iteration: 66, named_losses: [('ActivationMax Loss', -1534.9249),\n",
      " ('L-6.0 Norm Loss', 1.1804783),\n",
      " ('TV(2.0) Loss', 740.112)], overall loss: -793.632507324\n",
      "Iteration: 67, named_losses: [('ActivationMax Loss', -1535.5088),\n",
      " ('L-6.0 Norm Loss', 1.1873333),\n",
      " ('TV(2.0) Loss', 741.09711)], overall loss: -793.224304199\n",
      "Iteration: 68, named_losses: [('ActivationMax Loss', -1539.807),\n",
      " ('L-6.0 Norm Loss', 1.1929805),\n",
      " ('TV(2.0) Loss', 742.85437)], overall loss: -795.759643555\n",
      "Iteration: 69, named_losses: [('ActivationMax Loss', -1537.9623),\n",
      " ('L-6.0 Norm Loss', 1.1999267),\n",
      " ('TV(2.0) Loss', 742.83746)], overall loss: -793.924865723\n",
      "Iteration: 70, named_losses: [('ActivationMax Loss', -1549.0408),\n",
      " ('L-6.0 Norm Loss', 1.2060388),\n",
      " ('TV(2.0) Loss', 749.69873)], overall loss: -798.135986328\n",
      "Iteration: 71, named_losses: [('ActivationMax Loss', -1549.3425),\n",
      " ('L-6.0 Norm Loss', 1.2124674),\n",
      " ('TV(2.0) Loss', 750.94574)], overall loss: -797.184265137\n",
      "Iteration: 72, named_losses: [('ActivationMax Loss', -1553.8987),\n",
      " ('L-6.0 Norm Loss', 1.21632),\n",
      " ('TV(2.0) Loss', 752.06598)], overall loss: -800.616394043\n",
      "Iteration: 73, named_losses: [('ActivationMax Loss', -1554.4263),\n",
      " ('L-6.0 Norm Loss', 1.2229993),\n",
      " ('TV(2.0) Loss', 754.32281)], overall loss: -798.880432129\n",
      "Iteration: 74, named_losses: [('ActivationMax Loss', -1559.6265),\n",
      " ('L-6.0 Norm Loss', 1.2293643),\n",
      " ('TV(2.0) Loss', 758.35571)], overall loss: -800.041381836\n",
      "Iteration: 75, named_losses: [('ActivationMax Loss', -1561.2106),\n",
      " ('L-6.0 Norm Loss', 1.2329044),\n",
      " ('TV(2.0) Loss', 760.01758)], overall loss: -799.960083008\n",
      "Iteration: 76, named_losses: [('ActivationMax Loss', -1562.5452),\n",
      " ('L-6.0 Norm Loss', 1.2385361),\n",
      " ('TV(2.0) Loss', 759.76331)], overall loss: -801.543334961\n",
      "Iteration: 77, named_losses: [('ActivationMax Loss', -1564.2513),\n",
      " ('L-6.0 Norm Loss', 1.2413583),\n",
      " ('TV(2.0) Loss', 760.64935)], overall loss: -802.360656738\n",
      "Iteration: 78, named_losses: [('ActivationMax Loss', -1566.9346),\n",
      " ('L-6.0 Norm Loss', 1.2466581),\n",
      " ('TV(2.0) Loss', 764.04144)], overall loss: -801.64642334\n",
      "Iteration: 79, named_losses: [('ActivationMax Loss', -1570.8954),\n",
      " ('L-6.0 Norm Loss', 1.2509888),\n",
      " ('TV(2.0) Loss', 766.50098)], overall loss: -803.143432617\n",
      "Iteration: 80, named_losses: [('ActivationMax Loss', -1572.2623),\n",
      " ('L-6.0 Norm Loss', 1.2554729),\n",
      " ('TV(2.0) Loss', 766.65845)], overall loss: -804.348388672\n",
      "Iteration: 81, named_losses: [('ActivationMax Loss', -1574.7688),\n",
      " ('L-6.0 Norm Loss', 1.2608629),\n",
      " ('TV(2.0) Loss', 768.88452)], overall loss: -804.623413086\n",
      "Iteration: 82, named_losses: [('ActivationMax Loss', -1574.8457),\n",
      " ('L-6.0 Norm Loss', 1.264998),\n",
      " ('TV(2.0) Loss', 767.55579)], overall loss: -806.024902344\n",
      "Iteration: 83, named_losses: [('ActivationMax Loss', -1578.4631),\n",
      " ('L-6.0 Norm Loss', 1.2702675),\n",
      " ('TV(2.0) Loss', 770.93079)], overall loss: -806.262084961\n",
      "Iteration: 84, named_losses: [('ActivationMax Loss', -1579.1116),\n",
      " ('L-6.0 Norm Loss', 1.2734473),\n",
      " ('TV(2.0) Loss', 770.62146)], overall loss: -807.216674805\n",
      "Iteration: 85, named_losses: [('ActivationMax Loss', -1581.9132),\n",
      " ('L-6.0 Norm Loss', 1.2792308),\n",
      " ('TV(2.0) Loss', 772.18756)], overall loss: -808.446472168\n",
      "Iteration: 86, named_losses: [('ActivationMax Loss', -1579.4325),\n",
      " ('L-6.0 Norm Loss', 1.2830144),\n",
      " ('TV(2.0) Loss', 766.79077)], overall loss: -811.358764648\n",
      "Iteration: 87, named_losses: [('ActivationMax Loss', -1585.8596),\n",
      " ('L-6.0 Norm Loss', 1.2859445),\n",
      " ('TV(2.0) Loss', 775.53821)], overall loss: -809.035522461\n",
      "Iteration: 88, named_losses: [('ActivationMax Loss', -1585.7672),\n",
      " ('L-6.0 Norm Loss', 1.2900306),\n",
      " ('TV(2.0) Loss', 774.06342)], overall loss: -810.413757324\n",
      "Iteration: 89, named_losses: [('ActivationMax Loss', -1591.1372),\n",
      " ('L-6.0 Norm Loss', 1.2948806),\n",
      " ('TV(2.0) Loss', 778.63391)], overall loss: -811.208374023\n",
      "Iteration: 90, named_losses: [('ActivationMax Loss', -1590.0701),\n",
      " ('L-6.0 Norm Loss', 1.2955327),\n",
      " ('TV(2.0) Loss', 777.77234)], overall loss: -811.002197266\n",
      "Iteration: 91, named_losses: [('ActivationMax Loss', -1590.9371),\n",
      " ('L-6.0 Norm Loss', 1.3006396),\n",
      " ('TV(2.0) Loss', 777.98926)], overall loss: -811.647216797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 92, named_losses: [('ActivationMax Loss', -1597.2491),\n",
      " ('L-6.0 Norm Loss', 1.3033614),\n",
      " ('TV(2.0) Loss', 783.2879)], overall loss: -812.657897949\n",
      "Iteration: 93, named_losses: [('ActivationMax Loss', -1593.3665),\n",
      " ('L-6.0 Norm Loss', 1.3075004),\n",
      " ('TV(2.0) Loss', 779.44653)], overall loss: -812.612426758\n",
      "Iteration: 94, named_losses: [('ActivationMax Loss', -1597.2029),\n",
      " ('L-6.0 Norm Loss', 1.3099935),\n",
      " ('TV(2.0) Loss', 781.82526)], overall loss: -814.067687988\n",
      "Iteration: 95, named_losses: [('ActivationMax Loss', -1597.9543),\n",
      " ('L-6.0 Norm Loss', 1.3138223),\n",
      " ('TV(2.0) Loss', 783.27612)], overall loss: -813.364379883\n",
      "Iteration: 96, named_losses: [('ActivationMax Loss', -1600.9514),\n",
      " ('L-6.0 Norm Loss', 1.3189675),\n",
      " ('TV(2.0) Loss', 784.29456)], overall loss: -815.337890625\n",
      "Iteration: 97, named_losses: [('ActivationMax Loss', -1603.2629),\n",
      " ('L-6.0 Norm Loss', 1.3227088),\n",
      " ('TV(2.0) Loss', 787.86951)], overall loss: -814.070678711\n",
      "Iteration: 98, named_losses: [('ActivationMax Loss', -1603.4062),\n",
      " ('L-6.0 Norm Loss', 1.3246108),\n",
      " ('TV(2.0) Loss', 786.18884)], overall loss: -815.892822266\n",
      "Iteration: 99, named_losses: [('ActivationMax Loss', -1602.4215),\n",
      " ('L-6.0 Norm Loss', 1.3267232),\n",
      " ('TV(2.0) Loss', 784.94574)], overall loss: -816.148986816\n",
      "Iteration: 100, named_losses: [('ActivationMax Loss', -1606.3367),\n",
      " ('L-6.0 Norm Loss', 1.3313514),\n",
      " ('TV(2.0) Loss', 788.65411)], overall loss: -816.351257324\n",
      "Iteration: 101, named_losses: [('ActivationMax Loss', -1605.1202),\n",
      " ('L-6.0 Norm Loss', 1.3336935),\n",
      " ('TV(2.0) Loss', 787.86621)], overall loss: -815.920288086\n",
      "Iteration: 102, named_losses: [('ActivationMax Loss', -1609.257),\n",
      " ('L-6.0 Norm Loss', 1.3371928),\n",
      " ('TV(2.0) Loss', 790.25415)], overall loss: -817.665649414\n",
      "Iteration: 103, named_losses: [('ActivationMax Loss', -1610.245),\n",
      " ('L-6.0 Norm Loss', 1.3413918),\n",
      " ('TV(2.0) Loss', 791.96606)], overall loss: -816.9375\n",
      "Iteration: 104, named_losses: [('ActivationMax Loss', -1610.9143),\n",
      " ('L-6.0 Norm Loss', 1.3415542),\n",
      " ('TV(2.0) Loss', 789.74146)], overall loss: -819.831298828\n",
      "Iteration: 105, named_losses: [('ActivationMax Loss', -1609.8514),\n",
      " ('L-6.0 Norm Loss', 1.3457475),\n",
      " ('TV(2.0) Loss', 789.99353)], overall loss: -818.512207031\n",
      "Iteration: 106, named_losses: [('ActivationMax Loss', -1612.2958),\n",
      " ('L-6.0 Norm Loss', 1.3498583),\n",
      " ('TV(2.0) Loss', 789.27185)], overall loss: -821.674072266\n",
      "Iteration: 107, named_losses: [('ActivationMax Loss', -1613.5688),\n",
      " ('L-6.0 Norm Loss', 1.3565289),\n",
      " ('TV(2.0) Loss', 793.34521)], overall loss: -818.86706543\n",
      "Iteration: 108, named_losses: [('ActivationMax Loss', -1617.8153),\n",
      " ('L-6.0 Norm Loss', 1.358252),\n",
      " ('TV(2.0) Loss', 794.26929)], overall loss: -822.187744141\n",
      "Iteration: 109, named_losses: [('ActivationMax Loss', -1617.1315),\n",
      " ('L-6.0 Norm Loss', 1.3626406),\n",
      " ('TV(2.0) Loss', 794.89624)], overall loss: -820.872558594\n",
      "Iteration: 110, named_losses: [('ActivationMax Loss', -1619.2756),\n",
      " ('L-6.0 Norm Loss', 1.3654678),\n",
      " ('TV(2.0) Loss', 794.74115)], overall loss: -823.169006348\n",
      "Iteration: 111, named_losses: [('ActivationMax Loss', -1620.2999),\n",
      " ('L-6.0 Norm Loss', 1.368789),\n",
      " ('TV(2.0) Loss', 797.3844)], overall loss: -821.54675293\n",
      "Iteration: 112, named_losses: [('ActivationMax Loss', -1619.2432),\n",
      " ('L-6.0 Norm Loss', 1.3725339),\n",
      " ('TV(2.0) Loss', 793.66309)], overall loss: -824.207519531\n",
      "Iteration: 113, named_losses: [('ActivationMax Loss', -1625.923),\n",
      " ('L-6.0 Norm Loss', 1.3758172),\n",
      " ('TV(2.0) Loss', 800.75488)], overall loss: -823.792236328\n",
      "Iteration: 114, named_losses: [('ActivationMax Loss', -1622.631),\n",
      " ('L-6.0 Norm Loss', 1.3799908),\n",
      " ('TV(2.0) Loss', 797.35974)], overall loss: -823.891235352\n",
      "Iteration: 115, named_losses: [('ActivationMax Loss', -1630.325),\n",
      " ('L-6.0 Norm Loss', 1.384307),\n",
      " ('TV(2.0) Loss', 806.6814)], overall loss: -822.259277344\n",
      "Iteration: 116, named_losses: [('ActivationMax Loss', -1627.0356),\n",
      " ('L-6.0 Norm Loss', 1.3886331),\n",
      " ('TV(2.0) Loss', 802.25623)], overall loss: -823.39074707\n",
      "Iteration: 117, named_losses: [('ActivationMax Loss', -1632.6683),\n",
      " ('L-6.0 Norm Loss', 1.3907378),\n",
      " ('TV(2.0) Loss', 809.20844)], overall loss: -822.069152832\n",
      "Iteration: 118, named_losses: [('ActivationMax Loss', -1632.0132),\n",
      " ('L-6.0 Norm Loss', 1.3947749),\n",
      " ('TV(2.0) Loss', 806.38666)], overall loss: -824.231750488\n",
      "Iteration: 119, named_losses: [('ActivationMax Loss', -1633.7933),\n",
      " ('L-6.0 Norm Loss', 1.3970679),\n",
      " ('TV(2.0) Loss', 809.47327)], overall loss: -822.922973633\n",
      "Iteration: 120, named_losses: [('ActivationMax Loss', -1631.33),\n",
      " ('L-6.0 Norm Loss', 1.4000779),\n",
      " ('TV(2.0) Loss', 804.59418)], overall loss: -825.335754395\n",
      "Iteration: 121, named_losses: [('ActivationMax Loss', -1638.6511),\n",
      " ('L-6.0 Norm Loss', 1.4029664),\n",
      " ('TV(2.0) Loss', 812.93854)], overall loss: -824.309631348\n",
      "Iteration: 122, named_losses: [('ActivationMax Loss', -1631.8582),\n",
      " ('L-6.0 Norm Loss', 1.4053441),\n",
      " ('TV(2.0) Loss', 804.75452)], overall loss: -825.698242188\n",
      "Iteration: 123, named_losses: [('ActivationMax Loss', -1638.329),\n",
      " ('L-6.0 Norm Loss', 1.4088998),\n",
      " ('TV(2.0) Loss', 811.19293)], overall loss: -825.727111816\n",
      "Iteration: 124, named_losses: [('ActivationMax Loss', -1632.4836),\n",
      " ('L-6.0 Norm Loss', 1.4110554),\n",
      " ('TV(2.0) Loss', 804.57092)], overall loss: -826.501708984\n",
      "Iteration: 125, named_losses: [('ActivationMax Loss', -1639.8102),\n",
      " ('L-6.0 Norm Loss', 1.4141355),\n",
      " ('TV(2.0) Loss', 812.0838)], overall loss: -826.312194824\n",
      "Iteration: 126, named_losses: [('ActivationMax Loss', -1630.1312),\n",
      " ('L-6.0 Norm Loss', 1.4141332),\n",
      " ('TV(2.0) Loss', 800.64581)], overall loss: -828.071228027\n",
      "Iteration: 127, named_losses: [('ActivationMax Loss', -1639.1195),\n",
      " ('L-6.0 Norm Loss', 1.4192623),\n",
      " ('TV(2.0) Loss', 809.91083)], overall loss: -827.789367676\n",
      "Iteration: 128, named_losses: [('ActivationMax Loss', -1630.7975),\n",
      " ('L-6.0 Norm Loss', 1.4200757),\n",
      " ('TV(2.0) Loss', 800.08356)], overall loss: -829.293884277\n",
      "Iteration: 129, named_losses: [('ActivationMax Loss', -1638.1284),\n",
      " ('L-6.0 Norm Loss', 1.4252733),\n",
      " ('TV(2.0) Loss', 809.07318)], overall loss: -827.629943848\n",
      "Iteration: 130, named_losses: [('ActivationMax Loss', -1636.0492),\n",
      " ('L-6.0 Norm Loss', 1.4285793),\n",
      " ('TV(2.0) Loss', 803.84949)], overall loss: -830.771118164\n",
      "Iteration: 131, named_losses: [('ActivationMax Loss', -1641.9642),\n",
      " ('L-6.0 Norm Loss', 1.433162),\n",
      " ('TV(2.0) Loss', 811.50299)], overall loss: -829.028137207\n",
      "Iteration: 132, named_losses: [('ActivationMax Loss', -1639.944),\n",
      " ('L-6.0 Norm Loss', 1.4367878),\n",
      " ('TV(2.0) Loss', 807.71741)], overall loss: -830.789794922\n",
      "Iteration: 133, named_losses: [('ActivationMax Loss', -1642.401),\n",
      " ('L-6.0 Norm Loss', 1.4381592),\n",
      " ('TV(2.0) Loss', 811.63275)], overall loss: -829.33013916\n",
      "Iteration: 134, named_losses: [('ActivationMax Loss', -1643.0408),\n",
      " ('L-6.0 Norm Loss', 1.442253),\n",
      " ('TV(2.0) Loss', 810.08728)], overall loss: -831.511230469\n",
      "Iteration: 135, named_losses: [('ActivationMax Loss', -1647.0085),\n",
      " ('L-6.0 Norm Loss', 1.444943),\n",
      " ('TV(2.0) Loss', 815.15967)], overall loss: -830.403930664\n",
      "Iteration: 136, named_losses: [('ActivationMax Loss', -1644.9188),\n",
      " ('L-6.0 Norm Loss', 1.4477811),\n",
      " ('TV(2.0) Loss', 811.31506)], overall loss: -832.156005859\n",
      "Iteration: 137, named_losses: [('ActivationMax Loss', -1653.4288),\n",
      " ('L-6.0 Norm Loss', 1.4509344),\n",
      " ('TV(2.0) Loss', 821.15198)], overall loss: -830.825927734\n",
      "Iteration: 138, named_losses: [('ActivationMax Loss', -1648.1528),\n",
      " ('L-6.0 Norm Loss', 1.4527802),\n",
      " ('TV(2.0) Loss', 814.87396)], overall loss: -831.82611084\n",
      "Iteration: 139, named_losses: [('ActivationMax Loss', -1649.7593),\n",
      " ('L-6.0 Norm Loss', 1.4559247),\n",
      " ('TV(2.0) Loss', 817.54858)], overall loss: -830.754760742\n",
      "Iteration: 140, named_losses: [('ActivationMax Loss', -1651.0073),\n",
      " ('L-6.0 Norm Loss', 1.4594543),\n",
      " ('TV(2.0) Loss', 816.99554)], overall loss: -832.552307129\n",
      "Iteration: 141, named_losses: [('ActivationMax Loss', -1652.6647),\n",
      " ('L-6.0 Norm Loss', 1.4621137),\n",
      " ('TV(2.0) Loss', 819.40613)], overall loss: -831.796386719\n",
      "Iteration: 142, named_losses: [('ActivationMax Loss', -1654.1587),\n",
      " ('L-6.0 Norm Loss', 1.4648318),\n",
      " ('TV(2.0) Loss', 820.21576)], overall loss: -832.478088379\n",
      "Iteration: 143, named_losses: [('ActivationMax Loss', -1651.4489),\n",
      " ('L-6.0 Norm Loss', 1.465472),\n",
      " ('TV(2.0) Loss', 812.49847)], overall loss: -837.484924316\n",
      "Iteration: 144, named_losses: [('ActivationMax Loss', -1655.8921),\n",
      " ('L-6.0 Norm Loss', 1.4706283),\n",
      " ('TV(2.0) Loss', 819.5379)], overall loss: -834.883605957\n",
      "Iteration: 145, named_losses: [('ActivationMax Loss', -1651.3011),\n",
      " ('L-6.0 Norm Loss', 1.4713544),\n",
      " ('TV(2.0) Loss', 814.84924)], overall loss: -834.98059082\n",
      "Iteration: 146, named_losses: [('ActivationMax Loss', -1656.283),\n",
      " ('L-6.0 Norm Loss', 1.4738773),\n",
      " ('TV(2.0) Loss', 823.04236)], overall loss: -831.766723633\n",
      "Iteration: 147, named_losses: [('ActivationMax Loss', -1660.2869),\n",
      " ('L-6.0 Norm Loss', 1.4773164),\n",
      " ('TV(2.0) Loss', 824.23651)], overall loss: -834.573059082\n",
      "Iteration: 148, named_losses: [('ActivationMax Loss', -1662.1096),\n",
      " ('L-6.0 Norm Loss', 1.4793681),\n",
      " ('TV(2.0) Loss', 828.23236)], overall loss: -832.397888184\n",
      "Iteration: 149, named_losses: [('ActivationMax Loss', -1661.8433),\n",
      " ('L-6.0 Norm Loss', 1.4821515),\n",
      " ('TV(2.0) Loss', 825.2392)], overall loss: -835.121887207\n",
      "Iteration: 150, named_losses: [('ActivationMax Loss', -1665.4824),\n",
      " ('L-6.0 Norm Loss', 1.4861856),\n",
      " ('TV(2.0) Loss', 829.73688)], overall loss: -834.259338379\n",
      "Iteration: 151, named_losses: [('ActivationMax Loss', -1664.7596),\n",
      " ('L-6.0 Norm Loss', 1.4876471),\n",
      " ('TV(2.0) Loss', 826.73108)], overall loss: -836.540893555\n",
      "Iteration: 152, named_losses: [('ActivationMax Loss', -1666.2804),\n",
      " ('L-6.0 Norm Loss', 1.4919786),\n",
      " ('TV(2.0) Loss', 830.87958)], overall loss: -833.908874512\n",
      "Iteration: 153, named_losses: [('ActivationMax Loss', -1668.1177),\n",
      " ('L-6.0 Norm Loss', 1.4925706),\n",
      " ('TV(2.0) Loss', 830.95911)], overall loss: -835.666015625\n",
      "Iteration: 154, named_losses: [('ActivationMax Loss', -1668.7156),\n",
      " ('L-6.0 Norm Loss', 1.4961685),\n",
      " ('TV(2.0) Loss', 831.15045)], overall loss: -836.068908691\n",
      "Iteration: 155, named_losses: [('ActivationMax Loss', -1669.6495),\n",
      " ('L-6.0 Norm Loss', 1.4984686),\n",
      " ('TV(2.0) Loss', 831.70325)], overall loss: -836.447875977\n",
      "Iteration: 156, named_losses: [('ActivationMax Loss', -1673.0961),\n",
      " ('L-6.0 Norm Loss', 1.5010495),\n",
      " ('TV(2.0) Loss', 835.07239)], overall loss: -836.522583008\n",
      "Iteration: 157, named_losses: [('ActivationMax Loss', -1672.6519),\n",
      " ('L-6.0 Norm Loss', 1.5066335),\n",
      " ('TV(2.0) Loss', 835.20679)], overall loss: -835.938476562\n",
      "Iteration: 158, named_losses: [('ActivationMax Loss', -1677.0101),\n",
      " ('L-6.0 Norm Loss', 1.5054392),\n",
      " ('TV(2.0) Loss', 837.4469)], overall loss: -838.057739258\n",
      "Iteration: 159, named_losses: [('ActivationMax Loss', -1675.916),\n",
      " ('L-6.0 Norm Loss', 1.5089097),\n",
      " ('TV(2.0) Loss', 836.17157)], overall loss: -838.235534668\n",
      "Iteration: 160, named_losses: [('ActivationMax Loss', -1674.426),\n",
      " ('L-6.0 Norm Loss', 1.5099328),\n",
      " ('TV(2.0) Loss', 834.57648)], overall loss: -838.339660645\n",
      "Iteration: 161, named_losses: [('ActivationMax Loss', -1676.6719),\n",
      " ('L-6.0 Norm Loss', 1.5128522),\n",
      " ('TV(2.0) Loss', 836.09076)], overall loss: -839.06829834\n",
      "Iteration: 162, named_losses: [('ActivationMax Loss', -1676.5416),\n",
      " ('L-6.0 Norm Loss', 1.5148972),\n",
      " ('TV(2.0) Loss', 835.69391)], overall loss: -839.332824707\n",
      "Iteration: 163, named_losses: [('ActivationMax Loss', -1676.1887),\n",
      " ('L-6.0 Norm Loss', 1.516475),\n",
      " ('TV(2.0) Loss', 834.33459)], overall loss: -840.337646484\n",
      "Iteration: 164, named_losses: [('ActivationMax Loss', -1677.428),\n",
      " ('L-6.0 Norm Loss', 1.5198225),\n",
      " ('TV(2.0) Loss', 835.4469)], overall loss: -840.461303711\n",
      "Iteration: 165, named_losses: [('ActivationMax Loss', -1677.0521),\n",
      " ('L-6.0 Norm Loss', 1.5217334),\n",
      " ('TV(2.0) Loss', 833.99994)], overall loss: -841.530456543\n",
      "Iteration: 166, named_losses: [('ActivationMax Loss', -1680.4186),\n",
      " ('L-6.0 Norm Loss', 1.527281),\n",
      " ('TV(2.0) Loss', 839.37067)], overall loss: -839.520690918\n",
      "Iteration: 167, named_losses: [('ActivationMax Loss', -1682.5245),\n",
      " ('L-6.0 Norm Loss', 1.5286567),\n",
      " ('TV(2.0) Loss', 839.15784)], overall loss: -841.838012695\n",
      "Iteration: 168, named_losses: [('ActivationMax Loss', -1679.6903),\n",
      " ('L-6.0 Norm Loss', 1.5303226),\n",
      " ('TV(2.0) Loss', 837.42731)], overall loss: -840.732727051\n",
      "Iteration: 169, named_losses: [('ActivationMax Loss', -1684.0806),\n",
      " ('L-6.0 Norm Loss', 1.5321445),\n",
      " ('TV(2.0) Loss', 840.0152)], overall loss: -842.53326416\n",
      "Iteration: 170, named_losses: [('ActivationMax Loss', -1681.1489),\n",
      " ('L-6.0 Norm Loss', 1.5356567),\n",
      " ('TV(2.0) Loss', 837.11945)], overall loss: -842.493835449\n",
      "Iteration: 171, named_losses: [('ActivationMax Loss', -1683.8822),\n",
      " ('L-6.0 Norm Loss', 1.5385299),\n",
      " ('TV(2.0) Loss', 839.58362)], overall loss: -842.760009766\n",
      "Iteration: 172, named_losses: [('ActivationMax Loss', -1680.66),\n",
      " ('L-6.0 Norm Loss', 1.5389018),\n",
      " ('TV(2.0) Loss', 833.68958)], overall loss: -845.431518555\n",
      "Iteration: 173, named_losses: [('ActivationMax Loss', -1684.0814),\n",
      " ('L-6.0 Norm Loss', 1.5411844),\n",
      " ('TV(2.0) Loss', 839.33081)], overall loss: -843.209472656\n",
      "Iteration: 174, named_losses: [('ActivationMax Loss', -1681.9932),\n",
      " ('L-6.0 Norm Loss', 1.5433018),\n",
      " ('TV(2.0) Loss', 837.27722)], overall loss: -843.172607422\n",
      "Iteration: 175, named_losses: [('ActivationMax Loss', -1686.0037),\n",
      " ('L-6.0 Norm Loss', 1.5452924),\n",
      " ('TV(2.0) Loss', 841.27289)], overall loss: -843.18548584\n",
      "Iteration: 176, named_losses: [('ActivationMax Loss', -1687.5237),\n",
      " ('L-6.0 Norm Loss', 1.5497506),\n",
      " ('TV(2.0) Loss', 845.2395)], overall loss: -840.734375\n",
      "Iteration: 177, named_losses: [('ActivationMax Loss', -1692.7676),\n",
      " ('L-6.0 Norm Loss', 1.5519809),\n",
      " ('TV(2.0) Loss', 847.61469)], overall loss: -843.600891113\n",
      "Iteration: 178, named_losses: [('ActivationMax Loss', -1691.171),\n",
      " ('L-6.0 Norm Loss', 1.5546336),\n",
      " ('TV(2.0) Loss', 848.039)], overall loss: -841.577331543\n",
      "Iteration: 179, named_losses: [('ActivationMax Loss', -1692.5399),\n",
      " ('L-6.0 Norm Loss', 1.5569656),\n",
      " ('TV(2.0) Loss', 846.35913)], overall loss: -844.623779297\n",
      "Iteration: 180, named_losses: [('ActivationMax Loss', -1690.7684),\n",
      " ('L-6.0 Norm Loss', 1.558495),\n",
      " ('TV(2.0) Loss', 846.54626)], overall loss: -842.663696289\n",
      "Iteration: 181, named_losses: [('ActivationMax Loss', -1694.9828),\n",
      " ('L-6.0 Norm Loss', 1.558827),\n",
      " ('TV(2.0) Loss', 848.42737)], overall loss: -844.996582031\n",
      "Iteration: 182, named_losses: [('ActivationMax Loss', -1693.8423),\n",
      " ('L-6.0 Norm Loss', 1.5600398),\n",
      " ('TV(2.0) Loss', 847.75787)], overall loss: -844.524353027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 183, named_losses: [('ActivationMax Loss', -1692.3008),\n",
      " ('L-6.0 Norm Loss', 1.5624027),\n",
      " ('TV(2.0) Loss', 846.34583)], overall loss: -844.392578125\n",
      "Iteration: 184, named_losses: [('ActivationMax Loss', -1690.6182),\n",
      " ('L-6.0 Norm Loss', 1.5639281),\n",
      " ('TV(2.0) Loss', 845.07623)], overall loss: -843.977966309\n",
      "Iteration: 185, named_losses: [('ActivationMax Loss', -1694.6195),\n",
      " ('L-6.0 Norm Loss', 1.5679384),\n",
      " ('TV(2.0) Loss', 848.89496)], overall loss: -844.156555176\n",
      "Iteration: 186, named_losses: [('ActivationMax Loss', -1694.5382),\n",
      " ('L-6.0 Norm Loss', 1.5688758),\n",
      " ('TV(2.0) Loss', 849.05872)], overall loss: -843.910644531\n",
      "Iteration: 187, named_losses: [('ActivationMax Loss', -1700.241),\n",
      " ('L-6.0 Norm Loss', 1.5718434),\n",
      " ('TV(2.0) Loss', 853.30139)], overall loss: -845.367675781\n",
      "Iteration: 188, named_losses: [('ActivationMax Loss', -1694.207),\n",
      " ('L-6.0 Norm Loss', 1.5731733),\n",
      " ('TV(2.0) Loss', 848.63531)], overall loss: -843.998596191\n",
      "Iteration: 189, named_losses: [('ActivationMax Loss', -1702.188),\n",
      " ('L-6.0 Norm Loss', 1.5767503),\n",
      " ('TV(2.0) Loss', 855.16302)], overall loss: -845.448181152\n",
      "Iteration: 190, named_losses: [('ActivationMax Loss', -1696.559),\n",
      " ('L-6.0 Norm Loss', 1.5781298),\n",
      " ('TV(2.0) Loss', 848.68616)], overall loss: -846.294677734\n",
      "Iteration: 191, named_losses: [('ActivationMax Loss', -1698.0985),\n",
      " ('L-6.0 Norm Loss', 1.5782369),\n",
      " ('TV(2.0) Loss', 849.00452)], overall loss: -847.51574707\n",
      "Iteration: 192, named_losses: [('ActivationMax Loss', -1695.0345),\n",
      " ('L-6.0 Norm Loss', 1.5810226),\n",
      " ('TV(2.0) Loss', 846.17896)], overall loss: -847.274536133\n",
      "Iteration: 193, named_losses: [('ActivationMax Loss', -1698.2462),\n",
      " ('L-6.0 Norm Loss', 1.5834887),\n",
      " ('TV(2.0) Loss', 846.41547)], overall loss: -850.247253418\n",
      "Iteration: 194, named_losses: [('ActivationMax Loss', -1698.2938),\n",
      " ('L-6.0 Norm Loss', 1.5867252),\n",
      " ('TV(2.0) Loss', 846.93781)], overall loss: -849.769348145\n",
      "Iteration: 195, named_losses: [('ActivationMax Loss', -1700.386),\n",
      " ('L-6.0 Norm Loss', 1.5881807),\n",
      " ('TV(2.0) Loss', 848.39166)], overall loss: -850.406188965\n",
      "Iteration: 196, named_losses: [('ActivationMax Loss', -1703.4745),\n",
      " ('L-6.0 Norm Loss', 1.5905124),\n",
      " ('TV(2.0) Loss', 854.04553)], overall loss: -847.838500977\n",
      "Iteration: 197, named_losses: [('ActivationMax Loss', -1706.6498),\n",
      " ('L-6.0 Norm Loss', 1.5927349),\n",
      " ('TV(2.0) Loss', 856.89001)], overall loss: -848.166992188\n",
      "Iteration: 198, named_losses: [('ActivationMax Loss', -1710.0554),\n",
      " ('L-6.0 Norm Loss', 1.5939628),\n",
      " ('TV(2.0) Loss', 859.12634)], overall loss: -849.335083008\n",
      "Iteration: 199, named_losses: [('ActivationMax Loss', -1707.3005),\n",
      " ('L-6.0 Norm Loss', 1.5956845),\n",
      " ('TV(2.0) Loss', 855.88831)], overall loss: -849.81652832\n",
      "Iteration: 200, named_losses: [('ActivationMax Loss', -1709.6353),\n",
      " ('L-6.0 Norm Loss', 1.5962927),\n",
      " ('TV(2.0) Loss', 858.9491)], overall loss: -849.08984375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd9332cbe90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd93362e050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = visualize_activation(model, layer_idx, filter_indices=filter_idx, input_range=(0., 1.), verbose=True)\n",
    "plt.imshow(img[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `ActivationMax Loss` is not bouncing bouncing around and converging? Perhaps we could get that loss to be lower by reducing weights of other losses that might be dominating the overall loss being minimized. \n",
    "\n",
    "The simplest way to tune these weights is to first start with `0.` weights for all regularization losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, named_losses: [('ActivationMax Loss', 0.2693989)], overall loss: 0.269398897886\n",
      "Iteration: 2, named_losses: [('ActivationMax Loss', -6.6923928)], overall loss: -6.69239282608\n",
      "Iteration: 3, named_losses: [('ActivationMax Loss', -225.87207)], overall loss: -225.872070312\n",
      "Iteration: 4, named_losses: [('ActivationMax Loss', -462.48257)], overall loss: -462.482574463\n",
      "Iteration: 5, named_losses: [('ActivationMax Loss', -689.16077)], overall loss: -689.160766602\n",
      "Iteration: 6, named_losses: [('ActivationMax Loss', -889.75427)], overall loss: -889.754272461\n",
      "Iteration: 7, named_losses: [('ActivationMax Loss', -1073.9482)], overall loss: -1073.94824219\n",
      "Iteration: 8, named_losses: [('ActivationMax Loss', -1242.8525)], overall loss: -1242.85253906\n",
      "Iteration: 9, named_losses: [('ActivationMax Loss', -1402.7773)], overall loss: -1402.77734375\n",
      "Iteration: 10, named_losses: [('ActivationMax Loss', -1558.7063)], overall loss: -1558.70629883\n",
      "Iteration: 11, named_losses: [('ActivationMax Loss', -1701.7627)], overall loss: -1701.76269531\n",
      "Iteration: 12, named_losses: [('ActivationMax Loss', -1840.2097)], overall loss: -1840.2097168\n",
      "Iteration: 13, named_losses: [('ActivationMax Loss', -1975.5297)], overall loss: -1975.52966309\n",
      "Iteration: 14, named_losses: [('ActivationMax Loss', -2106.1282)], overall loss: -2106.12817383\n",
      "Iteration: 15, named_losses: [('ActivationMax Loss', -2241.844)], overall loss: -2241.84399414\n",
      "Iteration: 16, named_losses: [('ActivationMax Loss', -2373.1284)], overall loss: -2373.12841797\n",
      "Iteration: 17, named_losses: [('ActivationMax Loss', -2499.5166)], overall loss: -2499.51660156\n",
      "Iteration: 18, named_losses: [('ActivationMax Loss', -2621.3357)], overall loss: -2621.33569336\n",
      "Iteration: 19, named_losses: [('ActivationMax Loss', -2742.8274)], overall loss: -2742.82739258\n",
      "Iteration: 20, named_losses: [('ActivationMax Loss', -2862.6206)], overall loss: -2862.62060547\n",
      "Iteration: 21, named_losses: [('ActivationMax Loss', -2978.5793)], overall loss: -2978.5793457\n",
      "Iteration: 22, named_losses: [('ActivationMax Loss', -3092.2688)], overall loss: -3092.26879883\n",
      "Iteration: 23, named_losses: [('ActivationMax Loss', -3205.6792)], overall loss: -3205.67919922\n",
      "Iteration: 24, named_losses: [('ActivationMax Loss', -3318.1287)], overall loss: -3318.12866211\n",
      "Iteration: 25, named_losses: [('ActivationMax Loss', -3428.925)], overall loss: -3428.92504883\n",
      "Iteration: 26, named_losses: [('ActivationMax Loss', -3536.9773)], overall loss: -3536.97729492\n",
      "Iteration: 27, named_losses: [('ActivationMax Loss', -3644.4365)], overall loss: -3644.43652344\n",
      "Iteration: 28, named_losses: [('ActivationMax Loss', -3751.6382)], overall loss: -3751.63818359\n",
      "Iteration: 29, named_losses: [('ActivationMax Loss', -3857.9834)], overall loss: -3857.98339844\n",
      "Iteration: 30, named_losses: [('ActivationMax Loss', -3964.2317)], overall loss: -3964.23168945\n",
      "Iteration: 31, named_losses: [('ActivationMax Loss', -4069.6455)], overall loss: -4069.64550781\n",
      "Iteration: 32, named_losses: [('ActivationMax Loss', -4173.1313)], overall loss: -4173.13134766\n",
      "Iteration: 33, named_losses: [('ActivationMax Loss', -4277.3506)], overall loss: -4277.35058594\n",
      "Iteration: 34, named_losses: [('ActivationMax Loss', -4380.5352)], overall loss: -4380.53515625\n",
      "Iteration: 35, named_losses: [('ActivationMax Loss', -4484.7056)], overall loss: -4484.70556641\n",
      "Iteration: 36, named_losses: [('ActivationMax Loss', -4587.394)], overall loss: -4587.39404297\n",
      "Iteration: 37, named_losses: [('ActivationMax Loss', -4690.5303)], overall loss: -4690.53027344\n",
      "Iteration: 38, named_losses: [('ActivationMax Loss', -4792.5132)], overall loss: -4792.51318359\n",
      "Iteration: 39, named_losses: [('ActivationMax Loss', -4893.3652)], overall loss: -4893.36523438\n",
      "Iteration: 40, named_losses: [('ActivationMax Loss', -4993.0762)], overall loss: -4993.07617188\n",
      "Iteration: 41, named_losses: [('ActivationMax Loss', -5093.8433)], overall loss: -5093.84326172\n",
      "Iteration: 42, named_losses: [('ActivationMax Loss', -5193.5752)], overall loss: -5193.57519531\n",
      "Iteration: 43, named_losses: [('ActivationMax Loss', -5294.8237)], overall loss: -5294.82373047\n",
      "Iteration: 44, named_losses: [('ActivationMax Loss', -5394.9277)], overall loss: -5394.92773438\n",
      "Iteration: 45, named_losses: [('ActivationMax Loss', -5494.9424)], overall loss: -5494.94238281\n",
      "Iteration: 46, named_losses: [('ActivationMax Loss', -5593.3843)], overall loss: -5593.38427734\n",
      "Iteration: 47, named_losses: [('ActivationMax Loss', -5694.7734)], overall loss: -5694.7734375\n",
      "Iteration: 48, named_losses: [('ActivationMax Loss', -5793.5518)], overall loss: -5793.55175781\n",
      "Iteration: 49, named_losses: [('ActivationMax Loss', -5892.7148)], overall loss: -5892.71484375\n",
      "Iteration: 50, named_losses: [('ActivationMax Loss', -5992.6699)], overall loss: -5992.66992188\n",
      "Iteration: 51, named_losses: [('ActivationMax Loss', -6092.2173)], overall loss: -6092.21728516\n",
      "Iteration: 52, named_losses: [('ActivationMax Loss', -6190.8921)], overall loss: -6190.89208984\n",
      "Iteration: 53, named_losses: [('ActivationMax Loss', -6290.4185)], overall loss: -6290.41845703\n",
      "Iteration: 54, named_losses: [('ActivationMax Loss', -6388.0405)], overall loss: -6388.04052734\n",
      "Iteration: 55, named_losses: [('ActivationMax Loss', -6486.2891)], overall loss: -6486.2890625\n",
      "Iteration: 56, named_losses: [('ActivationMax Loss', -6585.1128)], overall loss: -6585.11279297\n",
      "Iteration: 57, named_losses: [('ActivationMax Loss', -6681.5127)], overall loss: -6681.51269531\n",
      "Iteration: 58, named_losses: [('ActivationMax Loss', -6779.0796)], overall loss: -6779.07958984\n",
      "Iteration: 59, named_losses: [('ActivationMax Loss', -6876.9258)], overall loss: -6876.92578125\n",
      "Iteration: 60, named_losses: [('ActivationMax Loss', -6974.938)], overall loss: -6974.93798828\n",
      "Iteration: 61, named_losses: [('ActivationMax Loss', -7070.7168)], overall loss: -7070.71679688\n",
      "Iteration: 62, named_losses: [('ActivationMax Loss', -7166.8511)], overall loss: -7166.85107422\n",
      "Iteration: 63, named_losses: [('ActivationMax Loss', -7264.6636)], overall loss: -7264.66357422\n",
      "Iteration: 64, named_losses: [('ActivationMax Loss', -7360.7495)], overall loss: -7360.74951172\n",
      "Iteration: 65, named_losses: [('ActivationMax Loss', -7456.3501)], overall loss: -7456.35009766\n",
      "Iteration: 66, named_losses: [('ActivationMax Loss', -7551.9409)], overall loss: -7551.94091797\n",
      "Iteration: 67, named_losses: [('ActivationMax Loss', -7648.4165)], overall loss: -7648.41650391\n",
      "Iteration: 68, named_losses: [('ActivationMax Loss', -7744.001)], overall loss: -7744.00097656\n",
      "Iteration: 69, named_losses: [('ActivationMax Loss', -7838.5376)], overall loss: -7838.53759766\n",
      "Iteration: 70, named_losses: [('ActivationMax Loss', -7934.4219)], overall loss: -7934.421875\n",
      "Iteration: 71, named_losses: [('ActivationMax Loss', -8030.0952)], overall loss: -8030.09521484\n",
      "Iteration: 72, named_losses: [('ActivationMax Loss', -8125.4727)], overall loss: -8125.47265625\n",
      "Iteration: 73, named_losses: [('ActivationMax Loss', -8220.7197)], overall loss: -8220.71972656\n",
      "Iteration: 74, named_losses: [('ActivationMax Loss', -8316.6143)], overall loss: -8316.61425781\n",
      "Iteration: 75, named_losses: [('ActivationMax Loss', -8412.3506)], overall loss: -8412.35058594\n",
      "Iteration: 76, named_losses: [('ActivationMax Loss', -8507.9229)], overall loss: -8507.92285156\n",
      "Iteration: 77, named_losses: [('ActivationMax Loss', -8604.7178)], overall loss: -8604.71777344\n",
      "Iteration: 78, named_losses: [('ActivationMax Loss', -8702.0508)], overall loss: -8702.05078125\n",
      "Iteration: 79, named_losses: [('ActivationMax Loss', -8797.6445)], overall loss: -8797.64453125\n",
      "Iteration: 80, named_losses: [('ActivationMax Loss', -8894.2295)], overall loss: -8894.22949219\n",
      "Iteration: 81, named_losses: [('ActivationMax Loss', -8990.502)], overall loss: -8990.50195312\n",
      "Iteration: 82, named_losses: [('ActivationMax Loss', -9087.7773)], overall loss: -9087.77734375\n",
      "Iteration: 83, named_losses: [('ActivationMax Loss', -9185.1807)], overall loss: -9185.18066406\n",
      "Iteration: 84, named_losses: [('ActivationMax Loss', -9282.834)], overall loss: -9282.83398438\n",
      "Iteration: 85, named_losses: [('ActivationMax Loss', -9380.6523)], overall loss: -9380.65234375\n",
      "Iteration: 86, named_losses: [('ActivationMax Loss', -9478.0107)], overall loss: -9478.01074219\n",
      "Iteration: 87, named_losses: [('ActivationMax Loss', -9575.5498)], overall loss: -9575.54980469\n",
      "Iteration: 88, named_losses: [('ActivationMax Loss', -9673.2188)], overall loss: -9673.21875\n",
      "Iteration: 89, named_losses: [('ActivationMax Loss', -9770.9697)], overall loss: -9770.96972656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 90, named_losses: [('ActivationMax Loss', -9868.7793)], overall loss: -9868.77929688\n",
      "Iteration: 91, named_losses: [('ActivationMax Loss', -9964.5371)], overall loss: -9964.53710938\n",
      "Iteration: 92, named_losses: [('ActivationMax Loss', -10062.412)], overall loss: -10062.4121094\n",
      "Iteration: 93, named_losses: [('ActivationMax Loss', -10158.876)], overall loss: -10158.8759766\n",
      "Iteration: 94, named_losses: [('ActivationMax Loss', -10256.677)], overall loss: -10256.6767578\n",
      "Iteration: 95, named_losses: [('ActivationMax Loss', -10352.661)], overall loss: -10352.6611328\n",
      "Iteration: 96, named_losses: [('ActivationMax Loss', -10447.359)], overall loss: -10447.359375\n",
      "Iteration: 97, named_losses: [('ActivationMax Loss', -10544.511)], overall loss: -10544.5107422\n",
      "Iteration: 98, named_losses: [('ActivationMax Loss', -10640.197)], overall loss: -10640.1972656\n",
      "Iteration: 99, named_losses: [('ActivationMax Loss', -10737.199)], overall loss: -10737.1992188\n",
      "Iteration: 100, named_losses: [('ActivationMax Loss', -10832.438)], overall loss: -10832.4375\n",
      "Iteration: 101, named_losses: [('ActivationMax Loss', -10929.422)], overall loss: -10929.421875\n",
      "Iteration: 102, named_losses: [('ActivationMax Loss', -11025.653)], overall loss: -11025.6533203\n",
      "Iteration: 103, named_losses: [('ActivationMax Loss', -11121.746)], overall loss: -11121.7460938\n",
      "Iteration: 104, named_losses: [('ActivationMax Loss', -11216.964)], overall loss: -11216.9638672\n",
      "Iteration: 105, named_losses: [('ActivationMax Loss', -11313.283)], overall loss: -11313.2832031\n",
      "Iteration: 106, named_losses: [('ActivationMax Loss', -11410.26)], overall loss: -11410.2597656\n",
      "Iteration: 107, named_losses: [('ActivationMax Loss', -11506.09)], overall loss: -11506.0898438\n",
      "Iteration: 108, named_losses: [('ActivationMax Loss', -11600.643)], overall loss: -11600.6425781\n",
      "Iteration: 109, named_losses: [('ActivationMax Loss', -11697.956)], overall loss: -11697.9560547\n",
      "Iteration: 110, named_losses: [('ActivationMax Loss', -11793.214)], overall loss: -11793.2138672\n",
      "Iteration: 111, named_losses: [('ActivationMax Loss', -11889.193)], overall loss: -11889.1933594\n",
      "Iteration: 112, named_losses: [('ActivationMax Loss', -11984.958)], overall loss: -11984.9580078\n",
      "Iteration: 113, named_losses: [('ActivationMax Loss', -12081.564)], overall loss: -12081.5644531\n",
      "Iteration: 114, named_losses: [('ActivationMax Loss', -12177.07)], overall loss: -12177.0703125\n",
      "Iteration: 115, named_losses: [('ActivationMax Loss', -12273.154)], overall loss: -12273.1542969\n",
      "Iteration: 116, named_losses: [('ActivationMax Loss', -12370.32)], overall loss: -12370.3203125\n",
      "Iteration: 117, named_losses: [('ActivationMax Loss', -12466.484)], overall loss: -12466.484375\n",
      "Iteration: 118, named_losses: [('ActivationMax Loss', -12562.509)], overall loss: -12562.5087891\n",
      "Iteration: 119, named_losses: [('ActivationMax Loss', -12658.77)], overall loss: -12658.7695312\n",
      "Iteration: 120, named_losses: [('ActivationMax Loss', -12755.0)], overall loss: -12755.0\n",
      "Iteration: 121, named_losses: [('ActivationMax Loss', -12852.065)], overall loss: -12852.0654297\n",
      "Iteration: 122, named_losses: [('ActivationMax Loss', -12948.583)], overall loss: -12948.5830078\n",
      "Iteration: 123, named_losses: [('ActivationMax Loss', -13043.738)], overall loss: -13043.7382812\n",
      "Iteration: 124, named_losses: [('ActivationMax Loss', -13139.02)], overall loss: -13139.0195312\n",
      "Iteration: 125, named_losses: [('ActivationMax Loss', -13234.346)], overall loss: -13234.3457031\n",
      "Iteration: 126, named_losses: [('ActivationMax Loss', -13331.491)], overall loss: -13331.4912109\n",
      "Iteration: 127, named_losses: [('ActivationMax Loss', -13427.523)], overall loss: -13427.5234375\n",
      "Iteration: 128, named_losses: [('ActivationMax Loss', -13522.241)], overall loss: -13522.2412109\n",
      "Iteration: 129, named_losses: [('ActivationMax Loss', -13617.408)], overall loss: -13617.4082031\n",
      "Iteration: 130, named_losses: [('ActivationMax Loss', -13714.182)], overall loss: -13714.1816406\n",
      "Iteration: 131, named_losses: [('ActivationMax Loss', -13809.398)], overall loss: -13809.3984375\n",
      "Iteration: 132, named_losses: [('ActivationMax Loss', -13904.697)], overall loss: -13904.6972656\n",
      "Iteration: 133, named_losses: [('ActivationMax Loss', -14002.63)], overall loss: -14002.6298828\n",
      "Iteration: 134, named_losses: [('ActivationMax Loss', -14099.208)], overall loss: -14099.2080078\n",
      "Iteration: 135, named_losses: [('ActivationMax Loss', -14195.607)], overall loss: -14195.6074219\n",
      "Iteration: 136, named_losses: [('ActivationMax Loss', -14291.809)], overall loss: -14291.8085938\n",
      "Iteration: 137, named_losses: [('ActivationMax Loss', -14389.49)], overall loss: -14389.4902344\n",
      "Iteration: 138, named_losses: [('ActivationMax Loss', -14486.239)], overall loss: -14486.2392578\n",
      "Iteration: 139, named_losses: [('ActivationMax Loss', -14584.095)], overall loss: -14584.0947266\n",
      "Iteration: 140, named_losses: [('ActivationMax Loss', -14682.867)], overall loss: -14682.8671875\n",
      "Iteration: 141, named_losses: [('ActivationMax Loss', -14781.787)], overall loss: -14781.7871094\n",
      "Iteration: 142, named_losses: [('ActivationMax Loss', -14879.436)], overall loss: -14879.4355469\n",
      "Iteration: 143, named_losses: [('ActivationMax Loss', -14978.084)], overall loss: -14978.0839844\n",
      "Iteration: 144, named_losses: [('ActivationMax Loss', -15077.616)], overall loss: -15077.6162109\n",
      "Iteration: 145, named_losses: [('ActivationMax Loss', -15175.915)], overall loss: -15175.9150391\n",
      "Iteration: 146, named_losses: [('ActivationMax Loss', -15274.069)], overall loss: -15274.0693359\n",
      "Iteration: 147, named_losses: [('ActivationMax Loss', -15373.179)], overall loss: -15373.1787109\n",
      "Iteration: 148, named_losses: [('ActivationMax Loss', -15471.81)], overall loss: -15471.8095703\n",
      "Iteration: 149, named_losses: [('ActivationMax Loss', -15570.835)], overall loss: -15570.8349609\n",
      "Iteration: 150, named_losses: [('ActivationMax Loss', -15669.877)], overall loss: -15669.8769531\n",
      "Iteration: 151, named_losses: [('ActivationMax Loss', -15768.857)], overall loss: -15768.8574219\n",
      "Iteration: 152, named_losses: [('ActivationMax Loss', -15867.168)], overall loss: -15867.1679688\n",
      "Iteration: 153, named_losses: [('ActivationMax Loss', -15965.778)], overall loss: -15965.7783203\n",
      "Iteration: 154, named_losses: [('ActivationMax Loss', -16064.601)], overall loss: -16064.6005859\n",
      "Iteration: 155, named_losses: [('ActivationMax Loss', -16162.262)], overall loss: -16162.2617188\n",
      "Iteration: 156, named_losses: [('ActivationMax Loss', -16262.289)], overall loss: -16262.2890625\n",
      "Iteration: 157, named_losses: [('ActivationMax Loss', -16359.585)], overall loss: -16359.5849609\n",
      "Iteration: 158, named_losses: [('ActivationMax Loss', -16459.596)], overall loss: -16459.5957031\n",
      "Iteration: 159, named_losses: [('ActivationMax Loss', -16559.24)], overall loss: -16559.2402344\n",
      "Iteration: 160, named_losses: [('ActivationMax Loss', -16659.156)], overall loss: -16659.15625\n",
      "Iteration: 161, named_losses: [('ActivationMax Loss', -16759.33)], overall loss: -16759.3300781\n",
      "Iteration: 162, named_losses: [('ActivationMax Loss', -16861.242)], overall loss: -16861.2421875\n",
      "Iteration: 163, named_losses: [('ActivationMax Loss', -16959.898)], overall loss: -16959.8984375\n",
      "Iteration: 164, named_losses: [('ActivationMax Loss', -17058.123)], overall loss: -17058.1230469\n",
      "Iteration: 165, named_losses: [('ActivationMax Loss', -17158.26)], overall loss: -17158.2597656\n",
      "Iteration: 166, named_losses: [('ActivationMax Loss', -17258.967)], overall loss: -17258.9667969\n",
      "Iteration: 167, named_losses: [('ActivationMax Loss', -17358.326)], overall loss: -17358.3261719\n",
      "Iteration: 168, named_losses: [('ActivationMax Loss', -17458.109)], overall loss: -17458.109375\n",
      "Iteration: 169, named_losses: [('ActivationMax Loss', -17557.682)], overall loss: -17557.6816406\n",
      "Iteration: 170, named_losses: [('ActivationMax Loss', -17655.744)], overall loss: -17655.7441406\n",
      "Iteration: 171, named_losses: [('ActivationMax Loss', -17755.303)], overall loss: -17755.3027344\n",
      "Iteration: 172, named_losses: [('ActivationMax Loss', -17855.494)], overall loss: -17855.4941406\n",
      "Iteration: 173, named_losses: [('ActivationMax Loss', -17955.205)], overall loss: -17955.2050781\n",
      "Iteration: 174, named_losses: [('ActivationMax Loss', -18053.369)], overall loss: -18053.3691406\n",
      "Iteration: 175, named_losses: [('ActivationMax Loss', -18151.721)], overall loss: -18151.7207031\n",
      "Iteration: 176, named_losses: [('ActivationMax Loss', -18251.09)], overall loss: -18251.0898438\n",
      "Iteration: 177, named_losses: [('ActivationMax Loss', -18349.832)], overall loss: -18349.8320312\n",
      "Iteration: 178, named_losses: [('ActivationMax Loss', -18448.842)], overall loss: -18448.8417969\n",
      "Iteration: 179, named_losses: [('ActivationMax Loss', -18547.826)], overall loss: -18547.8261719\n",
      "Iteration: 180, named_losses: [('ActivationMax Loss', -18646.492)], overall loss: -18646.4921875\n",
      "Iteration: 181, named_losses: [('ActivationMax Loss', -18745.336)], overall loss: -18745.3359375\n",
      "Iteration: 182, named_losses: [('ActivationMax Loss', -18844.691)], overall loss: -18844.6914062\n",
      "Iteration: 183, named_losses: [('ActivationMax Loss', -18944.184)], overall loss: -18944.1835938\n",
      "Iteration: 184, named_losses: [('ActivationMax Loss', -19042.6)], overall loss: -19042.5996094\n",
      "Iteration: 185, named_losses: [('ActivationMax Loss', -19142.037)], overall loss: -19142.0371094\n",
      "Iteration: 186, named_losses: [('ActivationMax Loss', -19241.221)], overall loss: -19241.2207031\n",
      "Iteration: 187, named_losses: [('ActivationMax Loss', -19340.268)], overall loss: -19340.2675781\n",
      "Iteration: 188, named_losses: [('ActivationMax Loss', -19439.75)], overall loss: -19439.75\n",
      "Iteration: 189, named_losses: [('ActivationMax Loss', -19539.535)], overall loss: -19539.5351562\n",
      "Iteration: 190, named_losses: [('ActivationMax Loss', -19639.137)], overall loss: -19639.1367188\n",
      "Iteration: 191, named_losses: [('ActivationMax Loss', -19738.834)], overall loss: -19738.8339844\n",
      "Iteration: 192, named_losses: [('ActivationMax Loss', -19838.582)], overall loss: -19838.5820312\n",
      "Iteration: 193, named_losses: [('ActivationMax Loss', -19938.512)], overall loss: -19938.5117188\n",
      "Iteration: 194, named_losses: [('ActivationMax Loss', -20038.475)], overall loss: -20038.4746094\n",
      "Iteration: 195, named_losses: [('ActivationMax Loss', -20139.158)], overall loss: -20139.1582031\n",
      "Iteration: 196, named_losses: [('ActivationMax Loss', -20238.393)], overall loss: -20238.3925781\n",
      "Iteration: 197, named_losses: [('ActivationMax Loss', -20339.186)], overall loss: -20339.1855469\n",
      "Iteration: 198, named_losses: [('ActivationMax Loss', -20438.551)], overall loss: -20438.5507812\n",
      "Iteration: 199, named_losses: [('ActivationMax Loss', -20537.754)], overall loss: -20537.7539062\n",
      "Iteration: 200, named_losses: [('ActivationMax Loss', -20637.371)], overall loss: -20637.3710938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd933115990>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd935e66110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = visualize_activation(model, layer_idx, filter_indices=filter_idx, input_range=(0., 1.), \n",
    "                           tv_weight=0., lp_norm_weight=0., verbose=True)\n",
    "plt.imshow(img[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does indeed go to much lower values, but the image looks less natural. Let's try varous range of total variation weights to enforce naturalness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9331519d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd932eb3d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd932c31c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9329a9ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd93273fc90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tv_weight in [1e-3, 1e-2, 1e-1, 1, 10]:\n",
    "    # Lets turn off verbose output this time to avoid clutter and just see the output.\n",
    "    img = visualize_activation(model, layer_idx, filter_indices=filter_idx, input_range=(0., 1.), \n",
    "                               tv_weight=tv_weight, lp_norm_weight=0.)\n",
    "    plt.figure()\n",
    "    plt.imshow(img[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how total variation loss is enforcing blobbiness. In this case the default value of `tv_weight=10` seems to work very well. The point of this exercise was to show how weights can be tuned.\n",
    "\n",
    "Lets visualize all other output categories and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9332fcf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd932043a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd931d7c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd931a26650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9316cfad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9313fdf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9310ba410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd930d61890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd930a0bd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9329df2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for output_idx in np.arange(10):\n",
    "    # Lets turn off verbose output this time to avoid clutter and just see the output.\n",
    "    img = visualize_activation(model, layer_idx, filter_indices=output_idx, input_range=(0., 1.))\n",
    "    plt.figure()\n",
    "    plt.title('Networks perception of {}'.format(output_idx))\n",
    "    plt.imshow(img[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool. Its amazing that we can even generate an input image via backprop! \n",
    "\n",
    "Obviously you can tune the visualizations to look better by experimenting with `image_modifiers`, lp-norm weight etc. Basically, a regularizer is needed to enforce image naturalness prior to limit the input image search space. By this point, GANs should come to your mind. We could easily take a GAN trained on mnist and use discriminator loss as a regularizer. For using custom loss, you can use `visualize_activation_with_losses` API.\n",
    "\n",
    "Feel free to submit a PR if you try the GAN regularizer :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other fun stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API to `visualize_activation` accepts `filter_indices`. This is generally meant for *multi label* classifiers, but nothing prevents us from having some fun. \n",
    "\n",
    "By setting `filter_indices=[1, 7]`, we can generate an input that the network thinks is both 1 and 7 simultaneously. Its like asking the network\n",
    "\n",
    "> Generate input image that you think is both 1 and a 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd92febccd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9301c2dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = visualize_activation(model, layer_idx, filter_indices=[1, 7], input_range=(0., 1.))\n",
    "plt.imshow(img[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the `1` generated above and you should be able to see the difference. Nifty indded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations without swapping softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alluded at the beginning of the tutorial, we want to compare and see what happens if we didnt swap out softmax for linear activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd936dca2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd937a8ea50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda1b340990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9358c2c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9355e9290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd93528d650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd934fb4bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd934c64f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd93491c550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9345cc810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Swap linear back with softmax\n",
    "model.layers[layer_idx].activation = activations.softmax\n",
    "model = utils.apply_modifications(model)\n",
    "\n",
    "for output_idx in np.arange(10):\n",
    "    # Lets turn off verbose output this time to avoid clutter and just see the output.\n",
    "    img = visualize_activation(model, layer_idx, filter_indices=output_idx, input_range=(0., 1.))\n",
    "    plt.figure()\n",
    "    plt.title('Networks perception of {}'.format(output_idx))\n",
    "    plt.imshow(img[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not work! The reason is that maximizing an output node can be done by minimizing other outputs. Softmax is weird that way. It is the only activation that depends on other node output(s) in the layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
